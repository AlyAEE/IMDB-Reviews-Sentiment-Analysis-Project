# Get CSV files list from a folder
csv_files = glob.glob(DATA_PATH + "/*")
# load the dataframes and concatenate them
raw_df = pd.concat(map(pd.read_csv, csv_files))
raw_df = raw_df.set_index('Unnamed: 0').reset_index(drop=True)
raw_df.shape


import pandas as pd
test = pd.read_csv('../Data/Processed/0_DataCleaned_df.csv')
test.head()


import numpy as np


test['company_name'].value_counts()


test['comp_count'] = test.groupby('company_name')['company_name'].transform('count')


test['companyname'] = np.where(test['comp_count'] >= 3, test['company_name'], np.nan)


test['statecount'] = test.groupby('job_state')['job_state'].transform('count')


test['jobstate'] = np.where(test['statecount'] >= 3, test['job_state'], np.nan)


test['indcount'] = test.groupby('Industry')['Industry'].transform('count')


test['industry'] = np.where(test['indcount'] >= 4, test['Industry'], np.nan)


test.replace(to_replace={-1: np.NAN,'-1':np.NAN},inplace=True) 


test['industry'].value_counts().head(60)


test.info()


test = test.drop(columns=['indcount','statecount','comp_count','job_state','company_name','Industry'],axis = 1)


test.info()





# I had to do onehotencoding before splitting the data, because we have too many categories, it will raise error
# when transforming the test set

df_dummy = pd.get_dummies(df_model)
df_dummy['avg_salaries_cat'] = pd.cut(df_dummy['avg_salary'], bins=[0.,75,100,135,170,np.inf],
                                      labels=[0,1,2,3,4])
split = StratifiedShuffleSplit(n_splits =1, test_size =0.2, random_state=42)
for train_index, test_index in split.split(df_dummy,df_dummy['avg_salaries_cat']):
    strat_train_set_dum = df_dummy.loc[train_index]
    strat_test_set_dum = df_dummy.loc[test_index]

# drop the avg_salaries_cat so the data is back to its original state
for strat_set in (strat_train_set_dum,strat_test_set_dum,df_dummy):
    strat_set.drop("avg_salaries_cat",axis = 1, inplace=True)


df_dummy


x_train_dum = strat_train_set_dum.drop("avg_salary",axis=1)
y_train_dum = strat_train_set_dum['avg_salary'].copy()

num_attribs_dum = list(num_cols.columns)
cat_attribs_dum = list(x_train_dum.iloc[:,8:])


from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler
from sklearn.impute import SimpleImputer

num_pipeline = Pipeline([
    ('imputer',SimpleImputer(missing_values=[np.NaN,-1,'-1'],strategy="mean")),
    ('min_max_scaler', MinMaxScaler()),
])

# full_pipeline = ColumnTransformer([
#     ("num", num_pipeline , num_attribs),
#     ("cat", OneHotEncoder(handle_unknown = "ignore"), cat_attribs),
# ])

model_prepared = num_pipeline.fit_transform(x_train_dum)


model_prepared








cat_cols_copy = cat_cols.copy()
num_cols_copy = num_cols.copy()


cat_cols_copy.replace(to_replace=['-1'],value=np.NAN,inplace=True)


num_cols_copy.replace(to_replace=[-1],value=np.NAN,inplace=True)


cat_cols_copy.info()


num_cols_copy.info()


# To perform KNN imputation, we need to normalize the input data and perform One Hot Encoding to categorical variables
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
cat_dummies = pd.get_dummies(cat_cols_copy, drop_first = True)
df_copy= pd.concat([num_cols_copy, cat_dummies], axis = 1)

scaler = MinMaxScaler()
# scaler = StandardScaler()
minmaxscaler = pd.DataFrame(scaler.fit_transform(df_copy), columns = df_copy.columns)

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors = 3)
knn_imputed_df = pd.DataFrame(imputer.fit_transform(minmaxscaler),columns = df_copy.columns)


knn_imputed_df





cat_cols_test=cat_cols.copy()


#Revenue Column
cat_cols_test['Revenue'].replace(to_replace=['-1'],value=np.NAN,inplace=True)


test = pd.DataFrame(cat_cols_test.Revenue.values.tolist()).stack().value_counts()


cat_cols_test = replace_cat_list(cat_cols_test,'Revenue',test.index)


# type of Ownership Column
cat_cols_test['Type of ownership'].replace(to_replace=['-1'],value=np.NAN,inplace=True)


test = pd.DataFrame(cat_cols_test['Type of ownership'].values.tolist()).stack().value_counts()


cat_cols_test = replace_cat_list(cat_cols_test,'Type of ownership',test.index)


# Industry Column
cat_cols_test['Industry'].replace(to_replace=['-1'],value=np.NAN,inplace=True)


test = pd.DataFrame(cat_cols_test.Industry.values.tolist()).stack().value_counts()


cat_cols_test = replace_cat_list(cat_cols_test,'Industry',test.index)


# Sector column
cat_cols_test['Sector'].replace(to_replace=['-1'],value=np.NAN,inplace=True)


test = pd.DataFrame(cat_cols_test.Sector.values.tolist()).stack().value_counts()


cat_cols_test = replace_cat_list(cat_cols_test,'Sector',test.index)


# Size Column
cat_cols_test['Size'].replace(to_replace=['-1'],value=np.NAN,inplace=True)


test = pd.DataFrame(cat_cols_test.Size.values.tolist()).stack().value_counts()


cat_cols_test = replace_cat_list(cat_cols_test,'Size',test.index)





def replace_cat_list(df,col,cat_list):
    count_cat_dict_initial = {'Total_cat':0}
    for cat in cat_list:
        count_cat_dict_initial[cat] = df.loc[df[col]==cat,col].count()
        count_cat_dict_initial['Total_cat'] = count_cat_dict_initial.get('Total_cat') + count_cat_dict_initial[cat]
    count_cat_dict_initial['Total'] = len(df[col])
    count_cat_dict_final = {'Total_cat':0}
    for cat in cat_list[:-1]:
        count_cat_dict_final[cat] = math.ceil((count_cat_dict_initial.get(cat)/count_cat_dict_initial.get('Total_cat'))*count_cat_dict_initial.get('Total'))
        count_cat_dict_final['Total_cat'] = count_cat_dict_final.get('Total_cat') + count_cat_dict_final[cat]
    count_cat_dict_final[cat_list[-1]] = count_cat_dict_initial['Total'] - count_cat_dict_final['Total_cat']
    fill_dict = {}
    for cat in cat_list:
        fill_dict[cat] = count_cat_dict_final[cat] - count_cat_dict_initial[cat]
    for cat in cat_list[:]:
        for i in range(fill_dict.get(cat)):
            null_index= list(df.loc[pd.isna(df[col]),:].index)
            if len(null_index) != 0:
                df.loc[null_index.pop(0),col] = cat
            else:
                 break
    return df


num_cols_test = num_cols.copy()


num_cols_test.replace(to_replace=[-1],value=np.NAN,inplace=True)


test = pd.DataFrame(num_cols_test.Rating.values.tolist()).stack().value_counts()


test.index


num_cols_test = replace_cat_list(num_cols_test,'Rating',test.index)


test = pd.DataFrame(num_cols_test.age.values.tolist()).stack().value_counts()


test.index


num_cols_test = replace_cat_list(num_cols_test,'age',test.index)


num_cols_test.info()





import numpy as np

# Importing the SimpleImputer class
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = -1,strategy ='mean')

imputer = imputer.fit(num_cols)
X = imputer.transform(num_cols)


imputer.statistics_


num_cols_imputed= pd.DataFrame(X,columns=num_cols.columns,index=num_cols.index)





import numpy as np
from sklearn.ensemble import VotingRegressor

# Define a custom function that returns the median along axis 1
def median_voting(estimators, X):
    # Get the predicted probabilities or scores from each estimator
    predictions = np.asarray([est.predict(X) for est in estimators])
    # Return the median along axis 1
    return np.median(predictions, axis=0)

# Define the base regressors
reg1 = ...
reg2 = ...
reg3 = ...

# Define the soft voting regressor with median voting
voting_reg = VotingRegressor(estimators=[("reg1", reg1), ("reg2", reg2), ("reg3", reg3)])
# Set the voting method to the custom function
voting_reg.voting = median_voting

# Fit and predict
voting_reg.fit(X_train, y_train)
y_pred = voting_reg.predict(X_test)





from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Lasso

# Define the transformations for each model
svm_transform = ... # some transformation for SVM
rf_transform = ... # some transformation for Random Forest
lasso_transform = ... # some transformation for Lasso

# Define the models
svm_model = SVC()
rf_model = RandomForestClassifier()
lasso_model = Lasso()

# Define the pipelines
svm_pipeline = Pipeline([("transform", svm_transform), ("model", svm_model)])
rf_pipeline = Pipeline([("transform", rf_transform), ("model", rf_model)])
lasso_pipeline = Pipeline([("transform", lasso_transform), ("model", lasso_model)])

# Define the voting classifier
voting_clf = VotingClassifier(estimators=[("svm", svm_pipeline), ("rf", rf_pipeline), ("lasso", lasso_pipeline)])

# Fit and predict
voting_clf.fit(X_train, y_train)
y_pred = voting_clf.predict(X_test)





# we need to analyze the Errors made by the Best Model which is random Forest
rf_clf_predictions = cross_val_predict(rf_clf,x_train,y_train, cv=3,verbose=0)


#create a dataframe contains the predictions 
rf_clf_predictions =  pd.DataFrame(rf_clf_predictions,
                            columns=y_train.columns)

rf_clf_prediction_scores = {score.__name__: predictions_per_col(rf_clf_predictions, y_train, score) 
                for score in [accuracy_score, precision_score, recall_score, f1_score]}

rf_clf_prediction_scores = pd.concat(rf_clf_prediction_scores,axis=1)
print(rf_clf_prediction_scores.mean())
rf_clf_prediction_scores.sort_values("precision_score")


# we need to analyze the Errors made by the Best Model which is random Forest
gd_clf_predictions = cross_val_predict(gd_clf,x_train,y_train, cv=3,verbose=0)


#create a dataframe contains the predictions 
gd_clf_predictions =  pd.DataFrame(gd_clf_predictions,
                            columns=y_train.columns)

gd_clf_prediction_scores = {score.__name__: predictions_per_col(gd_clf_predictions, y_train, score) 
                for score in [accuracy_score, precision_score, recall_score, f1_score]}

gd_clf_prediction_scores = pd.concat(gd_clf_prediction_scores,axis=1)
gd_clf_prediction_scores.sort_values("precision_score")


from sklearn.metrics import multilabel_confusion_matrix

# Assuming y_true and y_pred are your true and predicted labels for multilabel classification
# y_true and y_pred should be binary indicator matrices (0 or 1 values)

confusion_matrices = multilabel_confusion_matrix(y_train, rf_clf_predictions)

# Initialize an empty list to store false positives for each class
false_positives_per_class = []

# Iterate over each confusion matrix and extract false positives
for cm in confusion_matrices:
    # For binary classification, false positives are in the top right corner of the matrix
    false_positives = cm[0, 1]
    false_positives_per_class.append(false_positives)

# false_positives_per_class now contains the number of false positives for each class
print("False Positives per Class:", false_positives_per_class)





y_train.sum(axis=0).sort_values()


# DownSample majority classes and OverSample minority Classes of training set
samples_per_class = 600
resampled_jobs = []

for job in y_train.columns:
    sub_df = y_train.loc[y_train[job] == 1].copy()
    
    if len(sub_df) < samples_per_class:
        # Upsample
        sub_df = sub_df.sample(samples_per_class, replace=True, random_state=42)
    else:
        # Downsample
        sub_df = sub_df.sample(samples_per_class, random_state=42) 
    
    resampled_jobs.append(sub_df)


y_train = pd.concat(resampled_jobs)
x_train = x_train.loc[y_train.index].copy()
y_train.sum(axis=0).sort_values()


# DownSample majority classes and OverSample minority Classes of training set
samples_per_class = 400
resampled_jobs = []

for job in y_test.columns:
    sub_df = y_test.loc[y_test[job] == 1].copy()
    
    if len(sub_df) < samples_per_class:
        # Upsample
        sub_df = sub_df.sample(samples_per_class, replace=True, random_state=42)
    else:
        # Downsample
        sub_df = sub_df.sample(samples_per_class, random_state=42) 
    
    resampled_jobs.append(sub_df)


y_test = pd.concat(resampled_jobs)
x_test = x_test.loc[y_test.index].copy()
y_test.sum(axis=0).sort_values()





artifact_paths = runs["artifact_uri"].str.replace("file:///", "")


clfs = list(range(6))
i = 0
for path in artifact_paths:
    model_pkl = os.path.join(path, LOG_MODEL_PKL)
    with open(model_pkl, "rb") as f:
        model = pickle.load(f)
        clfs[i] = model
    i +=1



voting_clf = clfs[0]['model_object']
cat_clf = clfs[1]['model_object']
gd_clf =clfs[2]['model_object']
dec_clf =clfs[3]['model_object']
rf_clf = clfs[4]['model_object']
log_clf = clfs[5]['model_object']


best_run= runs.sort_values('metrics.test_precision',ascending=False).iloc[0]


best_run


artifact_path = best_run["artifact_uri"].replace("file:///", "")


model_pkl = os.path.join(artifact_path, LOG_MODEL_PKL)
with open(model_pkl, "rb") as f:
    model = pickle.load(f)

model['model_object']





simulated_results = []
for cluster, skills in clusters_config.items():
    additional_skill_prob = model.predict_job_probabilities(set(skills + entry_skills))
    additional_skill_uplift = (additional_skill_prob - base_predictions) / base_predictions
    additional_skill_uplift.name = cluster
    simulated_results.append(additional_skill_uplift)


simulated_results = pd.DataFrame(simulated_results)
simulated_results


target_job = 'Data scientist or machine learning specialist'


target_results = simulated_results[target_job].sort_values(ascending=False)
target_results.head(38)


threshold = 0.0
recommendations = target_results[target_results > threshold].index.tolist()


simulated_results = []
for cluster in recommendations:
    for skill in clusters_config[cluster]:
        additional_skill_prob = model.predict_job_probabilities(set([skill] + entry_skills))
        additional_skill_uplift = (additional_skill_prob - base_predictions) / base_predictions
        additional_skill_uplift.name = skill
        simulated_results.append(additional_skill_uplift)


simulated_results = pd.DataFrame(simulated_results)
simulated_results


target_job = 'Data scientist or machine learning specialist'


target_results = simulated_results[target_job].sort_values(ascending=False)
target_results.head(30)


threshold = 0.10
recommendations = target_results[target_results > threshold].index.tolist()


print("Your current skills: " + str(entry_skills))
print("Your target job: " + str(target_job))
print("You might also consider learning: " + str(recommendations))





# Code Snippet for Creating LDA visualization
def row_tokenizer(col):
    col_tokenized=[]
    for row in col:
        words=[word for word in word_tokenize(row) if len(word) > 2]
        col_tokenized.append(words)
    return col_tokenized
    
def get_lda_objects(col):    
    col_tokenized = create_corpus(col)
    dictionary=gensim.corpora.Dictionary(col_tokenized)
    bow_corpus = [dictionary.doc2bow(doc) for doc in col_tokenized]

    # Creating the object for LDA model using gensim library
    LDA = gensim.models.ldamodel.LdaModel
     
    # Build LDA model
    lda_model = LDA(corpus=bow_corpus, id2word=dictionary, 
                    num_topics=10, random_state=100,
                    chunksize=1000, passes=50,iterations=100)
    
    return lda_model, bow_corpus, dic
    
def plot_lda_vis(lda_model, bow_corpus, dic):
    pyLDAvis.enable_notebook()
    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)
    
    return vis


lda_model, bow_corpus, dic = get_lda_objects(temp_df['review_lemma'])


lda_model.show_topics()


plot_lda_vis(lda_model, bow_corpus, dic)
