




















DATA_PATH = '../Data/Raw/IMDB Dataset.csv'
PREPROCESSED_PATH = "../Data/Processed/preprocessed_df.pkl"

MLFLOW_TRACKING_URI = '../Models/mlruns'
MLFLOW_EXPERIMENT_NAME = "imdb_review_sentiment_analysis"
LOG_PATH = "../Models/temp/"
LOG_DATA_PKL    =  "data.pkl"
LOG_MODEL_PKL   =  "model.pkl"
LOG_METRICS_PKL =  "metrics.pkl"





import os
import numpy as np
import pandas as pd

import logging
import pickle
from pathlib import Path

import re
import string
from nltk.corpus import stopwords

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import plotly.figure_factory as ff
import plotly.io as pio


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

import mlflow
from mlflow.tracking import MlflowClient


# pd.options.display.max_rows = 10000
# pd.options.display.max_columns = 10000





# Function to log Data, Model, Metrics and Track models.
def log_data(x_train,y_train,x_test,y_test):
    # Save the model's dataset trained on
    data_details = {
                    "x_train": x_train,
                    "x_test":x_test,
                    "y_train":y_train,
                    "y_test": y_test
    }

    with open(os.path.join(LOG_PATH, LOG_DATA_PKL), "wb") as output_file:
        pickle.dump(data_details, output_file)
        
        
def log_model(clf,model_description=''):
    # save the model, model details and model's description
    model = {"model_description": model_description,
             "model_details": str(clf),
             "model_object": clf} 

    with open(os.path.join(LOG_PATH, LOG_MODEL_PKL), "wb") as output_file:
        pickle.dump(model, output_file)
        
    return model
        
def log_metrics(train_scores, test_scores):
    # save the model metrics
    classes_metrics = {"train_scores": train_scores,
                        "test_scores" : test_scores} 


    with open(os.path.join(LOG_PATH, LOG_METRICS_PKL), "wb") as output_file:
        pickle.dump(classes_metrics, output_file)

def track_model(model, scores):
    # Start a run in the experiment and track current model
    with mlflow.start_run(experiment_id=exp.experiment_id, run_name=model["model_description"]):
        # Track pickle files
        mlflow.log_artifacts(LOG_PATH)

        # Track metrics 
        for metric, score in scores.items():
            mlflow.log_metric(metric, score)





# Read Dataset and print shape
raw_df = pd.read_csv(DATA_PATH)
raw_df.shape





raw_df.info()





# Check for Duplicates
raw_df.duplicated().value_counts()





# Remove the Duplicates
raw_df = raw_df.drop_duplicates()


# Check whether the dataset is balanced or imbalanced?
raw_df['sentiment'].value_counts()





# Check whether any empty reviews exist
raw_df['length'] = raw_df['review'].apply(len)
print(len(raw_df[raw_df['length'] == 0]))
raw_df = raw_df.drop(columns='length')








df = raw_df.copy()


# Convert text to lowercase
raw_df['review'] = raw_df['review'].str.lower()


# Remove HTML tags
raw_df['review'] = raw_df['review'].apply(lambda x: re.sub('<[^<]+?>', ' ', x))


# Remove Punctuations
raw_df['review'] = raw_df['review'].apply(lambda x: re.sub(r'-', ' ', x))
# raw_df['review'] = raw_df['review'].str.translate(str.maketrans('', '', string.punctuation))
raw_df['review'] = raw_df['review'].apply(lambda x: re.sub(f"[{re.escape(string.punctuation)}]",' ', x))


# Remove Digits
raw_df['review'] = raw_df['review'].apply(lambda x: re.sub(r'\d+', '', x))


# Remove StopWords
stop_words = stopwords.words('english')
raw_df['review'] = raw_df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))





# Verify your Results
i = df.sample(1).index[0]
# i = 100
print(raw_df['review'].iloc[i])
print('###########################################################')
print(df['review'].iloc[i])








from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
raw_df["review"] = raw_df["review"].apply(lambda x: " ".join([stemmer.stem(word) for word in x.split()]))


from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
raw_df["review"] = raw_df["review"].apply(lambda x: " ".join([lemmatizer.lemmatize(word, pos='v') for word in x.split()]))





# First of all we made a corpus of All words 
corpus = []

# Run a Loop and Append Reviews in corpus
for i in range(len(raw_df)):
    # print(i)
    review = re.sub('[^a-zA-Z]', ' ', raw_df['review'].iloc[i])
    review = review.split()
    review = ' '.join(review)
    corpus.append(review)


# Total number of words in corpus
# Initialize total_words counter
total_words = 0

# Iterate through each element in the corpus list
for text in corpus:
    # Split the text into words and update the total_words counter
    total_words += len(text.split())

# Print the total number of words
print(f"Total words in Corpus is : {total_words}")


# Let's find the unique words in the corpus
vocabulary = set()

# Apply vocabulary
for review in corpus:
    # Split the review into words
    words = review.split()
    # Update the vocabulary set with unique words from the review
    vocabulary.update(words)

# Convert the set back to a list if needed
vocabulary = list(vocabulary)


# Lenght of Vocab
print(f'The Lenght of the Vocabulary  is : {len(vocabulary)}')


# Head of Vocab
vocabulary[0:10]


# Intilize 
tf_idf = TfidfVectorizer()

# Fitting
tf = tf_idf.fit_transform(raw_df['review'])


# Len of Vocabulary
print(f"The Lenght of Tf-idf Vocabulary is {len(tf_idf.vocabulary_)}")


# IDF scores of words
idf_scores = tf_idf.idf_

# Print the IDF scores of words and the vocabulary
print("IDF Scores of Words:", idf_scores)


# BAG of words
cv = CountVectorizer(ngram_range=(1,2))
traindata = cv.fit_transform(raw_df['review'])
x = traindata
y = raw_df['sentiment']


x.shape








# Create Directories
Path(MLFLOW_TRACKING_URI).mkdir(parents=True, exist_ok=True)
Path(LOG_PATH).mkdir(parents=True, exist_ok=True)


# Initialize client and experiment
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
client = MlflowClient()
mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)
exp = client.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)





x = raw_df.iloc[0:,0].values
y = raw_df.iloc[0:,1].values


x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = 42)


tf = TfidfVectorizer()
from sklearn.pipeline import Pipeline


from sklearn.linear_model import LogisticRegression
log_clf =LogisticRegression()
model=Pipeline([('vectorizer',tf),('Log_clf',log_clf)])

model.fit(x_train,y_train)


from sklearn.linear_model import LogisticRegression
log_clf = LogisticRegression(max_iter=10000)
log_clf.fit(x_train,y_train)


y_pred=log_clf.predict(x_test)


# model score
accuracy=accuracy_score(y_pred,y_test)
print(accuracy)


# confusion matrix
cm=confusion_matrix(y_test,y_pred)
print(cm)


recall= recall_score(y_test, y_pred, average="binary", pos_label="negative")
precision = precision_score(y_test, y_pred, average="binary", pos_label="negative")
f1 = f1_score(y_test, y_pred, average="binary", pos_label="negative")
print(f"precision: {precision}, Recall: {recall}, F1: {f1}")


scores={'accuracy' : accuracy,
        'precision' : precision,
        'recall' : recall,
        'f1': f1}


scores


# Log the model's dataset train and test indices
log_data(x_train,y_train,x_test,y_test)
# Log the model, model description
model = log_model(log_clf,'Logistic Regression, BOW, 1,2 bigram, with stopwords ')
# Log the model's train and test scores
log_metrics(scores, scores)
# track the model artifacts, validation scores with mlflow
track_model(model,scores)





runs = mlflow.search_runs([exp.experiment_id])
runs[['run_id','tags.mlflow.runName','metrics.precision','metrics.recall','metrics.f1','metrics.accuracy']]





raw_df





raw_df.describe()


raw_df.loc[raw_df['length'] == 32].iloc[0,0]














#word cloud for positive review words
plt.figure(figsize=(10,10))
positive_text=norm_train_reviews[1]
WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)
positive_words=WC.generate(positive_text)
plt.imshow(positive_words,interpolation='bilinear')
plt.show


MultinomialNB()



