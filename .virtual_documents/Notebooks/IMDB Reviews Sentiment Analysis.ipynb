





























DATA_PATH = '../Data/Raw/IMDB Dataset.csv'
PREPROCESSED_PATH = "../Data/Processed/preprocessed_df.pkl"

MLFLOW_TRACKING_URI = '../Models/mlruns'
MLFLOW_EXPERIMENT_NAME = "imdb_review_sentiment_analysis"
LOG_PATH = "../Models/temp/"
LOG_DATA_PKL    =  "data.pkl"
LOG_MODEL_PKL   =  "model.pkl"
LOG_METRICS_PKL =  "metrics.pkl"





import os
import numpy as np
import pandas as pd

import logging
import pickle
from pathlib import Path

import re
import string
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

import mlflow
from mlflow.tracking import MlflowClient


# pd.options.display.max_rows = 10000
# pd.options.display.max_columns = 10000





# Function to log Data, Model, Metrics and Track models.
def log_data(x_train,y_train,x_test,y_test):
    # Save the model's dataset trained on
    data_details = {
                    "x_train": x_train,
                    "x_test": x_test,
                    "y_train": y_train,
                    "y_test": y_test
    }

    with open(os.path.join(LOG_PATH, LOG_DATA_PKL), "wb") as output_file:
        pickle.dump(data_details, output_file)
        
        
def log_model(clf,model_description=''):
    # save the model, model details and model's description
    model = {"model_description": model_description,
             "model_details": str(clf),
             "model_object": clf} 

    with open(os.path.join(LOG_PATH, LOG_MODEL_PKL), "wb") as output_file:
        pickle.dump(model, output_file)
        
    return model
        
def log_metrics(train_scores, test_scores):
    # save the model metrics
    classes_metrics = {"train_scores": train_scores,
                        "test_scores" : test_scores} 


    with open(os.path.join(LOG_PATH, LOG_METRICS_PKL), "wb") as output_file:
        pickle.dump(classes_metrics, output_file)

def track_model(model, scores):
    # Start a run in the experiment and track current model
    with mlflow.start_run(experiment_id=exp.experiment_id, run_name=model["model_description"]):
        # Track pickle files
        mlflow.log_artifacts(LOG_PATH)

        # Track metrics 
        for metric, score in scores.items():
            mlflow.log_metric(metric, score)


# count number of characters 
def chars_count(text):
    return len(text)


# count number of words 
def words_count(text):
    return len(text.split())


# count number of capital words
def capital_words_count(text):
    return sum(map(str.isupper,text.split()))


# count number of punctuations
def punctuations_count(text):
    punctuations='!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d


# count number of words in quotes
def words_in_quotes_count(text):
    x = re.findall("\'.\'|\".\"", text)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count


# count number of sentences
def sent_count(text):
    return len(sent_tokenize(text))


# calculate average word length
def avg_word_len(char_count,word_count):
    return char_count/word_count


# calculate average sentence length
def avg_sent_len(word_count,sent_count):
    return word_count/sent_count


# count number of unique words 
def unique_words_count(text):
    return len(set(text.split()))


# Calculate the percentage of unique words
def unique_words_percent(unique_count,words_count):
    return unique_count/words_count


# count of stopwords
def stopwords_count(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)


# stopwords vs words
def stopwords_percent(stopwords_count,text):
    return stopwords_count/len(word_tokenize(text))





# Read Dataset and print shape
raw_df = pd.read_csv(DATA_PATH)
raw_df.shape





raw_df.info()





# Check for Duplicates
raw_df.duplicated().value_counts()





# Remove the Duplicates
raw_df = raw_df.drop_duplicates()


# Check whether the dataset is balanced or imbalanced?
raw_df['sentiment'].value_counts()





# Check whether any empty reviews exist
raw_df['length'] = raw_df['review'].apply(len)
print(len(raw_df[raw_df['length'] == 0]))
raw_df = raw_df.drop(columns='length')








df = raw_df.copy()


# Convert text to lowercase
raw_df['review_cleaned'] = raw_df['review'].str.lower()


# Remove HTML tags
raw_df['review_cleaned'] = raw_df['review_cleaned'].apply(lambda x: re.sub('<[^<]+?>', ' ', x))


# Remove Punctuations
# raw_df['review'] = raw_df['review'].str.translate(str.maketrans('', '', string.punctuation))
raw_df['review_cleaned'] = raw_df['review_cleaned'].apply(lambda x: re.sub(f"[{re.escape(string.punctuation)}]",' ', x))


# Remove Digits
raw_df['review_cleaned'] = raw_df['review_cleaned'].apply(lambda x: re.sub(r'\d+', '', x))


# Remove URLs
raw_df['review_cleaned'] = raw_df['review_cleaned'].apply(lambda x: re.sub(r'https?://\S+|www\.\S+', '', x))


# Create new column with No StopWords
stop_words = stopwords.words('english')
raw_df['review_nostopwords'] = raw_df['review_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))


# Create new column for stemmed words
stemmer = PorterStemmer()
raw_df["review_stemmed"] = raw_df["review_nostopwords"].apply(lambda x: " ".join([stemmer.stem(word) for word in x.split()]))


# Create new column for lemmatized words 
#Load English tokenizer, tagger, parser and NER
nlp = spacy.load("en_core_web_sm")
# Process the text using spaCy and Extract lemmatized tokens
raw_df['review_lemma'] = raw_df["review_nostopwords"].apply(lambda x: " ".join([word.lemma_ for word in nlp(x)]))





# Verify your Results
i = df.sample(1).index[0]
# i = 100
print(raw_df['review_lemma'].iloc[i])
print('###########################################################')
print(df['review'].iloc[i])


# Export the preprocessed dataset with pickle
raw_df.to_pickle(PREPROCESSED_PATH)


prep_df = pd.read_pickle(PREPROCESSED_PATH)








df['char_count'] = df["tweet"].apply(lambda x:count_chars(x))
df['word_count'] = df["tweet"].apply(lambda x:count_words(x))
df['sent_count'] = df["tweet"].apply(lambda x:count_sent(x))
df['capital_char_count'] = df["tweet"].apply(lambda x:count_capital_chars(x))
df['capital_word_count'] = df["tweet"].apply(lambda x:count_capital_words(x))
df['quoted_word_count'] = df["tweet"].apply(lambda x:count_words_in_quotes(x))
df['stopword_count'] = df["tweet"].apply(lambda x:count_stopwords(x))
df['unique_word_count'] = df["tweet"].apply(lambda x:count_unique_words(x))
df['htag_count'] = df["tweet"].apply(lambda x:count_htags(x))
df['mention_count'] = df["tweet"].apply(lambda x:count_mentions(x))
df['punct_count'] = df["tweet"].apply(lambda x:count_punctuations(x))
df['avg_wordlength']=df['char_count']/df['word_count']
df['avg_sentlength']=df['word_count']/df['sent_count']
df['unique_vs_words']=df['unique_word_count']/df['word_count']
df['stopwords_vs_words']=df['stopword_count']/df['word_count']





# Intilize 
tf_idf = TfidfVectorizer(ngram_range=(1,2))

# Fitting
tf = tf_idf.fit_transform(raw_df['review_lemma'])


# Len of Vocabulary
print(f"The Lenght of Tf-idf Vocabulary is {len(tf_idf.vocabulary_)}")


x = tf
y = raw_df['sentiment']


# IDF scores of words
idf_scores = tf_idf.idf_

# Print the IDF scores of words and the vocabulary
print("IDF Scores of Words:", idf_scores)


# BAG of words
cv = CountVectorizer(ngram_range=(1,2))
traindata = cv.fit_transform(raw_df['review'])
x = traindata
y = raw_df['sentiment']


x.shape








# Create Directories
Path(MLFLOW_TRACKING_URI).mkdir(parents=True, exist_ok=True)
Path(LOG_PATH).mkdir(parents=True, exist_ok=True)


# Initialize client and experiment
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
client = MlflowClient()
mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)
exp = client.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)





x = raw_df.iloc[0:,0].values
y = raw_df.iloc[0:,1].values


x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = 42)


tf = TfidfVectorizer()
from sklearn.pipeline import Pipeline


from sklearn.linear_model import LogisticRegression
log_clf =LogisticRegression()
model=Pipeline([('vectorizer',tf),('Log_clf',log_clf)])

model.fit(x_train,y_train)


from sklearn.linear_model import LogisticRegression
log_clf = LogisticRegression(max_iter=10000)
log_clf.fit(x_train,y_train)


y_pred=log_clf.predict(x_test)


# model score
accuracy=accuracy_score(y_pred,y_test)
print(accuracy)


# confusion matrix
cm=confusion_matrix(y_test,y_pred)
print(cm)


recall= recall_score(y_test, y_pred, average="binary", pos_label="negative")
precision = precision_score(y_test, y_pred, average="binary", pos_label="negative")
f1 = f1_score(y_test, y_pred, average="binary", pos_label="negative")
print(f"precision: {precision}, Recall: {recall}, F1: {f1}")


scores={'accuracy' : accuracy,
        'precision' : precision,
        'recall' : recall,
        'f1': f1}


scores


# Log the model's dataset train and test indices
log_data(x_train,y_train,x_test,y_test)
# Log the model, model description
model = log_model(log_clf,'Log ReG, BOW, 1,2 bigram')
# Log the model's train and test scores
log_metrics(scores, scores)
# track the model artifacts, validation scores with mlflow
track_model(model,scores)





runs = mlflow.search_runs([exp.experiment_id])
runs[['run_id','tags.mlflow.runName','metrics.precision','metrics.recall','metrics.f1','metrics.accuracy']]


runs['tags.mlflow.runName'].iloc[5]





raw_df





raw_df.describe()


raw_df.loc[raw_df['length'] == 32].iloc[0,0]











#word cloud for positive review words
plt.figure(figsize=(10,10))
positive_text=norm_train_reviews[1]
WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)
positive_words=WC.generate(positive_text)
plt.imshow(positive_words,interpolation='bilinear')
plt.show


MultinomialNB()



