{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33318e-5e44-408c-87fb-395fad70db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSV files list from a folder\n",
    "csv_files = glob.glob(DATA_PATH + \"/*\")\n",
    "# load the dataframes and concatenate them\n",
    "raw_df = pd.concat(map(pd.read_csv, csv_files))\n",
    "raw_df = raw_df.set_index('Unnamed: 0').reset_index(drop=True)\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eede67b-f12b-4bb1-98e7-ff013762b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('../Data/Processed/0_DataCleaned_df.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a76709-abca-4934-9921-280e97a38e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d8094-d95f-44bd-bb5c-ce186ca279a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Meta                         17\n",
       "DSMH LLC                      9\n",
       "PayPal                        9\n",
       "Disney                        7\n",
       "Notion                        7\n",
       "                             ..\n",
       "Invisible Technologies        1\n",
       "BJ Services                   1\n",
       "Cybotic System                1\n",
       "Infosys                       1\n",
       "Toyota Research Institute     1\n",
       "Name: company_name, Length: 874, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['company_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc4ef8-dbee-46e1-b002-23f452a4e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['comp_count'] = test.groupby('company_name')['company_name'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa31e8f-40cc-4cdb-ad76-90e13759d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['companyname'] = np.where(test['comp_count'] >= 3, test['company_name'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e735f2d-e836-465b-98c7-55a0dd1ec902",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['statecount'] = test.groupby('job_state')['job_state'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270cce8b-509a-4e45-9684-acd45340b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['jobstate'] = np.where(test['statecount'] >= 3, test['job_state'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480029b8-9f75-4c41-a69a-39e19fa9e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['indcount'] = test.groupby('Industry')['Industry'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1daab23-d139-44e3-8608-6de0e67e5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['industry'] = np.where(test['indcount'] >= 4, test['Industry'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe6d88-e1bd-4dbe-b71f-533ce0529107",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.replace(to_replace={-1: np.NAN,'-1':np.NAN},inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578a376-1e7a-45d4-83eb-a2aab5fd5dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Information Technology Support Services    128\n",
       "Internet & Web Services                     87\n",
       "Enterprise Software & Network Solutions     83\n",
       "Health Care Services & Hospitals            83\n",
       "Computer Hardware Development               68\n",
       "Business Consulting                         40\n",
       "Banking & Lending                           33\n",
       "Software Development                        32\n",
       "Biotech & Pharmaceuticals                   28\n",
       "Insurance Carriers                          26\n",
       "Investment & Asset Management               22\n",
       "Advertising & Public Relations              22\n",
       "Energy & Utilities                          21\n",
       "Financial Transaction Processing            18\n",
       "Consumer Product Manufacturing              18\n",
       "Aerospace & Defense                         15\n",
       "Colleges & Universities                     12\n",
       "Film Production                             11\n",
       "HR Consulting                               11\n",
       "Food & Beverage Manufacturing               10\n",
       "Civic & Social Services                      9\n",
       "Transportation Equipment Manufacturing       9\n",
       "Staffing & Subcontracting                    8\n",
       "Other Retail Stores                          8\n",
       "Insurance Agencies & Brokerages              8\n",
       "Machinery Manufacturing                      7\n",
       "Telecommunications Services                  7\n",
       "Construction                                 7\n",
       "National Agencies                            7\n",
       "Architectural & Engineering Services         7\n",
       "Research & Development                       7\n",
       "Shipping & Trucking                          6\n",
       "Wholesale                                    6\n",
       "Video Game Publishing                        6\n",
       "Hotels & Resorts                             6\n",
       "Department, Clothing & Shoe Stores           6\n",
       "Primary & Secondary Schools                  5\n",
       "Electronics Manufacturing                    5\n",
       "Home Furniture & Housewares Stores           5\n",
       "Real Estate                                  4\n",
       "Taxi & Car Services                          4\n",
       "Publishing                                   4\n",
       "Education & Training Services                4\n",
       "Legal                                        4\n",
       "Chemical Manufacturing                       4\n",
       "Accounting & Tax                             4\n",
       "Drug & Health Stores                         4\n",
       "Name: industry, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['industry'].value_counts().head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87a227-a159-4460-b3c5-4a7669f3a1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1217 entries, 0 to 1216\n",
      "Data columns (total 32 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Job Title          1217 non-null   object \n",
      " 1   Salary Estimate    1217 non-null   object \n",
      " 2   Job Description    1217 non-null   object \n",
      " 3   Rating             1041 non-null   float64\n",
      " 4   Company Name       1217 non-null   object \n",
      " 5   Location           1217 non-null   object \n",
      " 6   Size               1052 non-null   object \n",
      " 7   Founded            869 non-null    float64\n",
      " 8   Type of ownership  1126 non-null   object \n",
      " 9   Industry           978 non-null    object \n",
      " 10  Sector             978 non-null    object \n",
      " 11  Revenue            680 non-null    object \n",
      " 12  hourly             1217 non-null   int64  \n",
      " 13  employer_provided  1217 non-null   int64  \n",
      " 14  min_salary         1217 non-null   int64  \n",
      " 15  max_salary         1217 non-null   int64  \n",
      " 16  avg_salary         1217 non-null   float64\n",
      " 17  job_title          1217 non-null   object \n",
      " 18  seniority          1217 non-null   object \n",
      " 19  desc_len           1217 non-null   int64  \n",
      " 20  tools              1217 non-null   int64  \n",
      " 21  techs              1217 non-null   int64  \n",
      " 22  education          1217 non-null   int64  \n",
      " 23  company_name       1217 non-null   object \n",
      " 24  job_state          1217 non-null   object \n",
      " 25  age                869 non-null    float64\n",
      " 26  comp_count         1217 non-null   int64  \n",
      " 27  companyname        270 non-null    object \n",
      " 28  statecount         1217 non-null   int64  \n",
      " 29  jobstate           1202 non-null   object \n",
      " 30  indcount           978 non-null    float64\n",
      " 31  industry           929 non-null    object \n",
      "dtypes: float64(5), int64(10), object(17)\n",
      "memory usage: 304.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aad8f3-0415-44dc-981e-4a24a18a8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns=['indcount','statecount','comp_count','job_state','company_name','Industry'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90513826-196e-4932-b7b2-625a39de78b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1217 entries, 0 to 1216\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Job Title          1217 non-null   object \n",
      " 1   Salary Estimate    1217 non-null   object \n",
      " 2   Job Description    1217 non-null   object \n",
      " 3   Rating             1041 non-null   float64\n",
      " 4   Company Name       1217 non-null   object \n",
      " 5   Location           1217 non-null   object \n",
      " 6   Size               1052 non-null   object \n",
      " 7   Founded            869 non-null    float64\n",
      " 8   Type of ownership  1126 non-null   object \n",
      " 9   Sector             978 non-null    object \n",
      " 10  Revenue            680 non-null    object \n",
      " 11  hourly             1217 non-null   int64  \n",
      " 12  employer_provided  1217 non-null   int64  \n",
      " 13  min_salary         1217 non-null   int64  \n",
      " 14  max_salary         1217 non-null   int64  \n",
      " 15  avg_salary         1217 non-null   float64\n",
      " 16  job_title          1217 non-null   object \n",
      " 17  seniority          1217 non-null   object \n",
      " 18  desc_len           1217 non-null   int64  \n",
      " 19  tools              1217 non-null   int64  \n",
      " 20  techs              1217 non-null   int64  \n",
      " 21  education          1217 non-null   int64  \n",
      " 22  age                869 non-null    float64\n",
      " 23  companyname        270 non-null    object \n",
      " 24  jobstate           1202 non-null   object \n",
      " 25  industry           929 non-null    object \n",
      "dtypes: float64(4), int64(8), object(14)\n",
      "memory usage: 247.3+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7310e2-b2df-4cf2-9698-5dc4ed773b06",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### a. onehot, simpleimputer, stdscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe9b05e6-0dc8-43bb-ad62-4d2922c74252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I had to do onehotencoding before splitting the data, because we have too many categories, it will raise error\n",
    "# when transforming the test set\n",
    "\n",
    "df_dummy = pd.get_dummies(df_model)\n",
    "df_dummy['avg_salaries_cat'] = pd.cut(df_dummy['avg_salary'], bins=[0.,75,100,135,170,np.inf],\n",
    "                                      labels=[0,1,2,3,4])\n",
    "split = StratifiedShuffleSplit(n_splits =1, test_size =0.2, random_state=42)\n",
    "for train_index, test_index in split.split(df_dummy,df_dummy['avg_salaries_cat']):\n",
    "    strat_train_set_dum = df_dummy.loc[train_index]\n",
    "    strat_test_set_dum = df_dummy.loc[test_index]\n",
    "\n",
    "# drop the avg_salaries_cat so the data is back to its original state\n",
    "for strat_set in (strat_train_set_dum,strat_test_set_dum,df_dummy):\n",
    "    strat_set.drop(\"avg_salaries_cat\",axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eddc2418-b515-4e1c-8904-04cbd951754f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_salary</th>\n",
       "      <th>Rating</th>\n",
       "      <th>age</th>\n",
       "      <th>tools</th>\n",
       "      <th>techs</th>\n",
       "      <th>education</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>hourly</th>\n",
       "      <th>employer_provided</th>\n",
       "      <th>Size_-1</th>\n",
       "      <th>...</th>\n",
       "      <th>company_name_eTek IT Services, Inc.</th>\n",
       "      <th>company_name_eimagine</th>\n",
       "      <th>company_name_fairlife, LLC</th>\n",
       "      <th>company_name_gointellects</th>\n",
       "      <th>company_name_iSpot.tv</th>\n",
       "      <th>company_name_inAssist</th>\n",
       "      <th>company_name_kea</th>\n",
       "      <th>company_name_keasis Inc</th>\n",
       "      <th>company_name_shaped.ai Inc.</th>\n",
       "      <th>company_name_zettalogix.Inc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>187.5</td>\n",
       "      <td>3.7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3144</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8120</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1338</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>217</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5372</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>828</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>121.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>94</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>167.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3769</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>159.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2988</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>221.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7039</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>165.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8046</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1217 rows × 1065 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      avg_salary  Rating  age  tools  techs  education  desc_len  hourly  \\\n",
       "0          187.5     3.7   10      3      2          2      3144       0   \n",
       "1          136.0     4.3   16      3      6          0      8120       0   \n",
       "2          100.0    -1.0   -1      4      2          0      1338       0   \n",
       "3           98.0     4.2  217      7      7          1      5372       0   \n",
       "4          117.0     4.6   -1      3      0          0       828       1   \n",
       "...          ...     ...  ...    ...    ...        ...       ...     ...   \n",
       "1212       121.0     3.9   94      5      5          2      6439       0   \n",
       "1213       167.0     3.9   12      3      5          1      3769       0   \n",
       "1214       159.0     3.9  100      2      3          2      2988       0   \n",
       "1215       221.5     3.2   12      2      4          0      7039       0   \n",
       "1216       165.5     3.9   25      8      5          1      8046       0   \n",
       "\n",
       "      employer_provided  Size_-1  ...  company_name_eTek IT Services, Inc.  \\\n",
       "0                     1        0  ...                                    0   \n",
       "1                     1        0  ...                                    0   \n",
       "2                     1        1  ...                                    0   \n",
       "3                     1        0  ...                                    0   \n",
       "4                     1        0  ...                                    0   \n",
       "...                 ...      ...  ...                                  ...   \n",
       "1212                  0        0  ...                                    0   \n",
       "1213                  1        0  ...                                    0   \n",
       "1214                  1        0  ...                                    0   \n",
       "1215                  1        0  ...                                    0   \n",
       "1216                  1        0  ...                                    0   \n",
       "\n",
       "      company_name_eimagine  company_name_fairlife, LLC  \\\n",
       "0                         0                           0   \n",
       "1                         0                           0   \n",
       "2                         0                           0   \n",
       "3                         0                           0   \n",
       "4                         0                           0   \n",
       "...                     ...                         ...   \n",
       "1212                      0                           0   \n",
       "1213                      0                           0   \n",
       "1214                      0                           0   \n",
       "1215                      0                           0   \n",
       "1216                      0                           0   \n",
       "\n",
       "      company_name_gointellects  company_name_iSpot.tv  company_name_inAssist  \\\n",
       "0                             0                      0                      0   \n",
       "1                             0                      0                      0   \n",
       "2                             0                      0                      0   \n",
       "3                             0                      0                      0   \n",
       "4                             0                      0                      0   \n",
       "...                         ...                    ...                    ...   \n",
       "1212                          0                      0                      0   \n",
       "1213                          0                      0                      0   \n",
       "1214                          0                      0                      0   \n",
       "1215                          0                      0                      0   \n",
       "1216                          0                      0                      0   \n",
       "\n",
       "      company_name_kea  company_name_keasis Inc  company_name_shaped.ai Inc.  \\\n",
       "0                    0                        0                            0   \n",
       "1                    0                        0                            0   \n",
       "2                    0                        0                            0   \n",
       "3                    0                        0                            0   \n",
       "4                    0                        0                            0   \n",
       "...                ...                      ...                          ...   \n",
       "1212                 0                        0                            0   \n",
       "1213                 0                        0                            0   \n",
       "1214                 0                        0                            0   \n",
       "1215                 0                        0                            0   \n",
       "1216                 0                        0                            0   \n",
       "\n",
       "      company_name_zettalogix.Inc  \n",
       "0                               0  \n",
       "1                               0  \n",
       "2                               0  \n",
       "3                               0  \n",
       "4                               0  \n",
       "...                           ...  \n",
       "1212                            0  \n",
       "1213                            0  \n",
       "1214                            0  \n",
       "1215                            0  \n",
       "1216                            0  \n",
       "\n",
       "[1217 rows x 1065 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0778c55d-b660-4ff1-8eae-eb813930576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dum = strat_train_set_dum.drop(\"avg_salary\",axis=1)\n",
    "y_train_dum = strat_train_set_dum['avg_salary'].copy()\n",
    "\n",
    "num_attribs_dum = list(num_cols.columns)\n",
    "cat_attribs_dum = list(x_train_dum.iloc[:,8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448741c-946c-46b3-81e9-640963fc672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(missing_values=[np.NaN,-1,'-1'],strategy=\"mean\")),\n",
    "    ('min_max_scaler', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "# full_pipeline = ColumnTransformer([\n",
    "#     (\"num\", num_pipeline , num_attribs),\n",
    "#     (\"cat\", OneHotEncoder(handle_unknown = \"ignore\"), cat_attribs),\n",
    "# ])\n",
    "\n",
    "model_prepared = num_pipeline.fit_transform(x_train_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51144028-1eae-424d-b18f-10b3f39cfadc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86666667, 0.21907216, 0.2       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.76666667, 0.        , 0.2       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.91666667, 0.        , 0.1       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.55      , 0.        , 0.1       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.83333333, 0.29639175, 0.1       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.83333333, 0.06958763, 0.3       , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a645899-8240-4a7c-ace4-eb94d4466fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c639e3da-ba88-4594-b193-e70d04570e2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imputation using KNN neighbors + feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb3aaaf9-410f-4dbe-a317-6f42b2bd4ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols_copy = cat_cols.copy()\n",
    "num_cols_copy = num_cols.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a4a6386-b525-430b-b654-7d892f1f6a63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols_copy.replace(to_replace=['-1'],value=np.NAN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15c55c89-a44b-4880-aa16-218fd9899fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols_copy.replace(to_replace=[-1],value=np.NAN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "636285df-ea5f-4fdf-af1b-479adc5a3c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 973 entries, 0 to 972\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Size               847 non-null    object\n",
      " 1   Type of ownership  907 non-null    object\n",
      " 2   Industry           794 non-null    object\n",
      " 3   Sector             794 non-null    object\n",
      " 4   Revenue            549 non-null    object\n",
      " 5   job_title          973 non-null    object\n",
      " 6   job_state          973 non-null    object\n",
      " 7   seniority          973 non-null    object\n",
      " 8   company_name       973 non-null    object\n",
      "dtypes: object(9)\n",
      "memory usage: 68.5+ KB\n"
     ]
    }
   ],
   "source": [
    "cat_cols_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b4936cf-a044-41a4-92d4-98b372fdd9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 973 entries, 0 to 972\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Rating             843 non-null    float64\n",
      " 1   age                706 non-null    float64\n",
      " 2   tools              973 non-null    int64  \n",
      " 3   techs              973 non-null    int64  \n",
      " 4   education          973 non-null    int64  \n",
      " 5   desc_len           973 non-null    int64  \n",
      " 6   hourly             973 non-null    int64  \n",
      " 7   employer_provided  973 non-null    int64  \n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 60.9 KB\n"
     ]
    }
   ],
   "source": [
    "num_cols_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1b67421-4361-40a0-a2e8-8124aa4022dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To perform KNN imputation, we need to normalize the input data and perform One Hot Encoding to categorical variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "cat_dummies = pd.get_dummies(cat_cols_copy, drop_first = True)\n",
    "df_copy= pd.concat([num_cols_copy, cat_dummies], axis = 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "minmaxscaler = pd.DataFrame(scaler.fit_transform(df_copy), columns = df_copy.columns)\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors = 3)\n",
    "knn_imputed_df = pd.DataFrame(imputer.fit_transform(minmaxscaler),columns = df_copy.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53173d4c-192e-4201-9e94-f177a8e9274f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>age</th>\n",
       "      <th>tools</th>\n",
       "      <th>techs</th>\n",
       "      <th>education</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>hourly</th>\n",
       "      <th>employer_provided</th>\n",
       "      <th>Size_10000+ Employees</th>\n",
       "      <th>Size_1001 to 5000 Employees</th>\n",
       "      <th>...</th>\n",
       "      <th>company_name_eTek IT Services, Inc.</th>\n",
       "      <th>company_name_eimagine</th>\n",
       "      <th>company_name_fairlife, LLC</th>\n",
       "      <th>company_name_gointellects</th>\n",
       "      <th>company_name_iSpot.tv</th>\n",
       "      <th>company_name_inAssist</th>\n",
       "      <th>company_name_kea</th>\n",
       "      <th>company_name_keasis Inc</th>\n",
       "      <th>company_name_shaped.ai Inc.</th>\n",
       "      <th>company_name_zettalogix.Inc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.215026</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.050086</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.380393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.044905</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.139896</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.041451</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.116580</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.300518</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.028497</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.292746</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.064767</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.310190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>973 rows × 913 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rating       age  tools     techs  education  desc_len  hourly  \\\n",
       "0     0.800  0.215026    0.2  0.222222   0.000000  0.062626     0.0   \n",
       "1     0.650  0.050086    0.2  0.333333   0.666667  0.380393     0.0   \n",
       "2     0.875  0.044905    0.1  0.000000   0.000000  0.064559     0.0   \n",
       "3     0.775  0.139896    0.2  0.444444   0.000000  0.192825     0.0   \n",
       "4     0.725  0.041451    0.5  0.444444   0.000000  0.215556     0.0   \n",
       "..      ...       ...    ...       ...        ...       ...     ...   \n",
       "968   0.800  0.116580    0.1  0.111111   0.000000  0.080795     0.0   \n",
       "969   0.700  0.300518    0.1  0.000000   0.000000  0.014226     0.0   \n",
       "970   0.325  0.028497    0.1  0.111111   0.000000  0.044688     0.0   \n",
       "971   0.750  0.292746    0.1  0.111111   0.000000  0.059146     0.0   \n",
       "972   0.750  0.064767    0.3  0.222222   0.333333  0.310190     0.0   \n",
       "\n",
       "     employer_provided  Size_10000+ Employees  Size_1001 to 5000 Employees  \\\n",
       "0                  0.0                    1.0                          0.0   \n",
       "1                  0.0                    0.0                          0.0   \n",
       "2                  0.0                    0.0                          0.0   \n",
       "3                  1.0                    1.0                          0.0   \n",
       "4                  1.0                    0.0                          1.0   \n",
       "..                 ...                    ...                          ...   \n",
       "968                1.0                    0.0                          1.0   \n",
       "969                1.0                    1.0                          0.0   \n",
       "970                1.0                    0.0                          0.0   \n",
       "971                0.0                    1.0                          0.0   \n",
       "972                0.0                    0.0                          1.0   \n",
       "\n",
       "     ...  company_name_eTek IT Services, Inc.  company_name_eimagine  \\\n",
       "0    ...                                  0.0                    0.0   \n",
       "1    ...                                  0.0                    0.0   \n",
       "2    ...                                  0.0                    0.0   \n",
       "3    ...                                  0.0                    0.0   \n",
       "4    ...                                  0.0                    0.0   \n",
       "..   ...                                  ...                    ...   \n",
       "968  ...                                  0.0                    0.0   \n",
       "969  ...                                  0.0                    0.0   \n",
       "970  ...                                  0.0                    0.0   \n",
       "971  ...                                  0.0                    0.0   \n",
       "972  ...                                  0.0                    0.0   \n",
       "\n",
       "     company_name_fairlife, LLC  company_name_gointellects  \\\n",
       "0                           0.0                        0.0   \n",
       "1                           0.0                        0.0   \n",
       "2                           0.0                        0.0   \n",
       "3                           0.0                        0.0   \n",
       "4                           0.0                        0.0   \n",
       "..                          ...                        ...   \n",
       "968                         0.0                        0.0   \n",
       "969                         0.0                        0.0   \n",
       "970                         0.0                        0.0   \n",
       "971                         0.0                        0.0   \n",
       "972                         0.0                        0.0   \n",
       "\n",
       "     company_name_iSpot.tv  company_name_inAssist  company_name_kea  \\\n",
       "0                      0.0                    0.0               0.0   \n",
       "1                      0.0                    0.0               0.0   \n",
       "2                      0.0                    0.0               0.0   \n",
       "3                      0.0                    0.0               0.0   \n",
       "4                      0.0                    0.0               0.0   \n",
       "..                     ...                    ...               ...   \n",
       "968                    0.0                    0.0               0.0   \n",
       "969                    0.0                    0.0               0.0   \n",
       "970                    0.0                    0.0               0.0   \n",
       "971                    0.0                    0.0               0.0   \n",
       "972                    0.0                    0.0               0.0   \n",
       "\n",
       "     company_name_keasis Inc  company_name_shaped.ai Inc.  \\\n",
       "0                        0.0                          0.0   \n",
       "1                        0.0                          0.0   \n",
       "2                        0.0                          0.0   \n",
       "3                        0.0                          0.0   \n",
       "4                        0.0                          0.0   \n",
       "..                       ...                          ...   \n",
       "968                      0.0                          0.0   \n",
       "969                      0.0                          0.0   \n",
       "970                      0.0                          0.0   \n",
       "971                      0.0                          0.0   \n",
       "972                      0.0                          0.0   \n",
       "\n",
       "     company_name_zettalogix.Inc  \n",
       "0                            0.0  \n",
       "1                            0.0  \n",
       "2                            0.0  \n",
       "3                            0.0  \n",
       "4                            0.0  \n",
       "..                           ...  \n",
       "968                          0.0  \n",
       "969                          0.0  \n",
       "970                          0.0  \n",
       "971                          0.0  \n",
       "972                          0.0  \n",
       "\n",
       "[973 rows x 913 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926a76e-e038-4624-a5d8-960383b8d12f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### we will perform imputation by keeping the percentage of each category in the population constant even after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "93e6d302-e947-42b3-ba45-8785e6165537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols_test=cat_cols.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "681d4216-f38e-49dd-b901-de92255978df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Revenue Column\n",
    "cat_cols_test['Revenue'].replace(to_replace=['-1'],value=np.NAN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3e05d1d6-d901-45d7-aaee-dec3b584619f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(cat_cols_test.Revenue.values.tolist()).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9424f283-46a3-428c-83db-af2112a8a974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols_test = replace_cat_list(cat_cols_test,'Revenue',test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b58e657c-ab1c-49c3-8787-d7d98a9c7889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# type of Ownership Column\n",
    "cat_cols_test['Type of ownership'].replace(to_replace=['-1'],value=np.NAN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d4ceb3a8-0384-4980-9946-9ffcc354aeb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(cat_cols_test['Type of ownership'].values.tolist()).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "13c6849e-e62a-41a2-826f-f944c149429b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols_test = replace_cat_list(cat_cols_test,'Type of ownership',test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7eab788e-c6c3-4871-8fc8-4d279aa00190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Industry Column\n",
    "cat_cols_test['Industry'].replace(to_replace=['-1'],value=np.NAN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1539daa7-e3c2-488d-8521-196926106545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(cat_cols_test.Industry.values.tolist()).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bf407660-33d4-4955-ac1d-fc4a2a360186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols_test = replace_cat_list(cat_cols_test,'Industry',test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0e0a1ab5-9fd1-42bb-b47f-d08279eafd48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sector column\n",
    "cat_cols_test['Sector'].replace(to_replace=['-1'],value=np.NAN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6a3ef1eb-d08b-419a-b8bb-85724f5cbebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(cat_cols_test.Sector.values.tolist()).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "743dc821-172d-418f-bab0-ccae75b42149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols_test = replace_cat_list(cat_cols_test,'Sector',test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "51a5e2b4-139a-4412-8db0-efbd333fd234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Size Column\n",
    "cat_cols_test['Size'].replace(to_replace=['-1'],value=np.NAN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "026f04af-f458-4b81-af0e-f1e8d2cefb7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(cat_cols_test.Size.values.tolist()).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0253b05b-a885-4aa7-9b41-91a0c2c753ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols_test = replace_cat_list(cat_cols_test,'Size',test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e263e6-918c-4d0f-8e01-f44d2424927a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### we will perform imputation by keeping the percentage of each category in the population constant even after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "83374da1-adf0-4a76-a852-481198bcf321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_cat_list(df,col,cat_list):\n",
    "    count_cat_dict_initial = {'Total_cat':0}\n",
    "    for cat in cat_list:\n",
    "        count_cat_dict_initial[cat] = df.loc[df[col]==cat,col].count()\n",
    "        count_cat_dict_initial['Total_cat'] = count_cat_dict_initial.get('Total_cat') + count_cat_dict_initial[cat]\n",
    "    count_cat_dict_initial['Total'] = len(df[col])\n",
    "    count_cat_dict_final = {'Total_cat':0}\n",
    "    for cat in cat_list[:-1]:\n",
    "        count_cat_dict_final[cat] = math.ceil((count_cat_dict_initial.get(cat)/count_cat_dict_initial.get('Total_cat'))*count_cat_dict_initial.get('Total'))\n",
    "        count_cat_dict_final['Total_cat'] = count_cat_dict_final.get('Total_cat') + count_cat_dict_final[cat]\n",
    "    count_cat_dict_final[cat_list[-1]] = count_cat_dict_initial['Total'] - count_cat_dict_final['Total_cat']\n",
    "    fill_dict = {}\n",
    "    for cat in cat_list:\n",
    "        fill_dict[cat] = count_cat_dict_final[cat] - count_cat_dict_initial[cat]\n",
    "    for cat in cat_list[:]:\n",
    "        for i in range(fill_dict.get(cat)):\n",
    "            null_index= list(df.loc[pd.isna(df[col]),:].index)\n",
    "            if len(null_index) != 0:\n",
    "                df.loc[null_index.pop(0),col] = cat\n",
    "            else:\n",
    "                 break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e523026b-425a-4a55-8c09-a5b9f5d34abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols_test = num_cols.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c2ef92b8-4d23-45d1-8d34-699b91040be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols_test.replace(to_replace=[-1],value=np.NAN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c92a70b9-8f61-4313-856a-5644abe1b6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(num_cols_test.Rating.values.tolist()).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cbb81faf-a56c-4bee-ba46-676ed5557762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64Index([3.9, 3.7, 4.0, 3.8, 4.2, 4.1, 4.3, 5.0, 3.6, 4.4, 3.5, 3.4, 4.5,\n",
       "              3.3, 4.6, 3.2, 4.9, 3.1, 4.7, 3.0, 2.9, 4.8, 2.6, 2.7, 2.8, 2.0,\n",
       "              2.4, 2.3, 1.0, 2.5],\n",
       "             dtype='float64')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cde76527-c7d9-4de9-a17e-c90fb6150faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_test = replace_cat_list(num_cols_test,'Rating',test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a66485b9-7d51-4404-bb32-aef5fceee2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(num_cols_test.age.values.tolist()).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "647cb553-7962-4833-b77d-586aa3907e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64Index([  8.0,   7.0,  19.0,   6.0,  25.0,  17.0,  24.0,  12.0,  11.0,\n",
       "               20.0,\n",
       "              ...\n",
       "              212.0, 147.0,  83.0, 120.0, 104.0, 162.0, 151.0, 187.0, 322.0,\n",
       "              159.0],\n",
       "             dtype='float64', length=134)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4193de0b-4d16-4d48-97b1-7adc1b371ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols_test = replace_cat_list(num_cols_test,'age',test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "58c1d4ee-3c92-4e10-a2b0-d7493efe4053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 973 entries, 0 to 972\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Rating             973 non-null    float64\n",
      " 1   age                973 non-null    float64\n",
      " 2   tools              973 non-null    int64  \n",
      " 3   techs              973 non-null    int64  \n",
      " 4   education          973 non-null    int64  \n",
      " 5   desc_len           973 non-null    int64  \n",
      " 6   hourly             973 non-null    int64  \n",
      " 7   employer_provided  973 non-null    int64  \n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 60.9 KB\n"
     ]
    }
   ],
   "source": [
    "num_cols_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8058f69-8854-4846-9834-738dc6613230",
   "metadata": {
    "tags": []
   },
   "source": [
    "### perform imputation using SimpleImputer for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "69bb5ab8-bb08-4ce7-8e09-13a162c3c5c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Importing the SimpleImputer class\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = -1,strategy ='mean')\n",
    "\n",
    "imputer = imputer.fit(num_cols)\n",
    "X = imputer.transform(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "54371348-d25b-4c3b-a94f-946ef10de4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.93534994e+00, 4.10552408e+01, 2.68139774e+00, 2.81397739e+00,\n",
       "       6.16649538e-01, 3.06520966e+03, 1.61356629e-01, 6.48509764e-01])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "65e77766-e83c-4ce8-9e76-64e0a5d1327c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols_imputed= pd.DataFrame(X,columns=num_cols.columns,index=num_cols.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16292692-5c7a-4ef8-8e41-71411156ea30",
   "metadata": {},
   "source": [
    "## using median instead of mean in a soft voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143d502-9e43-4c96-b508-d68d9cffca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Define a custom function that returns the median along axis 1\n",
    "def median_voting(estimators, X):\n",
    "    # Get the predicted probabilities or scores from each estimator\n",
    "    predictions = np.asarray([est.predict(X) for est in estimators])\n",
    "    # Return the median along axis 1\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "# Define the base regressors\n",
    "reg1 = ...\n",
    "reg2 = ...\n",
    "reg3 = ...\n",
    "\n",
    "# Define the soft voting regressor with median voting\n",
    "voting_reg = VotingRegressor(estimators=[(\"reg1\", reg1), (\"reg2\", reg2), (\"reg3\", reg3)])\n",
    "# Set the voting method to the custom function\n",
    "voting_reg.voting = median_voting\n",
    "\n",
    "# Fit and predict\n",
    "voting_reg.fit(X_train, y_train)\n",
    "y_pred = voting_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad360d-a5cf-47d2-9067-48cbded87dc1",
   "metadata": {},
   "source": [
    "## use a Pipeline for each model, where you apply the transformation and then the model in sequence. This way, you can avoid data leakage and ensure that each model gets the appropriate input. You can then use a VotingClassifier to combine the predictions from each Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b11cf-96cc-4f08-98cd-4eeb702b2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Define the transformations for each model\n",
    "svm_transform = ... # some transformation for SVM\n",
    "rf_transform = ... # some transformation for Random Forest\n",
    "lasso_transform = ... # some transformation for Lasso\n",
    "\n",
    "# Define the models\n",
    "svm_model = SVC()\n",
    "rf_model = RandomForestClassifier()\n",
    "lasso_model = Lasso()\n",
    "\n",
    "# Define the pipelines\n",
    "svm_pipeline = Pipeline([(\"transform\", svm_transform), (\"model\", svm_model)])\n",
    "rf_pipeline = Pipeline([(\"transform\", rf_transform), (\"model\", rf_model)])\n",
    "lasso_pipeline = Pipeline([(\"transform\", lasso_transform), (\"model\", lasso_model)])\n",
    "\n",
    "# Define the voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[(\"svm\", svm_pipeline), (\"rf\", rf_pipeline), (\"lasso\", lasso_pipeline)])\n",
    "\n",
    "# Fit and predict\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d6e53-4912-44dc-a904-1d7d8efd1864",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Error analysis and multilabel confusion matrix of multi label classificaiotn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54ef7408-d967-4da4-a7d1-77d1b8455788",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to analyze the Errors made by the Best Model which is random Forest\n",
    "rf_clf_predictions = cross_val_predict(rf_clf,x_train,y_train, cv=3,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ee2d0a0-40a0-4fb6-8c9f-8638a5fd526c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score     91.795263\n",
      "precision_score    80.354211\n",
      "recall_score       57.597368\n",
      "f1_score           60.154737\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Developer, game or graphics</th>\n",
       "      <td>93.47</td>\n",
       "      <td>59.77</td>\n",
       "      <td>52.28</td>\n",
       "      <td>52.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scientist</th>\n",
       "      <td>92.00</td>\n",
       "      <td>70.55</td>\n",
       "      <td>68.19</td>\n",
       "      <td>69.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, full-stack</th>\n",
       "      <td>82.72</td>\n",
       "      <td>71.24</td>\n",
       "      <td>58.40</td>\n",
       "      <td>59.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, embedded applications or devices</th>\n",
       "      <td>92.82</td>\n",
       "      <td>71.49</td>\n",
       "      <td>55.05</td>\n",
       "      <td>57.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cloud infrastructure engineer</th>\n",
       "      <td>91.55</td>\n",
       "      <td>74.38</td>\n",
       "      <td>56.93</td>\n",
       "      <td>59.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, back-end</th>\n",
       "      <td>81.78</td>\n",
       "      <td>74.40</td>\n",
       "      <td>57.44</td>\n",
       "      <td>58.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, front-end</th>\n",
       "      <td>92.29</td>\n",
       "      <td>75.62</td>\n",
       "      <td>54.65</td>\n",
       "      <td>56.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, desktop or enterprise applications</th>\n",
       "      <td>90.49</td>\n",
       "      <td>75.84</td>\n",
       "      <td>55.19</td>\n",
       "      <td>56.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blockchain</th>\n",
       "      <td>95.67</td>\n",
       "      <td>76.41</td>\n",
       "      <td>53.43</td>\n",
       "      <td>55.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DevOps specialist</th>\n",
       "      <td>91.63</td>\n",
       "      <td>79.13</td>\n",
       "      <td>60.50</td>\n",
       "      <td>64.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Database administrator</th>\n",
       "      <td>93.52</td>\n",
       "      <td>82.42</td>\n",
       "      <td>57.56</td>\n",
       "      <td>61.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data scientist or machine learning specialist</th>\n",
       "      <td>91.52</td>\n",
       "      <td>83.91</td>\n",
       "      <td>58.43</td>\n",
       "      <td>61.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Academic researcher</th>\n",
       "      <td>92.72</td>\n",
       "      <td>84.13</td>\n",
       "      <td>68.83</td>\n",
       "      <td>73.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, mobile</th>\n",
       "      <td>93.39</td>\n",
       "      <td>87.28</td>\n",
       "      <td>58.15</td>\n",
       "      <td>62.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security professional</th>\n",
       "      <td>95.53</td>\n",
       "      <td>90.94</td>\n",
       "      <td>55.37</td>\n",
       "      <td>58.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data or business analyst</th>\n",
       "      <td>93.36</td>\n",
       "      <td>91.23</td>\n",
       "      <td>56.76</td>\n",
       "      <td>60.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, QA or test</th>\n",
       "      <td>94.33</td>\n",
       "      <td>92.13</td>\n",
       "      <td>55.35</td>\n",
       "      <td>58.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>System administrator</th>\n",
       "      <td>92.52</td>\n",
       "      <td>92.82</td>\n",
       "      <td>56.07</td>\n",
       "      <td>58.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engineer, data</th>\n",
       "      <td>92.80</td>\n",
       "      <td>93.04</td>\n",
       "      <td>55.77</td>\n",
       "      <td>58.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               accuracy_score  \\\n",
       "Developer, game or graphics                             93.47   \n",
       "Scientist                                               92.00   \n",
       "Developer, full-stack                                   82.72   \n",
       "Developer, embedded applications or devices             92.82   \n",
       "Cloud infrastructure engineer                           91.55   \n",
       "Developer, back-end                                     81.78   \n",
       "Developer, front-end                                    92.29   \n",
       "Developer, desktop or enterprise applications           90.49   \n",
       "Blockchain                                              95.67   \n",
       "DevOps specialist                                       91.63   \n",
       "Database administrator                                  93.52   \n",
       "Data scientist or machine learning specialist           91.52   \n",
       "Academic researcher                                     92.72   \n",
       "Developer, mobile                                       93.39   \n",
       "Security professional                                   95.53   \n",
       "Data or business analyst                                93.36   \n",
       "Developer, QA or test                                   94.33   \n",
       "System administrator                                    92.52   \n",
       "Engineer, data                                          92.80   \n",
       "\n",
       "                                               precision_score  recall_score  \\\n",
       "Developer, game or graphics                              59.77         52.28   \n",
       "Scientist                                                70.55         68.19   \n",
       "Developer, full-stack                                    71.24         58.40   \n",
       "Developer, embedded applications or devices              71.49         55.05   \n",
       "Cloud infrastructure engineer                            74.38         56.93   \n",
       "Developer, back-end                                      74.40         57.44   \n",
       "Developer, front-end                                     75.62         54.65   \n",
       "Developer, desktop or enterprise applications            75.84         55.19   \n",
       "Blockchain                                               76.41         53.43   \n",
       "DevOps specialist                                        79.13         60.50   \n",
       "Database administrator                                   82.42         57.56   \n",
       "Data scientist or machine learning specialist            83.91         58.43   \n",
       "Academic researcher                                      84.13         68.83   \n",
       "Developer, mobile                                        87.28         58.15   \n",
       "Security professional                                    90.94         55.37   \n",
       "Data or business analyst                                 91.23         56.76   \n",
       "Developer, QA or test                                    92.13         55.35   \n",
       "System administrator                                     92.82         56.07   \n",
       "Engineer, data                                           93.04         55.77   \n",
       "\n",
       "                                               f1_score  \n",
       "Developer, game or graphics                       52.89  \n",
       "Scientist                                         69.27  \n",
       "Developer, full-stack                             59.97  \n",
       "Developer, embedded applications or devices       57.10  \n",
       "Cloud infrastructure engineer                     59.60  \n",
       "Developer, back-end                               58.35  \n",
       "Developer, front-end                              56.45  \n",
       "Developer, desktop or enterprise applications     56.91  \n",
       "Blockchain                                        55.20  \n",
       "DevOps specialist                                 64.37  \n",
       "Database administrator                            61.11  \n",
       "Data scientist or machine learning specialist     61.94  \n",
       "Academic researcher                               73.68  \n",
       "Developer, mobile                                 62.07  \n",
       "Security professional                             58.47  \n",
       "Data or business analyst                          60.11  \n",
       "Developer, QA or test                             58.16  \n",
       "System administrator                              58.84  \n",
       "Engineer, data                                    58.45  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe contains the predictions \n",
    "rf_clf_predictions =  pd.DataFrame(rf_clf_predictions,\n",
    "                            columns=y_train.columns)\n",
    "\n",
    "rf_clf_prediction_scores = {score.__name__: predictions_per_col(rf_clf_predictions, y_train, score) \n",
    "                for score in [accuracy_score, precision_score, recall_score, f1_score]}\n",
    "\n",
    "rf_clf_prediction_scores = pd.concat(rf_clf_prediction_scores,axis=1)\n",
    "print(rf_clf_prediction_scores.mean())\n",
    "rf_clf_prediction_scores.sort_values(\"precision_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2561624d-3a28-4bd8-a263-c0d40b178c12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3562            0.50s\n",
      "         2           0.3329            0.62s\n",
      "         3           0.3193            0.57s\n",
      "         4           0.3103            0.59s\n",
      "         5           0.3004            0.60s\n",
      "         6           0.2916            0.59s\n",
      "         7           0.2847            0.66s\n",
      "         8           0.2774            0.64s\n",
      "         9           0.2729            0.64s\n",
      "        10           0.2677            0.62s\n",
      "        20           0.2386            0.52s\n",
      "        30           0.2234            0.43s\n",
      "        40           0.2147            0.36s\n",
      "        50           0.2081            0.30s\n",
      "        60           0.2021            0.25s\n",
      "        70           0.1976            0.18s\n",
      "        80           0.1934            0.12s\n",
      "        90           0.1897            0.06s\n",
      "       100           0.1860            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2178            0.50s\n",
      "         2           0.2144            0.49s\n",
      "         3           0.2109            0.52s\n",
      "         4           0.2067            0.50s\n",
      "         5           0.2056            0.52s\n",
      "         6           0.2035            0.51s\n",
      "         7           0.2014            0.50s\n",
      "         8           0.1995            0.49s\n",
      "         9           0.1961            0.47s\n",
      "        10           0.1948            0.46s\n",
      "        20           0.1798            0.43s\n",
      "        30           0.1722            0.37s\n",
      "        40           0.1653            0.31s\n",
      "        50           0.1601            0.26s\n",
      "        60           0.1557            0.21s\n",
      "        70           0.1507            0.16s\n",
      "        80           0.1474            0.11s\n",
      "        90           0.1427            0.05s\n",
      "       100           0.1395            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.1754            0.40s\n",
      "         2           0.1717            0.44s\n",
      "         3           0.1669            0.55s\n",
      "         4           0.1620            0.55s\n",
      "         5           0.1595            0.53s\n",
      "         6           0.1568            0.50s\n",
      "         7           0.1537            0.49s\n",
      "         8           0.1519            0.47s\n",
      "         9           0.1503            0.48s\n",
      "        10           0.1479            0.47s\n",
      "        20           0.1354            0.40s\n",
      "        30           0.1274            0.34s\n",
      "        40           0.1212            0.29s\n",
      "        50           0.1148            0.25s\n",
      "        60           0.1101            0.20s\n",
      "        70           0.1056            0.15s\n",
      "        80           0.1028            0.10s\n",
      "        90           0.0999            0.05s\n",
      "       100           0.0970            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1535            0.60s\n",
      "         2           1.1422            0.58s\n",
      "         3           1.1356            0.51s\n",
      "         4           1.1297            0.50s\n",
      "         5           1.1236            0.51s\n",
      "         6           1.1162            0.48s\n",
      "         7           1.1098            0.48s\n",
      "         8           1.1044            0.48s\n",
      "         9           1.1004            0.47s\n",
      "        10           1.0963            0.47s\n",
      "        20           1.0575            0.43s\n",
      "        30           1.0325            0.37s\n",
      "        40           1.0154            0.32s\n",
      "        50           0.9986            0.26s\n",
      "        60           0.9849            0.21s\n",
      "        70           0.9740            0.16s\n",
      "        80           0.9635            0.11s\n",
      "        90           0.9548            0.05s\n",
      "       100           0.9477            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2044            0.41s\n",
      "         2           0.2006            0.45s\n",
      "         3           0.1962            0.45s\n",
      "         4           0.1945            0.45s\n",
      "         5           0.1913            0.45s\n",
      "         6           0.1900            0.46s\n",
      "         7           0.1877            0.49s\n",
      "         8           0.1862            0.50s\n",
      "         9           0.1832            0.50s\n",
      "        10           0.1802            0.49s\n",
      "        20           0.1669            0.47s\n",
      "        30           0.1580            0.41s\n",
      "        40           0.1520            0.35s\n",
      "        50           0.1476            0.28s\n",
      "        60           0.1431            0.22s\n",
      "        70           0.1388            0.17s\n",
      "        80           0.1355            0.11s\n",
      "        90           0.1325            0.06s\n",
      "       100           0.1290            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3669            0.65s\n",
      "         2           0.3552            0.72s\n",
      "         3           0.3447            0.73s\n",
      "         4           0.3348            0.64s\n",
      "         5           0.3184            0.59s\n",
      "         6           0.3098            0.58s\n",
      "         7           0.3038            0.55s\n",
      "         8           0.2944            0.54s\n",
      "         9           0.2864            0.54s\n",
      "        10           0.2782            0.52s\n",
      "        20           0.2426            0.42s\n",
      "        30           0.2275            0.37s\n",
      "        40           0.2134            0.34s\n",
      "        50           0.2036            0.29s\n",
      "        60           0.1960            0.22s\n",
      "        70           0.1893            0.16s\n",
      "        80           0.1846            0.11s\n",
      "        90           0.1809            0.05s\n",
      "       100           0.1770            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1618            0.61s\n",
      "         2           1.1405            0.55s\n",
      "         3           1.1211            0.53s\n",
      "         4           1.1018            0.51s\n",
      "         5           1.0803            0.50s\n",
      "         6           1.0682            0.47s\n",
      "         7           1.0510            0.47s\n",
      "         8           1.0394            0.46s\n",
      "         9           1.0320            0.45s\n",
      "        10           1.0219            0.44s\n",
      "        20           0.9451            0.43s\n",
      "        30           0.9098            0.38s\n",
      "        40           0.8894            0.32s\n",
      "        50           0.8734            0.27s\n",
      "        60           0.8628            0.22s\n",
      "        70           0.8527            0.17s\n",
      "        80           0.8445            0.11s\n",
      "        90           0.8372            0.05s\n",
      "       100           0.8295            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7385            0.50s\n",
      "         2           0.7117            0.61s\n",
      "         3           0.6956            0.53s\n",
      "         4           0.6825            0.52s\n",
      "         5           0.6613            0.50s\n",
      "         6           0.6532            0.49s\n",
      "         7           0.6435            0.47s\n",
      "         8           0.6316            0.46s\n",
      "         9           0.6236            0.46s\n",
      "        10           0.6144            0.45s\n",
      "        20           0.5601            0.39s\n",
      "        30           0.5314            0.36s\n",
      "        40           0.5140            0.31s\n",
      "        50           0.4999            0.25s\n",
      "        60           0.4862            0.20s\n",
      "        70           0.4778            0.15s\n",
      "        80           0.4703            0.10s\n",
      "        90           0.4637            0.05s\n",
      "       100           0.4567            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7024            0.40s\n",
      "         2           0.6779            0.49s\n",
      "         3           0.6590            0.49s\n",
      "         4           0.6516            0.46s\n",
      "         5           0.6448            0.49s\n",
      "         6           0.6362            0.56s\n",
      "         7           0.6258            0.54s\n",
      "         8           0.6145            0.53s\n",
      "         9           0.6105            0.52s\n",
      "        10           0.6051            0.51s\n",
      "        20           0.5620            0.43s\n",
      "        30           0.5312            0.39s\n",
      "        40           0.5125            0.34s\n",
      "        50           0.4998            0.27s\n",
      "        60           0.4899            0.22s\n",
      "        70           0.4804            0.17s\n",
      "        80           0.4721            0.11s\n",
      "        90           0.4654            0.06s\n",
      "       100           0.4601            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6167            0.40s\n",
      "         2           0.6141            0.44s\n",
      "         3           0.6107            0.48s\n",
      "         4           0.6074            0.45s\n",
      "         5           0.6043            0.45s\n",
      "         6           0.6019            0.48s\n",
      "         7           0.5991            0.48s\n",
      "         8           0.5969            0.48s\n",
      "         9           0.5948            0.49s\n",
      "        10           0.5920            0.48s\n",
      "        20           0.5779            0.45s\n",
      "        30           0.5659            0.39s\n",
      "        40           0.5567            0.33s\n",
      "        50           0.5475            0.27s\n",
      "        60           0.5398            0.22s\n",
      "        70           0.5321            0.16s\n",
      "        80           0.5257            0.10s\n",
      "        90           0.5206            0.05s\n",
      "       100           0.5151            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7130            0.60s\n",
      "         2           0.7059            0.57s\n",
      "         3           0.6965            0.53s\n",
      "         4           0.6880            0.52s\n",
      "         5           0.6831            0.58s\n",
      "         6           0.6767            0.61s\n",
      "         7           0.6714            0.64s\n",
      "         8           0.6671            0.63s\n",
      "         9           0.6639            0.61s\n",
      "        10           0.6610            0.59s\n",
      "        20           0.6282            0.50s\n",
      "        30           0.6052            0.50s\n",
      "        40           0.5903            0.41s\n",
      "        50           0.5784            0.32s\n",
      "        60           0.5683            0.25s\n",
      "        70           0.5595            0.18s\n",
      "        80           0.5510            0.12s\n",
      "        90           0.5442            0.06s\n",
      "       100           0.5382            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6809            0.50s\n",
      "         2           0.6604            0.47s\n",
      "         3           0.6277            0.47s\n",
      "         4           0.6126            0.47s\n",
      "         5           0.5962            0.47s\n",
      "         6           0.5796            0.46s\n",
      "         7           0.5652            0.45s\n",
      "         8           0.5557            0.46s\n",
      "         9           0.5491            0.44s\n",
      "        10           0.5374            0.44s\n",
      "        20           0.4855            0.40s\n",
      "        30           0.4569            0.34s\n",
      "        40           0.4379            0.31s\n",
      "        50           0.4235            0.26s\n",
      "        60           0.4144            0.21s\n",
      "        70           0.4064            0.16s\n",
      "        80           0.3995            0.10s\n",
      "        90           0.3934            0.05s\n",
      "       100           0.3870            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6311            0.40s\n",
      "         2           0.6245            0.49s\n",
      "         3           0.6196            0.52s\n",
      "         4           0.6135            0.62s\n",
      "         5           0.6091            0.61s\n",
      "         6           0.6049            0.61s\n",
      "         7           0.6021            0.61s\n",
      "         8           0.5985            0.61s\n",
      "         9           0.5951            0.59s\n",
      "        10           0.5926            0.57s\n",
      "        20           0.5684            0.45s\n",
      "        30           0.5522            0.38s\n",
      "        40           0.5369            0.32s\n",
      "        50           0.5242            0.27s\n",
      "        60           0.5137            0.21s\n",
      "        70           0.5036            0.16s\n",
      "        80           0.4961            0.11s\n",
      "        90           0.4879            0.05s\n",
      "       100           0.4795            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5900            0.49s\n",
      "         2           0.5708            0.54s\n",
      "         3           0.5580            0.48s\n",
      "         4           0.5461            0.55s\n",
      "         5           0.5235            0.57s\n",
      "         6           0.5133            0.56s\n",
      "         7           0.5054            0.56s\n",
      "         8           0.4919            0.60s\n",
      "         9           0.4778            0.57s\n",
      "        10           0.4727            0.55s\n",
      "        20           0.4205            0.44s\n",
      "        30           0.3906            0.37s\n",
      "        40           0.3744            0.31s\n",
      "        50           0.3605            0.27s\n",
      "        60           0.3502            0.21s\n",
      "        70           0.3419            0.16s\n",
      "        80           0.3333            0.11s\n",
      "        90           0.3272            0.05s\n",
      "       100           0.3215            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7868            0.50s\n",
      "         2           0.7720            0.49s\n",
      "         3           0.7519            0.49s\n",
      "         4           0.7362            0.53s\n",
      "         5           0.7201            0.53s\n",
      "         6           0.7073            0.52s\n",
      "         7           0.6965            0.52s\n",
      "         8           0.6888            0.78s\n",
      "         9           0.6824            0.76s\n",
      "        10           0.6765            0.72s\n",
      "        20           0.6141            0.54s\n",
      "        30           0.5791            0.45s\n",
      "        40           0.5586            0.37s\n",
      "        50           0.5436            0.30s\n",
      "        60           0.5317            0.24s\n",
      "        70           0.5214            0.18s\n",
      "        80           0.5124            0.12s\n",
      "        90           0.5056            0.06s\n",
      "       100           0.4981            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5861            0.50s\n",
      "         2           0.5767            0.47s\n",
      "         3           0.5634            0.47s\n",
      "         4           0.5317            0.47s\n",
      "         5           0.5263            0.47s\n",
      "         6           0.5184            0.45s\n",
      "         7           0.5139            0.45s\n",
      "         8           0.5107            0.45s\n",
      "         9           0.4961            0.44s\n",
      "        10           0.4919            0.42s\n",
      "        20           0.4471            0.39s\n",
      "        30           0.4232            0.35s\n",
      "        40           0.4059            0.32s\n",
      "        50           0.3913            0.27s\n",
      "        60           0.3800            0.22s\n",
      "        70           0.3683            0.17s\n",
      "        80           0.3590            0.11s\n",
      "        90           0.3512            0.06s\n",
      "       100           0.3430            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8430            0.52s\n",
      "         2           0.8287            0.50s\n",
      "         3           0.8144            0.54s\n",
      "         4           0.8002            0.58s\n",
      "         5           0.7888            0.58s\n",
      "         6           0.7792            0.57s\n",
      "         7           0.7712            0.57s\n",
      "         8           0.7640            0.57s\n",
      "         9           0.7573            0.55s\n",
      "        10           0.7511            0.54s\n",
      "        20           0.7149            0.45s\n",
      "        30           0.6920            0.40s\n",
      "        40           0.6760            0.32s\n",
      "        50           0.6616            0.27s\n",
      "        60           0.6511            0.22s\n",
      "        70           0.6412            0.16s\n",
      "        80           0.6322            0.11s\n",
      "        90           0.6245            0.06s\n",
      "       100           0.6186            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8066            0.50s\n",
      "         2           0.7834            0.49s\n",
      "         3           0.7688            0.49s\n",
      "         4           0.7570            0.46s\n",
      "         5           0.7418            0.48s\n",
      "         6           0.7325            0.47s\n",
      "         7           0.7216            0.47s\n",
      "         8           0.7135            0.46s\n",
      "         9           0.7087            0.45s\n",
      "        10           0.7014            0.44s\n",
      "        20           0.6534            0.41s\n",
      "        30           0.6255            0.37s\n",
      "        40           0.6080            0.32s\n",
      "        50           0.5952            0.27s\n",
      "        60           0.5852            0.22s\n",
      "        70           0.5760            0.16s\n",
      "        80           0.5685            0.11s\n",
      "        90           0.5603            0.05s\n",
      "       100           0.5542            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7741            0.51s\n",
      "         2           0.7419            0.68s\n",
      "         3           0.7076            0.65s\n",
      "         4           0.6916            0.57s\n",
      "         5           0.6756            0.57s\n",
      "         6           0.6591            0.55s\n",
      "         7           0.6460            0.52s\n",
      "         8           0.6362            0.56s\n",
      "         9           0.6293            0.56s\n",
      "        10           0.6158            0.56s\n",
      "        20           0.5570            0.50s\n",
      "        30           0.5240            0.41s\n",
      "        40           0.5039            0.33s\n",
      "        50           0.4889            0.27s\n",
      "        60           0.4770            0.21s\n",
      "        70           0.4681            0.16s\n",
      "        80           0.4611            0.11s\n",
      "        90           0.4534            0.05s\n",
      "       100           0.4471            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7490            0.47s\n",
      "         2           0.7010            0.52s\n",
      "         3           0.6691            0.51s\n",
      "         4           0.6458            0.54s\n",
      "         5           0.6216            0.55s\n",
      "         6           0.6001            0.51s\n",
      "         7           0.5837            0.50s\n",
      "         8           0.5678            0.49s\n",
      "         9           0.5579            0.47s\n",
      "        10           0.5462            0.48s\n",
      "        20           0.4838            0.43s\n",
      "        30           0.4531            0.36s\n",
      "        40           0.4367            0.31s\n",
      "        50           0.4237            0.26s\n",
      "        60           0.4152            0.21s\n",
      "        70           0.4078            0.15s\n",
      "        80           0.4014            0.10s\n",
      "        90           0.3949            0.05s\n",
      "       100           0.3892            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7236            0.45s\n",
      "         2           0.7105            0.47s\n",
      "         3           0.7001            0.47s\n",
      "         4           0.6866            0.47s\n",
      "         5           0.6768            0.47s\n",
      "         6           0.6666            0.46s\n",
      "         7           0.6580            0.45s\n",
      "         8           0.6532            0.45s\n",
      "         9           0.6432            0.45s\n",
      "        10           0.6377            0.45s\n",
      "        20           0.6031            0.42s\n",
      "        30           0.5815            0.37s\n",
      "        40           0.5680            0.31s\n",
      "        50           0.5554            0.26s\n",
      "        60           0.5465            0.21s\n",
      "        70           0.5374            0.15s\n",
      "        80           0.5308            0.10s\n",
      "        90           0.5250            0.05s\n",
      "       100           0.5197            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7078            0.49s\n",
      "         2           0.6948            0.49s\n",
      "         3           0.6774            0.52s\n",
      "         4           0.6665            0.50s\n",
      "         5           0.6577            0.49s\n",
      "         6           0.6452            0.49s\n",
      "         7           0.6369            0.48s\n",
      "         8           0.6304            0.48s\n",
      "         9           0.6225            0.48s\n",
      "        10           0.6146            0.48s\n",
      "        20           0.5756            0.42s\n",
      "        30           0.5488            0.37s\n",
      "        40           0.5335            0.31s\n",
      "        50           0.5213            0.26s\n",
      "        60           0.5106            0.21s\n",
      "        70           0.5020            0.16s\n",
      "        80           0.4945            0.11s\n",
      "        90           0.4883            0.05s\n",
      "       100           0.4818            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2454            0.69s\n",
      "         2           1.2307            0.59s\n",
      "         3           1.2210            0.55s\n",
      "         4           1.2128            0.53s\n",
      "         5           1.2050            0.53s\n",
      "         6           1.1947            0.52s\n",
      "         7           1.1855            0.53s\n",
      "         8           1.1775            0.52s\n",
      "         9           1.1715            0.51s\n",
      "        10           1.1657            0.50s\n",
      "        20           1.1131            0.43s\n",
      "        30           1.0828            0.36s\n",
      "        40           1.0623            0.34s\n",
      "        50           1.0428            0.27s\n",
      "        60           1.0290            0.22s\n",
      "        70           1.0176            0.16s\n",
      "        80           1.0067            0.11s\n",
      "        90           0.9966            0.06s\n",
      "       100           0.9880            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6526            0.40s\n",
      "         2           0.6474            0.55s\n",
      "         3           0.6433            0.65s\n",
      "         4           0.6401            0.63s\n",
      "         5           0.6297            0.59s\n",
      "         6           0.6261            0.57s\n",
      "         7           0.6219            0.55s\n",
      "         8           0.6187            0.53s\n",
      "         9           0.6114            0.51s\n",
      "        10           0.6032            0.51s\n",
      "        20           0.5721            0.43s\n",
      "        30           0.5511            0.37s\n",
      "        40           0.5403            0.31s\n",
      "        50           0.5286            0.27s\n",
      "        60           0.5205            0.21s\n",
      "        70           0.5116            0.16s\n",
      "        80           0.5042            0.11s\n",
      "        90           0.4980            0.05s\n",
      "       100           0.4913            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6962            0.49s\n",
      "         2           0.6754            0.49s\n",
      "         3           0.6570            0.45s\n",
      "         4           0.6307            0.46s\n",
      "         5           0.5953            0.48s\n",
      "         6           0.5720            0.46s\n",
      "         7           0.5576            0.45s\n",
      "         8           0.5366            0.48s\n",
      "         9           0.5213            0.49s\n",
      "        10           0.5031            0.48s\n",
      "        20           0.4143            0.43s\n",
      "        30           0.3799            0.36s\n",
      "        40           0.3527            0.32s\n",
      "        50           0.3317            0.27s\n",
      "        60           0.3188            0.23s\n",
      "        70           0.3090            0.17s\n",
      "        80           0.3007            0.11s\n",
      "        90           0.2937            0.06s\n",
      "       100           0.2872            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1392            0.50s\n",
      "         2           1.1178            0.49s\n",
      "         3           1.0982            0.50s\n",
      "         4           1.0777            0.47s\n",
      "         5           1.0571            0.48s\n",
      "         6           1.0432            0.48s\n",
      "         7           1.0262            0.47s\n",
      "         8           1.0126            0.47s\n",
      "         9           1.0055            0.46s\n",
      "        10           0.9949            0.46s\n",
      "        20           0.9258            0.41s\n",
      "        30           0.8927            0.37s\n",
      "        40           0.8719            0.32s\n",
      "        50           0.8589            0.26s\n",
      "        60           0.8496            0.21s\n",
      "        70           0.8407            0.16s\n",
      "        80           0.8335            0.11s\n",
      "        90           0.8267            0.06s\n",
      "       100           0.8203            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3729            0.52s\n",
      "         2           0.3617            0.50s\n",
      "         3           0.3555            0.49s\n",
      "         4           0.3500            0.49s\n",
      "         5           0.3413            0.50s\n",
      "         6           0.3373            0.49s\n",
      "         7           0.3322            0.48s\n",
      "         8           0.3282            0.47s\n",
      "         9           0.3255            0.48s\n",
      "        10           0.3217            0.47s\n",
      "        20           0.2976            0.41s\n",
      "        30           0.2826            0.36s\n",
      "        40           0.2718            0.32s\n",
      "        50           0.2631            0.27s\n",
      "        60           0.2548            0.22s\n",
      "        70           0.2490            0.16s\n",
      "        80           0.2432            0.11s\n",
      "        90           0.2384            0.05s\n",
      "       100           0.2347            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2807            0.45s\n",
      "         2           0.2718            0.47s\n",
      "         3           0.2643            0.47s\n",
      "         4           0.2617            0.47s\n",
      "         5           0.2591            0.51s\n",
      "         6           0.2569            0.50s\n",
      "         7           0.2534            0.50s\n",
      "         8           0.2496            0.49s\n",
      "         9           0.2480            0.47s\n",
      "        10           0.2457            0.47s\n",
      "        20           0.2282            0.44s\n",
      "        30           0.2170            0.39s\n",
      "        40           0.2095            0.34s\n",
      "        50           0.2040            0.28s\n",
      "        60           0.1991            0.23s\n",
      "        70           0.1938            0.17s\n",
      "        80           0.1890            0.11s\n",
      "        90           0.1849            0.05s\n",
      "       100           0.1815            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.1681            0.40s\n",
      "         2           0.1662            0.44s\n",
      "         3           0.1634            0.45s\n",
      "         4           0.1615            0.44s\n",
      "         5           0.1602            0.44s\n",
      "         6           0.1594            0.47s\n",
      "         7           0.1584            0.47s\n",
      "         8           0.1577            0.46s\n",
      "         9           0.1569            0.45s\n",
      "        10           0.1558            0.46s\n",
      "        20           0.1468            0.45s\n",
      "        30           0.1400            0.38s\n",
      "        40           0.1345            0.32s\n",
      "        50           0.1298            0.26s\n",
      "        60           0.1248            0.21s\n",
      "        70           0.1205            0.16s\n",
      "        80           0.1171            0.10s\n",
      "        90           0.1146            0.05s\n",
      "       100           0.1109            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3356            0.60s\n",
      "         2           0.3308            0.49s\n",
      "         3           0.3258            0.48s\n",
      "         4           0.3217            0.48s\n",
      "         5           0.3187            0.48s\n",
      "         6           0.3156            0.46s\n",
      "         7           0.3127            0.45s\n",
      "         8           0.3105            0.46s\n",
      "         9           0.3088            0.46s\n",
      "        10           0.3076            0.45s\n",
      "        20           0.2900            0.40s\n",
      "        30           0.2779            0.35s\n",
      "        40           0.2688            0.30s\n",
      "        50           0.2614            0.25s\n",
      "        60           0.2550            0.20s\n",
      "        70           0.2505            0.15s\n",
      "        80           0.2452            0.10s\n",
      "        90           0.2419            0.05s\n",
      "       100           0.2362            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3525            0.40s\n",
      "         2           0.3417            0.49s\n",
      "         3           0.3262            0.44s\n",
      "         4           0.3176            0.45s\n",
      "         5           0.3086            0.43s\n",
      "         6           0.3011            0.45s\n",
      "         7           0.2950            0.46s\n",
      "         8           0.2887            0.47s\n",
      "         9           0.2852            0.46s\n",
      "        10           0.2798            0.46s\n",
      "        20           0.2539            0.47s\n",
      "        30           0.2396            0.39s\n",
      "        40           0.2281            0.32s\n",
      "        50           0.2199            0.27s\n",
      "        60           0.2136            0.22s\n",
      "        70           0.2087            0.16s\n",
      "        80           0.2041            0.10s\n",
      "        90           0.2001            0.05s\n",
      "       100           0.1963            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3028            0.74s\n",
      "         2           0.2979            0.68s\n",
      "         3           0.2934            0.64s\n",
      "         4           0.2897            0.60s\n",
      "         5           0.2868            0.57s\n",
      "         6           0.2839            0.58s\n",
      "         7           0.2813            0.56s\n",
      "         8           0.2786            0.54s\n",
      "         9           0.2769            0.53s\n",
      "        10           0.2744            0.51s\n",
      "        20           0.2566            0.44s\n",
      "        30           0.2450            0.37s\n",
      "        40           0.2360            0.31s\n",
      "        50           0.2282            0.26s\n",
      "        60           0.2206            0.21s\n",
      "        70           0.2159            0.15s\n",
      "        80           0.2115            0.10s\n",
      "        90           0.2076            0.05s\n",
      "       100           0.2036            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5874            0.40s\n",
      "         2           0.5654            0.44s\n",
      "         3           0.5503            0.45s\n",
      "         4           0.5369            0.50s\n",
      "         5           0.5123            0.53s\n",
      "         6           0.4964            0.51s\n",
      "         7           0.4883            0.50s\n",
      "         8           0.4818            0.49s\n",
      "         9           0.4664            0.47s\n",
      "        10           0.4612            0.48s\n",
      "        20           0.4063            0.42s\n",
      "        30           0.3735            0.36s\n",
      "        40           0.3564            0.31s\n",
      "        50           0.3423            0.25s\n",
      "        60           0.3323            0.20s\n",
      "        70           0.3233            0.15s\n",
      "        80           0.3159            0.10s\n",
      "        90           0.3093            0.05s\n",
      "       100           0.3038            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8313            0.49s\n",
      "         2           0.8111            0.44s\n",
      "         3           0.7895            0.45s\n",
      "         4           0.7745            0.46s\n",
      "         5           0.7586            0.46s\n",
      "         6           0.7447            0.45s\n",
      "         7           0.7326            0.45s\n",
      "         8           0.7232            0.45s\n",
      "         9           0.7158            0.44s\n",
      "        10           0.7075            0.42s\n",
      "        20           0.6488            0.39s\n",
      "        30           0.6146            0.34s\n",
      "        40           0.5928            0.30s\n",
      "        50           0.5757            0.26s\n",
      "        60           0.5624            0.21s\n",
      "        70           0.5518            0.15s\n",
      "        80           0.5438            0.10s\n",
      "        90           0.5356            0.05s\n",
      "       100           0.5283            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5775            0.69s\n",
      "         2           0.5668            0.64s\n",
      "         3           0.5552            0.57s\n",
      "         4           0.5251            0.54s\n",
      "         5           0.5167            0.52s\n",
      "         6           0.5083            0.53s\n",
      "         7           0.5029            0.51s\n",
      "         8           0.4984            0.52s\n",
      "         9           0.4847            0.53s\n",
      "        10           0.4796            0.52s\n",
      "        20           0.4370            0.43s\n",
      "        30           0.4122            0.38s\n",
      "        40           0.3949            0.33s\n",
      "        50           0.3809            0.27s\n",
      "        60           0.3701            0.21s\n",
      "        70           0.3610            0.16s\n",
      "        80           0.3522            0.11s\n",
      "        90           0.3433            0.05s\n",
      "       100           0.3360            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8147            0.50s\n",
      "         2           0.8010            0.54s\n",
      "         3           0.7858            0.49s\n",
      "         4           0.7702            0.48s\n",
      "         5           0.7549            0.49s\n",
      "         6           0.7445            0.52s\n",
      "         7           0.7365            0.51s\n",
      "         8           0.7282            0.49s\n",
      "         9           0.7204            0.48s\n",
      "        10           0.7141            0.47s\n",
      "        20           0.6770            0.42s\n",
      "        30           0.6535            0.37s\n",
      "        40           0.6375            0.31s\n",
      "        50           0.6227            0.26s\n",
      "        60           0.6125            0.21s\n",
      "        70           0.6032            0.16s\n",
      "        80           0.5946            0.10s\n",
      "        90           0.5889            0.05s\n",
      "       100           0.5822            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6487            0.66s\n",
      "         2           0.6299            0.57s\n",
      "         3           0.6174            0.54s\n",
      "         4           0.6066            0.52s\n",
      "         5           0.5947            0.51s\n",
      "         6           0.5871            0.50s\n",
      "         7           0.5780            0.49s\n",
      "         8           0.5714            0.49s\n",
      "         9           0.5670            0.47s\n",
      "        10           0.5613            0.47s\n",
      "        20           0.5169            0.43s\n",
      "        30           0.4923            0.38s\n",
      "        40           0.4770            0.33s\n",
      "        50           0.4650            0.27s\n",
      "        60           0.4548            0.22s\n",
      "        70           0.4467            0.16s\n",
      "        80           0.4395            0.11s\n",
      "        90           0.4327            0.05s\n",
      "       100           0.4258            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6657            0.50s\n",
      "         2           0.6511            0.59s\n",
      "         3           0.6264            0.52s\n",
      "         4           0.6146            0.48s\n",
      "         5           0.6028            0.50s\n",
      "         6           0.5900            0.48s\n",
      "         7           0.5817            0.47s\n",
      "         8           0.5750            0.47s\n",
      "         9           0.5683            0.45s\n",
      "        10           0.5575            0.45s\n",
      "        20           0.5157            0.38s\n",
      "        30           0.4892            0.35s\n",
      "        40           0.4713            0.29s\n",
      "        50           0.4581            0.25s\n",
      "        60           0.4471            0.20s\n",
      "        70           0.4392            0.15s\n",
      "        80           0.4321            0.10s\n",
      "        90           0.4254            0.05s\n",
      "       100           0.4193            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7616            0.63s\n",
      "         2           0.7185            0.63s\n",
      "         3           0.6898            0.58s\n",
      "         4           0.6694            0.55s\n",
      "         5           0.6466            0.53s\n",
      "         6           0.6242            0.52s\n",
      "         7           0.6084            0.50s\n",
      "         8           0.5928            0.48s\n",
      "         9           0.5790            0.52s\n",
      "        10           0.5690            0.53s\n",
      "        20           0.5130            0.42s\n",
      "        30           0.4812            0.36s\n",
      "        40           0.4630            0.30s\n",
      "        50           0.4511            0.26s\n",
      "        60           0.4414            0.20s\n",
      "        70           0.4335            0.15s\n",
      "        80           0.4274            0.10s\n",
      "        90           0.4216            0.05s\n",
      "       100           0.4159            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7396            0.45s\n",
      "         2           0.7291            0.47s\n",
      "         3           0.7192            0.47s\n",
      "         4           0.7076            0.47s\n",
      "         5           0.7007            0.46s\n",
      "         6           0.6911            0.45s\n",
      "         7           0.6828            0.45s\n",
      "         8           0.6788            0.45s\n",
      "         9           0.6689            0.43s\n",
      "        10           0.6618            0.46s\n",
      "        20           0.6318            0.41s\n",
      "        30           0.6113            0.36s\n",
      "        40           0.5982            0.31s\n",
      "        50           0.5857            0.25s\n",
      "        60           0.5757            0.20s\n",
      "        70           0.5667            0.15s\n",
      "        80           0.5607            0.10s\n",
      "        90           0.5545            0.05s\n",
      "       100           0.5477            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7167            0.50s\n",
      "         2           0.7061            0.44s\n",
      "         3           0.6900            0.45s\n",
      "         4           0.6802            0.48s\n",
      "         5           0.6709            0.48s\n",
      "         6           0.6588            0.47s\n",
      "         7           0.6500            0.45s\n",
      "         8           0.6430            0.45s\n",
      "         9           0.6357            0.45s\n",
      "        10           0.6280            0.44s\n",
      "        20           0.5893            0.39s\n",
      "        30           0.5628            0.35s\n",
      "        40           0.5474            0.29s\n",
      "        50           0.5337            0.26s\n",
      "        60           0.5242            0.21s\n",
      "        70           0.5159            0.16s\n",
      "        80           0.5085            0.11s\n",
      "        90           0.5021            0.05s\n",
      "       100           0.4965            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2115            0.44s\n",
      "         2           1.2012            0.48s\n",
      "         3           1.1933            0.48s\n",
      "         4           1.1867            0.50s\n",
      "         5           1.1796            0.50s\n",
      "         6           1.1718            0.49s\n",
      "         7           1.1650            0.48s\n",
      "         8           1.1586            0.51s\n",
      "         9           1.1547            0.51s\n",
      "        10           1.1505            0.53s\n",
      "        20           1.1103            0.43s\n",
      "        30           1.0871            0.37s\n",
      "        40           1.0680            0.31s\n",
      "        50           1.0509            0.25s\n",
      "        60           1.0363            0.20s\n",
      "        70           1.0255            0.15s\n",
      "        80           1.0141            0.10s\n",
      "        90           1.0041            0.05s\n",
      "       100           0.9967            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6878            0.53s\n",
      "         2           0.6812            0.46s\n",
      "         3           0.6761            0.53s\n",
      "         4           0.6708            0.60s\n",
      "         5           0.6614            0.61s\n",
      "         6           0.6543            0.59s\n",
      "         7           0.6503            0.56s\n",
      "         8           0.6469            0.56s\n",
      "         9           0.6405            0.54s\n",
      "        10           0.6349            0.53s\n",
      "        20           0.6055            0.50s\n",
      "        30           0.5875            0.42s\n",
      "        40           0.5754            0.35s\n",
      "        50           0.5642            0.29s\n",
      "        60           0.5563            0.22s\n",
      "        70           0.5470            0.16s\n",
      "        80           0.5398            0.11s\n",
      "        90           0.5329            0.05s\n",
      "       100           0.5266            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6367            0.40s\n",
      "         2           0.6146            0.44s\n",
      "         3           0.6000            0.45s\n",
      "         4           0.5739            0.43s\n",
      "         5           0.5381            0.44s\n",
      "         6           0.5154            0.48s\n",
      "         7           0.5014            0.53s\n",
      "         8           0.4842            0.52s\n",
      "         9           0.4694            0.50s\n",
      "        10           0.4540            0.48s\n",
      "        20           0.3696            0.42s\n",
      "        30           0.3338            0.36s\n",
      "        40           0.3078            0.32s\n",
      "        50           0.2894            0.26s\n",
      "        60           0.2766            0.21s\n",
      "        70           0.2678            0.15s\n",
      "        80           0.2598            0.10s\n",
      "        90           0.2530            0.05s\n",
      "       100           0.2466            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1505            0.50s\n",
      "         2           1.1209            0.59s\n",
      "         3           1.0982            0.69s\n",
      "         4           1.0727            0.63s\n",
      "         5           1.0502            0.58s\n",
      "         6           1.0339            0.58s\n",
      "         7           1.0132            0.56s\n",
      "         8           0.9994            0.54s\n",
      "         9           0.9912            0.53s\n",
      "        10           0.9782            0.51s\n",
      "        20           0.8955            0.45s\n",
      "        30           0.8575            0.38s\n",
      "        40           0.8357            0.31s\n",
      "        50           0.8208            0.26s\n",
      "        60           0.8104            0.20s\n",
      "        70           0.8009            0.15s\n",
      "        80           0.7932            0.10s\n",
      "        90           0.7869            0.05s\n",
      "       100           0.7803            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6903            0.50s\n",
      "         2           0.6678            0.49s\n",
      "         3           0.6538            0.49s\n",
      "         4           0.6414            0.46s\n",
      "         5           0.6225            0.51s\n",
      "         6           0.6155            0.53s\n",
      "         7           0.6070            0.55s\n",
      "         8           0.5975            0.53s\n",
      "         9           0.5917            0.52s\n",
      "        10           0.5840            0.51s\n",
      "        20           0.5372            0.44s\n",
      "        30           0.5102            0.38s\n",
      "        40           0.4928            0.32s\n",
      "        50           0.4811            0.27s\n",
      "        60           0.4720            0.21s\n",
      "        70           0.4638            0.16s\n",
      "        80           0.4565            0.11s\n",
      "        90           0.4500            0.05s\n",
      "       100           0.4445            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6362            0.39s\n",
      "         2           0.6107            0.44s\n",
      "         3           0.5875            0.44s\n",
      "         4           0.5789            0.45s\n",
      "         5           0.5704            0.49s\n",
      "         6           0.5600            0.52s\n",
      "         7           0.5491            0.55s\n",
      "         8           0.5371            0.58s\n",
      "         9           0.5310            0.56s\n",
      "        10           0.5247            0.55s\n",
      "        20           0.4779            0.47s\n",
      "        30           0.4517            0.40s\n",
      "        40           0.4329            0.33s\n",
      "        50           0.4210            0.28s\n",
      "        60           0.4125            0.22s\n",
      "        70           0.4029            0.16s\n",
      "        80           0.3956            0.11s\n",
      "        90           0.3903            0.05s\n",
      "       100           0.3841            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6161            0.50s\n",
      "         2           0.6133            0.49s\n",
      "         3           0.6097            0.45s\n",
      "         4           0.6070            0.43s\n",
      "         5           0.6038            0.44s\n",
      "         6           0.6016            0.44s\n",
      "         7           0.5992            0.43s\n",
      "         8           0.5969            0.43s\n",
      "         9           0.5949            0.46s\n",
      "        10           0.5930            0.45s\n",
      "        20           0.5780            0.40s\n",
      "        30           0.5659            0.36s\n",
      "        40           0.5555            0.32s\n",
      "        50           0.5456            0.26s\n",
      "        60           0.5382            0.21s\n",
      "        70           0.5312            0.16s\n",
      "        80           0.5244            0.10s\n",
      "        90           0.5188            0.05s\n",
      "       100           0.5136            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7202            0.49s\n",
      "         2           0.7129            0.49s\n",
      "         3           0.7047            0.50s\n",
      "         4           0.6957            0.49s\n",
      "         5           0.6894            0.54s\n",
      "         6           0.6817            0.52s\n",
      "         7           0.6748            0.52s\n",
      "         8           0.6704            0.53s\n",
      "         9           0.6667            0.52s\n",
      "        10           0.6641            0.50s\n",
      "        20           0.6278            0.42s\n",
      "        30           0.6036            0.36s\n",
      "        40           0.5866            0.32s\n",
      "        50           0.5734            0.27s\n",
      "        60           0.5635            0.21s\n",
      "        70           0.5548            0.16s\n",
      "        80           0.5476            0.11s\n",
      "        90           0.5417            0.06s\n",
      "       100           0.5351            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6292            0.50s\n",
      "         2           0.6146            0.54s\n",
      "         3           0.5912            0.58s\n",
      "         4           0.5790            0.58s\n",
      "         5           0.5657            0.57s\n",
      "         6           0.5520            0.58s\n",
      "         7           0.5427            0.56s\n",
      "         8           0.5359            0.53s\n",
      "         9           0.5283            0.51s\n",
      "        10           0.5175            0.50s\n",
      "        20           0.4735            0.43s\n",
      "        30           0.4460            0.39s\n",
      "        40           0.4268            0.32s\n",
      "        50           0.4133            0.26s\n",
      "        60           0.4038            0.21s\n",
      "        70           0.3947            0.15s\n",
      "        80           0.3880            0.10s\n",
      "        90           0.3821            0.05s\n",
      "       100           0.3747            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.4777            0.49s\n",
      "         2           0.4718            0.49s\n",
      "         3           0.4669            0.48s\n",
      "         4           0.4623            0.48s\n",
      "         5           0.4579            0.50s\n",
      "         6           0.4541            0.52s\n",
      "         7           0.4505            0.51s\n",
      "         8           0.4472            0.50s\n",
      "         9           0.4450            0.48s\n",
      "        10           0.4424            0.47s\n",
      "        20           0.4206            0.40s\n",
      "        30           0.4065            0.42s\n",
      "        40           0.3952            0.35s\n",
      "        50           0.3846            0.29s\n",
      "        60           0.3758            0.22s\n",
      "        70           0.3684            0.16s\n",
      "        80           0.3619            0.11s\n",
      "        90           0.3555            0.05s\n",
      "       100           0.3489            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.1153            0.42s\n",
      "         2           0.1125            0.45s\n",
      "         3           0.1108            0.53s\n",
      "         4           0.1089            0.53s\n",
      "         5           0.1030            0.54s\n",
      "         6           0.1012            0.51s\n",
      "         7           0.0994            0.50s\n",
      "         8           0.0967            0.49s\n",
      "         9           0.0944            0.47s\n",
      "        10           0.0934            0.46s\n",
      "        20           0.0821            0.39s\n",
      "        30           0.0760            0.35s\n",
      "        40           0.0705            0.30s\n",
      "        50           0.0676            0.25s\n",
      "        60           0.0646            0.20s\n",
      "        70           0.0620            0.15s\n",
      "        80           0.0598            0.10s\n",
      "        90           0.0575            0.05s\n",
      "       100           0.0550            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.4884            0.49s\n",
      "         2           0.4785            0.74s\n",
      "         3           0.4682            0.70s\n",
      "         4           0.4611            0.69s\n",
      "         5           0.4533            0.68s\n",
      "         6           0.4466            0.64s\n",
      "         7           0.4409            0.61s\n",
      "         8           0.4350            0.59s\n",
      "         9           0.4319            0.56s\n",
      "        10           0.4278            0.54s\n",
      "        20           0.4015            0.47s\n",
      "        30           0.3835            0.40s\n",
      "        40           0.3717            0.34s\n",
      "        50           0.3627            0.28s\n",
      "        60           0.3556            0.22s\n",
      "        70           0.3492            0.17s\n",
      "        80           0.3437            0.11s\n",
      "        90           0.3392            0.05s\n",
      "       100           0.3353            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0949            0.78s\n",
      "         2           0.0941            0.59s\n",
      "         3           0.0933            0.52s\n",
      "         4           0.0850            0.50s\n",
      "         5           0.0842            0.50s\n",
      "         6           0.0834            0.51s\n",
      "         7           0.0825            0.52s\n",
      "         8           0.0820            0.52s\n",
      "         9           0.0810            0.50s\n",
      "        10           0.0800            0.49s\n",
      "        20           0.0720            0.44s\n",
      "        30           0.0674            0.36s\n",
      "        40           0.0625            0.32s\n",
      "        50           0.0584            0.27s\n",
      "        60           0.0554            0.21s\n",
      "        70           0.0529            0.16s\n",
      "        80           0.0499            0.11s\n",
      "        90           0.0476            0.05s\n",
      "       100           0.0458            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5725            0.39s\n",
      "         2           0.5631            0.44s\n",
      "         3           0.5528            0.45s\n",
      "         4           0.5411            0.43s\n",
      "         5           0.5305            0.44s\n",
      "         6           0.5239            0.46s\n",
      "         7           0.5182            0.48s\n",
      "         8           0.5120            0.48s\n",
      "         9           0.5074            0.48s\n",
      "        10           0.5037            0.46s\n",
      "        20           0.4785            0.40s\n",
      "        30           0.4621            0.34s\n",
      "        40           0.4512            0.31s\n",
      "        50           0.4414            0.26s\n",
      "        60           0.4344            0.21s\n",
      "        70           0.4286            0.15s\n",
      "        80           0.4235            0.10s\n",
      "        90           0.4180            0.05s\n",
      "       100           0.4138            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5065            0.40s\n",
      "         2           0.4910            0.49s\n",
      "         3           0.4822            0.48s\n",
      "         4           0.4741            0.46s\n",
      "         5           0.4649            0.47s\n",
      "         6           0.4591            0.45s\n",
      "         7           0.4537            0.45s\n",
      "         8           0.4494            0.45s\n",
      "         9           0.4470            0.44s\n",
      "        10           0.4428            0.44s\n",
      "        20           0.4154            0.41s\n",
      "        30           0.3978            0.36s\n",
      "        40           0.3860            0.36s\n",
      "        50           0.3765            0.29s\n",
      "        60           0.3693            0.23s\n",
      "        70           0.3627            0.17s\n",
      "        80           0.3565            0.11s\n",
      "        90           0.3506            0.06s\n",
      "       100           0.3450            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5099            0.49s\n",
      "         2           0.4956            0.54s\n",
      "         3           0.4745            0.52s\n",
      "         4           0.4644            0.50s\n",
      "         5           0.4515            0.50s\n",
      "         6           0.4398            0.49s\n",
      "         7           0.4318            0.47s\n",
      "         8           0.4266            0.47s\n",
      "         9           0.4215            0.48s\n",
      "        10           0.4135            0.50s\n",
      "        20           0.3785            0.45s\n",
      "        30           0.3562            0.37s\n",
      "        40           0.3420            0.32s\n",
      "        50           0.3318            0.26s\n",
      "        60           0.3236            0.21s\n",
      "        70           0.3163            0.16s\n",
      "        80           0.3103            0.10s\n",
      "        90           0.3051            0.05s\n",
      "       100           0.2995            0.00s\n"
     ]
    }
   ],
   "source": [
    "# we need to analyze the Errors made by the Best Model which is random Forest\n",
    "gd_clf_predictions = cross_val_predict(gd_clf,x_train,y_train, cv=3,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85548810-1925-4e27-8b5e-2571eb776cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Developer, embedded applications or devices</th>\n",
       "      <td>90.59</td>\n",
       "      <td>60.85</td>\n",
       "      <td>53.34</td>\n",
       "      <td>54.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Database administrator</th>\n",
       "      <td>92.07</td>\n",
       "      <td>64.94</td>\n",
       "      <td>51.16</td>\n",
       "      <td>50.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blockchain</th>\n",
       "      <td>93.63</td>\n",
       "      <td>66.39</td>\n",
       "      <td>52.82</td>\n",
       "      <td>53.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, back-end</th>\n",
       "      <td>72.63</td>\n",
       "      <td>66.77</td>\n",
       "      <td>60.09</td>\n",
       "      <td>60.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data or business analyst</th>\n",
       "      <td>91.21</td>\n",
       "      <td>67.54</td>\n",
       "      <td>52.86</td>\n",
       "      <td>53.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scientist</th>\n",
       "      <td>90.78</td>\n",
       "      <td>69.21</td>\n",
       "      <td>63.62</td>\n",
       "      <td>65.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cloud infrastructure engineer</th>\n",
       "      <td>89.76</td>\n",
       "      <td>69.22</td>\n",
       "      <td>59.02</td>\n",
       "      <td>61.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Academic researcher</th>\n",
       "      <td>88.77</td>\n",
       "      <td>69.46</td>\n",
       "      <td>64.68</td>\n",
       "      <td>66.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, game or graphics</th>\n",
       "      <td>93.25</td>\n",
       "      <td>69.47</td>\n",
       "      <td>55.99</td>\n",
       "      <td>58.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>System administrator</th>\n",
       "      <td>90.73</td>\n",
       "      <td>71.25</td>\n",
       "      <td>53.70</td>\n",
       "      <td>54.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, desktop or enterprise applications</th>\n",
       "      <td>87.78</td>\n",
       "      <td>72.32</td>\n",
       "      <td>58.10</td>\n",
       "      <td>60.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, full-stack</th>\n",
       "      <td>78.34</td>\n",
       "      <td>72.74</td>\n",
       "      <td>70.23</td>\n",
       "      <td>71.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engineer, data</th>\n",
       "      <td>91.01</td>\n",
       "      <td>73.04</td>\n",
       "      <td>52.83</td>\n",
       "      <td>53.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, front-end</th>\n",
       "      <td>88.55</td>\n",
       "      <td>73.09</td>\n",
       "      <td>58.91</td>\n",
       "      <td>61.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DevOps specialist</th>\n",
       "      <td>89.43</td>\n",
       "      <td>73.20</td>\n",
       "      <td>61.50</td>\n",
       "      <td>64.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data scientist or machine learning specialist</th>\n",
       "      <td>90.22</td>\n",
       "      <td>75.04</td>\n",
       "      <td>65.74</td>\n",
       "      <td>68.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security professional</th>\n",
       "      <td>93.55</td>\n",
       "      <td>82.02</td>\n",
       "      <td>52.43</td>\n",
       "      <td>53.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, mobile</th>\n",
       "      <td>93.78</td>\n",
       "      <td>84.32</td>\n",
       "      <td>73.71</td>\n",
       "      <td>77.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Developer, QA or test</th>\n",
       "      <td>93.33</td>\n",
       "      <td>87.30</td>\n",
       "      <td>50.83</td>\n",
       "      <td>49.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               accuracy_score  \\\n",
       "Developer, embedded applications or devices             90.59   \n",
       "Database administrator                                  92.07   \n",
       "Blockchain                                              93.63   \n",
       "Developer, back-end                                     72.63   \n",
       "Data or business analyst                                91.21   \n",
       "Scientist                                               90.78   \n",
       "Cloud infrastructure engineer                           89.76   \n",
       "Academic researcher                                     88.77   \n",
       "Developer, game or graphics                             93.25   \n",
       "System administrator                                    90.73   \n",
       "Developer, desktop or enterprise applications           87.78   \n",
       "Developer, full-stack                                   78.34   \n",
       "Engineer, data                                          91.01   \n",
       "Developer, front-end                                    88.55   \n",
       "DevOps specialist                                       89.43   \n",
       "Data scientist or machine learning specialist           90.22   \n",
       "Security professional                                   93.55   \n",
       "Developer, mobile                                       93.78   \n",
       "Developer, QA or test                                   93.33   \n",
       "\n",
       "                                               precision_score  recall_score  \\\n",
       "Developer, embedded applications or devices              60.85         53.34   \n",
       "Database administrator                                   64.94         51.16   \n",
       "Blockchain                                               66.39         52.82   \n",
       "Developer, back-end                                      66.77         60.09   \n",
       "Data or business analyst                                 67.54         52.86   \n",
       "Scientist                                                69.21         63.62   \n",
       "Cloud infrastructure engineer                            69.22         59.02   \n",
       "Academic researcher                                      69.46         64.68   \n",
       "Developer, game or graphics                              69.47         55.99   \n",
       "System administrator                                     71.25         53.70   \n",
       "Developer, desktop or enterprise applications            72.32         58.10   \n",
       "Developer, full-stack                                    72.74         70.23   \n",
       "Engineer, data                                           73.04         52.83   \n",
       "Developer, front-end                                     73.09         58.91   \n",
       "DevOps specialist                                        73.20         61.50   \n",
       "Data scientist or machine learning specialist            75.04         65.74   \n",
       "Security professional                                    82.02         52.43   \n",
       "Developer, mobile                                        84.32         73.71   \n",
       "Developer, QA or test                                    87.30         50.83   \n",
       "\n",
       "                                               f1_score  \n",
       "Developer, embedded applications or devices       54.14  \n",
       "Database administrator                            50.45  \n",
       "Blockchain                                        53.75  \n",
       "Developer, back-end                               60.55  \n",
       "Data or business analyst                          53.35  \n",
       "Scientist                                         65.76  \n",
       "Cloud infrastructure engineer                     61.56  \n",
       "Academic researcher                               66.57  \n",
       "Developer, game or graphics                       58.38  \n",
       "System administrator                              54.62  \n",
       "Developer, desktop or enterprise applications     60.44  \n",
       "Developer, full-stack                             71.23  \n",
       "Engineer, data                                    53.18  \n",
       "Developer, front-end                              61.60  \n",
       "DevOps specialist                                 64.61  \n",
       "Data scientist or machine learning specialist     68.92  \n",
       "Security professional                             53.01  \n",
       "Developer, mobile                                 77.79  \n",
       "Developer, QA or test                             49.93  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe contains the predictions \n",
    "gd_clf_predictions =  pd.DataFrame(gd_clf_predictions,\n",
    "                            columns=y_train.columns)\n",
    "\n",
    "gd_clf_prediction_scores = {score.__name__: predictions_per_col(gd_clf_predictions, y_train, score) \n",
    "                for score in [accuracy_score, precision_score, recall_score, f1_score]}\n",
    "\n",
    "gd_clf_prediction_scores = pd.concat(gd_clf_prediction_scores,axis=1)\n",
    "gd_clf_prediction_scores.sort_values(\"precision_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eedfbfec-8ee8-4d65-940a-c7eee97231bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives per Class: [63, 8, 15, 205, 54, 35, 313, 120, 95, 9, 9, 419, 10, 116, 66, 28, 85, 128, 155]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# Assuming y_true and y_pred are your true and predicted labels for multilabel classification\n",
    "# y_true and y_pred should be binary indicator matrices (0 or 1 values)\n",
    "\n",
    "confusion_matrices = multilabel_confusion_matrix(y_train, rf_clf_predictions)\n",
    "\n",
    "# Initialize an empty list to store false positives for each class\n",
    "false_positives_per_class = []\n",
    "\n",
    "# Iterate over each confusion matrix and extract false positives\n",
    "for cm in confusion_matrices:\n",
    "    # For binary classification, false positives are in the top right corner of the matrix\n",
    "    false_positives = cm[0, 1]\n",
    "    false_positives_per_class.append(false_positives)\n",
    "\n",
    "# false_positives_per_class now contains the number of false positives for each class\n",
    "print(\"False Positives per Class:\", false_positives_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56f46e-bfc1-494b-9eaa-f4c2a24ccb1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecd61c-27f0-49e2-bc9f-17a4cf81dd9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blockchain                                         447\n",
       "Security professional                              485\n",
       "Developer, game or graphics                        640\n",
       "Scientist                                          736\n",
       "Developer, QA or test                              807\n",
       "Database administrator                             831\n",
       "Data or business analyst                          1009\n",
       "System administrator                              1170\n",
       "Academic researcher                               1243\n",
       "Engineer, data                                    1329\n",
       "Developer, embedded applications or devices       1484\n",
       "Data scientist or machine learning specialist     1546\n",
       "Cloud infrastructure engineer                     1655\n",
       "DevOps specialist                                 1985\n",
       "Developer, mobile                                 2994\n",
       "Developer, desktop or enterprise applications     3452\n",
       "Developer, front-end                              6392\n",
       "Developer, back-end                              12785\n",
       "Developer, full-stack                            14183\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f1ac3d-c555-4fc1-b04b-a020dd4980dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DownSample majority classes and OverSample minority Classes of training set\n",
    "samples_per_class = 600\n",
    "resampled_jobs = []\n",
    "\n",
    "for job in y_train.columns:\n",
    "    sub_df = y_train.loc[y_train[job] == 1].copy()\n",
    "    \n",
    "    if len(sub_df) < samples_per_class:\n",
    "        # Upsample\n",
    "        sub_df = sub_df.sample(samples_per_class, replace=True, random_state=42)\n",
    "    else:\n",
    "        # Downsample\n",
    "        sub_df = sub_df.sample(samples_per_class, random_state=42) \n",
    "    \n",
    "    resampled_jobs.append(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c788b00-21eb-4cfb-9fad-e21a84e3d30d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blockchain                                        700\n",
       "Developer, game or graphics                       747\n",
       "Security professional                             757\n",
       "Developer, QA or test                             770\n",
       "Database administrator                            888\n",
       "Developer, embedded applications or devices       960\n",
       "Scientist                                         980\n",
       "Data or business analyst                          982\n",
       "Developer, mobile                                1035\n",
       "Engineer, data                                   1036\n",
       "System administrator                             1061\n",
       "Cloud infrastructure engineer                    1134\n",
       "Academic researcher                              1219\n",
       "Data scientist or machine learning specialist    1227\n",
       "DevOps specialist                                1265\n",
       "Developer, front-end                             1365\n",
       "Developer, desktop or enterprise applications    1445\n",
       "Developer, full-stack                            3117\n",
       "Developer, back-end                              3405\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.concat(resampled_jobs)\n",
    "x_train = x_train.loc[y_train.index].copy()\n",
    "y_train.sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2135471e-3d39-4b8b-8da6-e39fa8c70c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DownSample majority classes and OverSample minority Classes of training set\n",
    "samples_per_class = 400\n",
    "resampled_jobs = []\n",
    "\n",
    "for job in y_test.columns:\n",
    "    sub_df = y_test.loc[y_test[job] == 1].copy()\n",
    "    \n",
    "    if len(sub_df) < samples_per_class:\n",
    "        # Upsample\n",
    "        sub_df = sub_df.sample(samples_per_class, replace=True, random_state=42)\n",
    "    else:\n",
    "        # Downsample\n",
    "        sub_df = sub_df.sample(samples_per_class, random_state=42) \n",
    "    \n",
    "    resampled_jobs.append(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bbc5f72-6125-4b03-b9ff-790e6bb26c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blockchain                                        473\n",
       "Developer, game or graphics                       504\n",
       "Security professional                             523\n",
       "Developer, QA or test                             525\n",
       "Developer, embedded applications or devices       608\n",
       "Database administrator                            616\n",
       "Scientist                                         628\n",
       "Data or business analyst                          642\n",
       "Developer, mobile                                 680\n",
       "System administrator                              714\n",
       "Engineer, data                                    722\n",
       "Cloud infrastructure engineer                     796\n",
       "Data scientist or machine learning specialist     833\n",
       "DevOps specialist                                 836\n",
       "Academic researcher                               840\n",
       "Developer, front-end                              891\n",
       "Developer, desktop or enterprise applications    1001\n",
       "Developer, full-stack                            2005\n",
       "Developer, back-end                              2092\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.concat(resampled_jobs)\n",
    "x_test = x_test.loc[y_test.index].copy()\n",
    "y_test.sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765afb5-7f68-4c71-b97b-098d43e66535",
   "metadata": {},
   "source": [
    "## Retrrieve multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c8ef5-a2c2-4ec7-904f-2d578819131e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artifact_paths = runs[\"artifact_uri\"].str.replace(\"file:///\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba64c95-0296-4c96-b96c-b56b2c15e2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clfs = list(range(6))\n",
    "i = 0\n",
    "for path in artifact_paths:\n",
    "    model_pkl = os.path.join(path, LOG_MODEL_PKL)\n",
    "    with open(model_pkl, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "        clfs[i] = model\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b5adc-7bb0-4815-a95a-94a38d4b413a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voting_clf = clfs[0]['model_object']\n",
    "cat_clf = clfs[1]['model_object']\n",
    "gd_clf =clfs[2]['model_object']\n",
    "dec_clf =clfs[3]['model_object']\n",
    "rf_clf = clfs[4]['model_object']\n",
    "log_clf = clfs[5]['model_object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856205d-8f69-4bcc-b113-cc85c752d7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_run= runs.sort_values('metrics.test_precision',ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8197022-401a-44ed-a88c-047800b82822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_id                                      465c94b53d7040648b0e701c656fd424\n",
       "experiment_id                                             996200319116358272\n",
       "status                                                              FINISHED\n",
       "artifact_uri               file:///C:/Users/Ali/Desktop/DS Projects/Tech ...\n",
       "start_time                                  2024-01-28 20:19:46.125000+00:00\n",
       "end_time                                    2024-01-28 20:19:46.394000+00:00\n",
       "metrics.test_accuracy                                                  92.15\n",
       "metrics.test_recall                                                    76.29\n",
       "metrics.test_f1                                                        78.92\n",
       "metrics.test_precision                                                 89.16\n",
       "tags.mlflow.user                                                         Ali\n",
       "tags.mlflow.source.type                                                LOCAL\n",
       "tags.mlflow.source.name    C:\\Users\\Ali\\mambaforge-pypy3\\envs\\env1\\Lib\\si...\n",
       "tags.mlflow.runName                Random Forest, multilabel, Data resampled\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef218bf-da61-42d0-90d1-a31c32d667c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artifact_path = best_run[\"artifact_uri\"].replace(\"file:///\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d1f87-f8ae-4adf-a0fd-7362651c8536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(n_jobs=-1, random_state=42,\n",
       "                                        verbose=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(n_jobs=-1, random_state=42,\n",
       "                                        verbose=1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1, random_state=42, verbose=1)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('randomforestclassifier',\n",
       "                 RandomForestClassifier(n_jobs=-1, random_state=42,\n",
       "                                        verbose=1))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pkl = os.path.join(artifact_path, LOG_MODEL_PKL)\n",
    "with open(model_pkl, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model['model_object']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33bccd-9686-4f3b-abcf-fd5ff15096be",
   "metadata": {},
   "source": [
    "### Recommend Skills by clusters then skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ed86163b-22fb-4b8b-915a-8dae33caecf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulated_results = []\n",
    "for cluster, skills in clusters_config.items():\n",
    "    additional_skill_prob = model.predict_job_probabilities(set(skills + entry_skills))\n",
    "    additional_skill_uplift = (additional_skill_prob - base_predictions) / base_predictions\n",
    "    additional_skill_uplift.name = cluster\n",
    "    simulated_results.append(additional_skill_uplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1bcfa39f-d5c7-4187-8c5b-7f55e8856455",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data scientist or machine learning specialist</th>\n",
       "      <th>Engineer, data</th>\n",
       "      <th>Data or business analyst</th>\n",
       "      <th>Developer, back-end</th>\n",
       "      <th>Database administrator</th>\n",
       "      <th>Developer, mobile</th>\n",
       "      <th>Developer, full-stack</th>\n",
       "      <th>Cloud infrastructure engineer</th>\n",
       "      <th>Developer, embedded applications or devices</th>\n",
       "      <th>Developer, QA or test</th>\n",
       "      <th>System administrator</th>\n",
       "      <th>Scientist</th>\n",
       "      <th>Security professional</th>\n",
       "      <th>Developer, game or graphics</th>\n",
       "      <th>Developer, front-end</th>\n",
       "      <th>Blockchain</th>\n",
       "      <th>Developer, desktop or enterprise applications</th>\n",
       "      <th>DevOps specialist</th>\n",
       "      <th>Academic researcher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>skills_group_0</th>\n",
       "      <td>1.517494</td>\n",
       "      <td>-0.417553</td>\n",
       "      <td>0.597069</td>\n",
       "      <td>-0.916752</td>\n",
       "      <td>-0.054370</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.509357</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.354765</td>\n",
       "      <td>0.122838</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.261794</td>\n",
       "      <td>-0.162034</td>\n",
       "      <td>-0.650385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.329480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_1</th>\n",
       "      <td>-0.640197</td>\n",
       "      <td>-0.203311</td>\n",
       "      <td>-0.230509</td>\n",
       "      <td>0.307620</td>\n",
       "      <td>2.043660</td>\n",
       "      <td>-0.394241</td>\n",
       "      <td>2.666041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.651572</td>\n",
       "      <td>1.004194</td>\n",
       "      <td>1.184363</td>\n",
       "      <td>-0.598746</td>\n",
       "      <td>0.203870</td>\n",
       "      <td>-0.150731</td>\n",
       "      <td>6.778736</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.671469</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.299176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_10</th>\n",
       "      <td>-0.312854</td>\n",
       "      <td>0.014441</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>2.581554</td>\n",
       "      <td>0.120411</td>\n",
       "      <td>-0.252599</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.053671</td>\n",
       "      <td>-0.208691</td>\n",
       "      <td>-0.015893</td>\n",
       "      <td>-0.416301</td>\n",
       "      <td>-0.762485</td>\n",
       "      <td>0.241270</td>\n",
       "      <td>1.036621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358787</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.160908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_11</th>\n",
       "      <td>-0.614438</td>\n",
       "      <td>-0.556184</td>\n",
       "      <td>-0.397352</td>\n",
       "      <td>-0.075253</td>\n",
       "      <td>0.657549</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.909005</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.699259</td>\n",
       "      <td>1.112895</td>\n",
       "      <td>-0.164621</td>\n",
       "      <td>-0.722157</td>\n",
       "      <td>-0.431295</td>\n",
       "      <td>0.146700</td>\n",
       "      <td>53.417014</td>\n",
       "      <td>2.744693</td>\n",
       "      <td>-0.744021</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.469353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_12</th>\n",
       "      <td>-0.469629</td>\n",
       "      <td>-0.414624</td>\n",
       "      <td>-0.281260</td>\n",
       "      <td>-0.087787</td>\n",
       "      <td>0.226833</td>\n",
       "      <td>5.868242</td>\n",
       "      <td>-0.419962</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.390197</td>\n",
       "      <td>0.243065</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>-0.279943</td>\n",
       "      <td>-0.239991</td>\n",
       "      <td>0.808071</td>\n",
       "      <td>1.065725</td>\n",
       "      <td>2.025803</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.136599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_13</th>\n",
       "      <td>-0.650933</td>\n",
       "      <td>-0.513576</td>\n",
       "      <td>-0.704749</td>\n",
       "      <td>-0.153107</td>\n",
       "      <td>0.841805</td>\n",
       "      <td>-0.683204</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.441872</td>\n",
       "      <td>2.246270</td>\n",
       "      <td>-0.263951</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>0.696122</td>\n",
       "      <td>2.818621</td>\n",
       "      <td>-0.824704</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.242668</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.195871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_14</th>\n",
       "      <td>-0.587044</td>\n",
       "      <td>-0.210024</td>\n",
       "      <td>-0.795543</td>\n",
       "      <td>1.598295</td>\n",
       "      <td>0.045956</td>\n",
       "      <td>-0.512044</td>\n",
       "      <td>0.245822</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.691525</td>\n",
       "      <td>0.282931</td>\n",
       "      <td>0.076672</td>\n",
       "      <td>-0.493489</td>\n",
       "      <td>2.488181</td>\n",
       "      <td>-0.160835</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.007848</td>\n",
       "      <td>-0.502433</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.612703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_15</th>\n",
       "      <td>-0.306133</td>\n",
       "      <td>-0.332833</td>\n",
       "      <td>-0.351245</td>\n",
       "      <td>0.676284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.287712</td>\n",
       "      <td>0.044069</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.411474</td>\n",
       "      <td>0.175340</td>\n",
       "      <td>0.179318</td>\n",
       "      <td>0.196965</td>\n",
       "      <td>-0.478516</td>\n",
       "      <td>-0.013239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.498812</td>\n",
       "      <td>0.237591</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.554479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_16</th>\n",
       "      <td>-0.411330</td>\n",
       "      <td>0.041472</td>\n",
       "      <td>0.048072</td>\n",
       "      <td>0.500251</td>\n",
       "      <td>0.685277</td>\n",
       "      <td>0.558838</td>\n",
       "      <td>0.699218</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.451416</td>\n",
       "      <td>0.023490</td>\n",
       "      <td>0.546017</td>\n",
       "      <td>-0.606420</td>\n",
       "      <td>-0.213486</td>\n",
       "      <td>2.485132</td>\n",
       "      <td>10.237299</td>\n",
       "      <td>0.965202</td>\n",
       "      <td>0.156124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_17</th>\n",
       "      <td>-0.356376</td>\n",
       "      <td>-0.504425</td>\n",
       "      <td>-0.550312</td>\n",
       "      <td>0.616092</td>\n",
       "      <td>0.938065</td>\n",
       "      <td>0.648874</td>\n",
       "      <td>-0.172539</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.566963</td>\n",
       "      <td>-0.146769</td>\n",
       "      <td>0.046263</td>\n",
       "      <td>-0.182764</td>\n",
       "      <td>2.270933</td>\n",
       "      <td>4.847263</td>\n",
       "      <td>3.090484</td>\n",
       "      <td>3.548800</td>\n",
       "      <td>0.442473</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.214999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_18</th>\n",
       "      <td>-0.100783</td>\n",
       "      <td>-0.099950</td>\n",
       "      <td>-0.114530</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>-0.288280</td>\n",
       "      <td>0.191833</td>\n",
       "      <td>0.089409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.200676</td>\n",
       "      <td>0.515227</td>\n",
       "      <td>-0.106661</td>\n",
       "      <td>-0.207635</td>\n",
       "      <td>0.251036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098367</td>\n",
       "      <td>1.015848</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.060068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_19</th>\n",
       "      <td>-0.051636</td>\n",
       "      <td>-0.002021</td>\n",
       "      <td>-0.020581</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.039036</td>\n",
       "      <td>0.373093</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.029553</td>\n",
       "      <td>0.186353</td>\n",
       "      <td>-0.095006</td>\n",
       "      <td>-0.048874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.175322</td>\n",
       "      <td>1.541766</td>\n",
       "      <td>1.015350</td>\n",
       "      <td>-0.063914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_2</th>\n",
       "      <td>0.534456</td>\n",
       "      <td>0.985103</td>\n",
       "      <td>0.678671</td>\n",
       "      <td>1.100918</td>\n",
       "      <td>0.760199</td>\n",
       "      <td>-0.694461</td>\n",
       "      <td>0.229385</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.464502</td>\n",
       "      <td>1.116843</td>\n",
       "      <td>-0.247126</td>\n",
       "      <td>-0.727216</td>\n",
       "      <td>-0.977758</td>\n",
       "      <td>-0.712825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.434614</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.674749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_20</th>\n",
       "      <td>-0.135479</td>\n",
       "      <td>0.037610</td>\n",
       "      <td>-0.051413</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>-0.039780</td>\n",
       "      <td>-0.092306</td>\n",
       "      <td>0.464943</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.319241</td>\n",
       "      <td>-0.117532</td>\n",
       "      <td>0.287037</td>\n",
       "      <td>-0.105714</td>\n",
       "      <td>-0.018329</td>\n",
       "      <td>0.856361</td>\n",
       "      <td>2.644405</td>\n",
       "      <td>2.016303</td>\n",
       "      <td>-0.044407</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_21</th>\n",
       "      <td>-0.650650</td>\n",
       "      <td>-0.343149</td>\n",
       "      <td>-0.719939</td>\n",
       "      <td>-0.274540</td>\n",
       "      <td>0.556280</td>\n",
       "      <td>-0.499533</td>\n",
       "      <td>-0.254126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.817656</td>\n",
       "      <td>-0.325161</td>\n",
       "      <td>0.607456</td>\n",
       "      <td>-0.180174</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.163230</td>\n",
       "      <td>0.609554</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.312719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.232863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_22</th>\n",
       "      <td>-0.186196</td>\n",
       "      <td>0.176287</td>\n",
       "      <td>-0.039878</td>\n",
       "      <td>0.315353</td>\n",
       "      <td>0.166507</td>\n",
       "      <td>-0.037797</td>\n",
       "      <td>0.240425</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.171774</td>\n",
       "      <td>0.191458</td>\n",
       "      <td>-0.016357</td>\n",
       "      <td>-0.203274</td>\n",
       "      <td>0.827789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.068463</td>\n",
       "      <td>2.030699</td>\n",
       "      <td>-0.088808</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.135796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_23</th>\n",
       "      <td>-0.241807</td>\n",
       "      <td>0.543208</td>\n",
       "      <td>-0.294804</td>\n",
       "      <td>1.058027</td>\n",
       "      <td>0.485617</td>\n",
       "      <td>0.376670</td>\n",
       "      <td>-0.674405</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.537161</td>\n",
       "      <td>0.104606</td>\n",
       "      <td>0.462889</td>\n",
       "      <td>-0.551150</td>\n",
       "      <td>1.080518</td>\n",
       "      <td>-0.452958</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.464840</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.045546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_24</th>\n",
       "      <td>0.342185</td>\n",
       "      <td>0.019116</td>\n",
       "      <td>-0.075215</td>\n",
       "      <td>0.129950</td>\n",
       "      <td>0.726066</td>\n",
       "      <td>-0.389022</td>\n",
       "      <td>-0.369232</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.722106</td>\n",
       "      <td>0.712324</td>\n",
       "      <td>-0.314348</td>\n",
       "      <td>0.341376</td>\n",
       "      <td>-0.762485</td>\n",
       "      <td>-0.378317</td>\n",
       "      <td>-0.076022</td>\n",
       "      <td>1.788803</td>\n",
       "      <td>-0.264230</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.190838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_25</th>\n",
       "      <td>-0.435840</td>\n",
       "      <td>-0.440842</td>\n",
       "      <td>-0.228487</td>\n",
       "      <td>0.282804</td>\n",
       "      <td>0.172667</td>\n",
       "      <td>1.674789</td>\n",
       "      <td>0.701197</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.599708</td>\n",
       "      <td>-0.144024</td>\n",
       "      <td>-0.043421</td>\n",
       "      <td>-0.225141</td>\n",
       "      <td>-0.211933</td>\n",
       "      <td>8.573560</td>\n",
       "      <td>1.036621</td>\n",
       "      <td>2.797836</td>\n",
       "      <td>-0.417659</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.251227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_26</th>\n",
       "      <td>-0.255513</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>-0.055810</td>\n",
       "      <td>0.327444</td>\n",
       "      <td>-0.169313</td>\n",
       "      <td>-0.399182</td>\n",
       "      <td>-0.006540</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.139870</td>\n",
       "      <td>0.217030</td>\n",
       "      <td>0.286916</td>\n",
       "      <td>-0.065419</td>\n",
       "      <td>1.193301</td>\n",
       "      <td>-0.141164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.917128</td>\n",
       "      <td>0.138442</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.092851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_27</th>\n",
       "      <td>-0.011894</td>\n",
       "      <td>0.047249</td>\n",
       "      <td>-0.099138</td>\n",
       "      <td>-0.077885</td>\n",
       "      <td>-0.386088</td>\n",
       "      <td>0.032214</td>\n",
       "      <td>-0.102019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.128035</td>\n",
       "      <td>-0.415957</td>\n",
       "      <td>-0.097161</td>\n",
       "      <td>-0.007781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.010563</td>\n",
       "      <td>0.081452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.137390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_28</th>\n",
       "      <td>-0.276586</td>\n",
       "      <td>-0.196097</td>\n",
       "      <td>-0.149499</td>\n",
       "      <td>0.725478</td>\n",
       "      <td>0.453342</td>\n",
       "      <td>-0.151209</td>\n",
       "      <td>0.677121</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.431895</td>\n",
       "      <td>0.917781</td>\n",
       "      <td>-0.408169</td>\n",
       "      <td>-0.447792</td>\n",
       "      <td>0.196613</td>\n",
       "      <td>-0.494265</td>\n",
       "      <td>4.311697</td>\n",
       "      <td>14.340527</td>\n",
       "      <td>-0.440161</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.094329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_29</th>\n",
       "      <td>-0.445276</td>\n",
       "      <td>-0.098741</td>\n",
       "      <td>-0.217214</td>\n",
       "      <td>0.915807</td>\n",
       "      <td>0.659938</td>\n",
       "      <td>1.228925</td>\n",
       "      <td>-0.019749</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.752378</td>\n",
       "      <td>1.249939</td>\n",
       "      <td>0.166354</td>\n",
       "      <td>-0.439618</td>\n",
       "      <td>-0.977758</td>\n",
       "      <td>0.997920</td>\n",
       "      <td>2.314699</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>1.164573</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.123334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_3</th>\n",
       "      <td>-0.507166</td>\n",
       "      <td>0.043211</td>\n",
       "      <td>-0.209348</td>\n",
       "      <td>0.018274</td>\n",
       "      <td>1.130763</td>\n",
       "      <td>-0.232994</td>\n",
       "      <td>0.125498</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.691294</td>\n",
       "      <td>-0.304506</td>\n",
       "      <td>1.268285</td>\n",
       "      <td>-0.361288</td>\n",
       "      <td>1.533028</td>\n",
       "      <td>1.482786</td>\n",
       "      <td>0.632368</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>-0.059597</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.185568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_30</th>\n",
       "      <td>-0.079168</td>\n",
       "      <td>-0.240000</td>\n",
       "      <td>-0.205072</td>\n",
       "      <td>-0.188169</td>\n",
       "      <td>-0.209922</td>\n",
       "      <td>-0.343307</td>\n",
       "      <td>-0.026302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.859648</td>\n",
       "      <td>0.125216</td>\n",
       "      <td>-0.181519</td>\n",
       "      <td>0.179602</td>\n",
       "      <td>0.271630</td>\n",
       "      <td>3.217218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.057930</td>\n",
       "      <td>0.509230</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.595838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_31</th>\n",
       "      <td>-0.437352</td>\n",
       "      <td>-0.129369</td>\n",
       "      <td>-0.311162</td>\n",
       "      <td>-0.328121</td>\n",
       "      <td>0.281354</td>\n",
       "      <td>5.223343</td>\n",
       "      <td>-0.550264</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.312544</td>\n",
       "      <td>-0.133399</td>\n",
       "      <td>0.122235</td>\n",
       "      <td>-0.255512</td>\n",
       "      <td>-0.487564</td>\n",
       "      <td>1.921109</td>\n",
       "      <td>7.817641</td>\n",
       "      <td>1.027635</td>\n",
       "      <td>0.788652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.342635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_32</th>\n",
       "      <td>-0.250467</td>\n",
       "      <td>-0.185671</td>\n",
       "      <td>0.014123</td>\n",
       "      <td>0.252229</td>\n",
       "      <td>0.319330</td>\n",
       "      <td>0.212998</td>\n",
       "      <td>0.914281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.129184</td>\n",
       "      <td>0.403749</td>\n",
       "      <td>0.104650</td>\n",
       "      <td>-0.147950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.699586</td>\n",
       "      <td>2.094485</td>\n",
       "      <td>1.008418</td>\n",
       "      <td>-0.225279</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.104082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_33</th>\n",
       "      <td>-0.402352</td>\n",
       "      <td>0.093830</td>\n",
       "      <td>-0.422812</td>\n",
       "      <td>0.219224</td>\n",
       "      <td>0.562402</td>\n",
       "      <td>-0.925937</td>\n",
       "      <td>-0.272044</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.161988</td>\n",
       "      <td>0.257020</td>\n",
       "      <td>2.268088</td>\n",
       "      <td>-0.504756</td>\n",
       "      <td>2.017154</td>\n",
       "      <td>-0.811439</td>\n",
       "      <td>0.546381</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>-0.035742</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.651990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_34</th>\n",
       "      <td>-0.209285</td>\n",
       "      <td>-0.169883</td>\n",
       "      <td>-0.003386</td>\n",
       "      <td>0.254957</td>\n",
       "      <td>0.498172</td>\n",
       "      <td>0.086064</td>\n",
       "      <td>0.892985</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.319241</td>\n",
       "      <td>0.685547</td>\n",
       "      <td>-0.202434</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>0.532885</td>\n",
       "      <td>0.009123</td>\n",
       "      <td>5.228890</td>\n",
       "      <td>1.015350</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.065173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_35</th>\n",
       "      <td>-0.339027</td>\n",
       "      <td>0.368915</td>\n",
       "      <td>0.284856</td>\n",
       "      <td>1.903484</td>\n",
       "      <td>2.614911</td>\n",
       "      <td>-0.199570</td>\n",
       "      <td>0.446511</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.839442</td>\n",
       "      <td>0.244268</td>\n",
       "      <td>-0.087710</td>\n",
       "      <td>-0.683689</td>\n",
       "      <td>-0.725405</td>\n",
       "      <td>-0.163380</td>\n",
       "      <td>0.390091</td>\n",
       "      <td>3.180525</td>\n",
       "      <td>-0.230537</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.387507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_36</th>\n",
       "      <td>-0.029647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011636</td>\n",
       "      <td>0.077258</td>\n",
       "      <td>-0.115710</td>\n",
       "      <td>-0.128836</td>\n",
       "      <td>-0.002469</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095679</td>\n",
       "      <td>-0.009409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.007116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_37</th>\n",
       "      <td>-0.232492</td>\n",
       "      <td>-0.325910</td>\n",
       "      <td>0.293412</td>\n",
       "      <td>1.408743</td>\n",
       "      <td>-0.314701</td>\n",
       "      <td>0.411631</td>\n",
       "      <td>1.338752</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.383428</td>\n",
       "      <td>0.388022</td>\n",
       "      <td>-0.334447</td>\n",
       "      <td>-0.294646</td>\n",
       "      <td>0.621430</td>\n",
       "      <td>0.890562</td>\n",
       "      <td>-0.526119</td>\n",
       "      <td>0.849729</td>\n",
       "      <td>0.086726</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.012750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_4</th>\n",
       "      <td>-0.649516</td>\n",
       "      <td>-0.508902</td>\n",
       "      <td>-0.488357</td>\n",
       "      <td>1.079184</td>\n",
       "      <td>1.963011</td>\n",
       "      <td>0.394454</td>\n",
       "      <td>1.551677</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.657881</td>\n",
       "      <td>0.119221</td>\n",
       "      <td>-0.051445</td>\n",
       "      <td>-0.538518</td>\n",
       "      <td>0.814089</td>\n",
       "      <td>1.998949</td>\n",
       "      <td>1.025694</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.940103</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.359071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_5</th>\n",
       "      <td>-0.746542</td>\n",
       "      <td>1.198882</td>\n",
       "      <td>1.095321</td>\n",
       "      <td>0.191095</td>\n",
       "      <td>2.891851</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.638654</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.601028</td>\n",
       "      <td>0.030055</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>-0.812488</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.665492</td>\n",
       "      <td>3.708018</td>\n",
       "      <td>0.376249</td>\n",
       "      <td>-0.234132</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.871520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_6</th>\n",
       "      <td>-0.088489</td>\n",
       "      <td>-0.161299</td>\n",
       "      <td>-0.083982</td>\n",
       "      <td>0.037519</td>\n",
       "      <td>0.935709</td>\n",
       "      <td>0.098013</td>\n",
       "      <td>0.364529</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.028501</td>\n",
       "      <td>0.172970</td>\n",
       "      <td>0.459311</td>\n",
       "      <td>-0.115400</td>\n",
       "      <td>0.019765</td>\n",
       "      <td>0.321269</td>\n",
       "      <td>0.603429</td>\n",
       "      <td>-0.005664</td>\n",
       "      <td>-0.163125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.054475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_7</th>\n",
       "      <td>-0.531493</td>\n",
       "      <td>-0.449850</td>\n",
       "      <td>-0.462415</td>\n",
       "      <td>-0.587379</td>\n",
       "      <td>-0.054281</td>\n",
       "      <td>10.718289</td>\n",
       "      <td>-0.224657</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.851573</td>\n",
       "      <td>0.025937</td>\n",
       "      <td>-0.613658</td>\n",
       "      <td>-0.644758</td>\n",
       "      <td>0.370625</td>\n",
       "      <td>0.168554</td>\n",
       "      <td>0.800246</td>\n",
       "      <td>0.050040</td>\n",
       "      <td>0.161697</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.001009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_8</th>\n",
       "      <td>-0.251357</td>\n",
       "      <td>-0.485740</td>\n",
       "      <td>0.033821</td>\n",
       "      <td>0.080769</td>\n",
       "      <td>1.019064</td>\n",
       "      <td>-0.565548</td>\n",
       "      <td>-0.132682</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.318403</td>\n",
       "      <td>1.301329</td>\n",
       "      <td>0.530425</td>\n",
       "      <td>0.184806</td>\n",
       "      <td>0.278909</td>\n",
       "      <td>0.631432</td>\n",
       "      <td>0.665235</td>\n",
       "      <td>-0.415453</td>\n",
       "      <td>0.587272</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.453217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skills_group_9</th>\n",
       "      <td>0.769869</td>\n",
       "      <td>-0.470228</td>\n",
       "      <td>0.310131</td>\n",
       "      <td>-0.908464</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.828226</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.905035</td>\n",
       "      <td>1.340360</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.665492</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.912017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.782368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Data scientist or machine learning specialist  \\\n",
       "skills_group_0                                        1.517494   \n",
       "skills_group_1                                       -0.640197   \n",
       "skills_group_10                                      -0.312854   \n",
       "skills_group_11                                      -0.614438   \n",
       "skills_group_12                                      -0.469629   \n",
       "skills_group_13                                      -0.650933   \n",
       "skills_group_14                                      -0.587044   \n",
       "skills_group_15                                      -0.306133   \n",
       "skills_group_16                                      -0.411330   \n",
       "skills_group_17                                      -0.356376   \n",
       "skills_group_18                                      -0.100783   \n",
       "skills_group_19                                      -0.051636   \n",
       "skills_group_2                                        0.534456   \n",
       "skills_group_20                                      -0.135479   \n",
       "skills_group_21                                      -0.650650   \n",
       "skills_group_22                                      -0.186196   \n",
       "skills_group_23                                      -0.241807   \n",
       "skills_group_24                                       0.342185   \n",
       "skills_group_25                                      -0.435840   \n",
       "skills_group_26                                      -0.255513   \n",
       "skills_group_27                                      -0.011894   \n",
       "skills_group_28                                      -0.276586   \n",
       "skills_group_29                                      -0.445276   \n",
       "skills_group_3                                       -0.507166   \n",
       "skills_group_30                                      -0.079168   \n",
       "skills_group_31                                      -0.437352   \n",
       "skills_group_32                                      -0.250467   \n",
       "skills_group_33                                      -0.402352   \n",
       "skills_group_34                                      -0.209285   \n",
       "skills_group_35                                      -0.339027   \n",
       "skills_group_36                                      -0.029647   \n",
       "skills_group_37                                      -0.232492   \n",
       "skills_group_4                                       -0.649516   \n",
       "skills_group_5                                       -0.746542   \n",
       "skills_group_6                                       -0.088489   \n",
       "skills_group_7                                       -0.531493   \n",
       "skills_group_8                                       -0.251357   \n",
       "skills_group_9                                        0.769869   \n",
       "\n",
       "                 Engineer, data  Data or business analyst  \\\n",
       "skills_group_0        -0.417553                  0.597069   \n",
       "skills_group_1        -0.203311                 -0.230509   \n",
       "skills_group_10        0.014441                  0.134700   \n",
       "skills_group_11       -0.556184                 -0.397352   \n",
       "skills_group_12       -0.414624                 -0.281260   \n",
       "skills_group_13       -0.513576                 -0.704749   \n",
       "skills_group_14       -0.210024                 -0.795543   \n",
       "skills_group_15       -0.332833                 -0.351245   \n",
       "skills_group_16        0.041472                  0.048072   \n",
       "skills_group_17       -0.504425                 -0.550312   \n",
       "skills_group_18       -0.099950                 -0.114530   \n",
       "skills_group_19       -0.002021                 -0.020581   \n",
       "skills_group_2         0.985103                  0.678671   \n",
       "skills_group_20        0.037610                 -0.051413   \n",
       "skills_group_21       -0.343149                 -0.719939   \n",
       "skills_group_22        0.176287                 -0.039878   \n",
       "skills_group_23        0.543208                 -0.294804   \n",
       "skills_group_24        0.019116                 -0.075215   \n",
       "skills_group_25       -0.440842                 -0.228487   \n",
       "skills_group_26        0.004470                 -0.055810   \n",
       "skills_group_27        0.047249                 -0.099138   \n",
       "skills_group_28       -0.196097                 -0.149499   \n",
       "skills_group_29       -0.098741                 -0.217214   \n",
       "skills_group_3         0.043211                 -0.209348   \n",
       "skills_group_30       -0.240000                 -0.205072   \n",
       "skills_group_31       -0.129369                 -0.311162   \n",
       "skills_group_32       -0.185671                  0.014123   \n",
       "skills_group_33        0.093830                 -0.422812   \n",
       "skills_group_34       -0.169883                 -0.003386   \n",
       "skills_group_35        0.368915                  0.284856   \n",
       "skills_group_36        0.000000                 -0.011636   \n",
       "skills_group_37       -0.325910                  0.293412   \n",
       "skills_group_4        -0.508902                 -0.488357   \n",
       "skills_group_5         1.198882                  1.095321   \n",
       "skills_group_6        -0.161299                 -0.083982   \n",
       "skills_group_7        -0.449850                 -0.462415   \n",
       "skills_group_8        -0.485740                  0.033821   \n",
       "skills_group_9        -0.470228                  0.310131   \n",
       "\n",
       "                 Developer, back-end  Database administrator  \\\n",
       "skills_group_0             -0.916752               -0.054370   \n",
       "skills_group_1              0.307620                2.043660   \n",
       "skills_group_10             0.716981                2.581554   \n",
       "skills_group_11            -0.075253                0.657549   \n",
       "skills_group_12            -0.087787                0.226833   \n",
       "skills_group_13            -0.153107                0.841805   \n",
       "skills_group_14             1.598295                0.045956   \n",
       "skills_group_15             0.676284                0.000000   \n",
       "skills_group_16             0.500251                0.685277   \n",
       "skills_group_17             0.616092                0.938065   \n",
       "skills_group_18             0.001259               -0.288280   \n",
       "skills_group_19            -0.000495               -0.039036   \n",
       "skills_group_2              1.100918                0.760199   \n",
       "skills_group_20             0.089958               -0.039780   \n",
       "skills_group_21            -0.274540                0.556280   \n",
       "skills_group_22             0.315353                0.166507   \n",
       "skills_group_23             1.058027                0.485617   \n",
       "skills_group_24             0.129950                0.726066   \n",
       "skills_group_25             0.282804                0.172667   \n",
       "skills_group_26             0.327444               -0.169313   \n",
       "skills_group_27            -0.077885               -0.386088   \n",
       "skills_group_28             0.725478                0.453342   \n",
       "skills_group_29             0.915807                0.659938   \n",
       "skills_group_3              0.018274                1.130763   \n",
       "skills_group_30            -0.188169               -0.209922   \n",
       "skills_group_31            -0.328121                0.281354   \n",
       "skills_group_32             0.252229                0.319330   \n",
       "skills_group_33             0.219224                0.562402   \n",
       "skills_group_34             0.254957                0.498172   \n",
       "skills_group_35             1.903484                2.614911   \n",
       "skills_group_36             0.077258               -0.115710   \n",
       "skills_group_37             1.408743               -0.314701   \n",
       "skills_group_4              1.079184                1.963011   \n",
       "skills_group_5              0.191095                2.891851   \n",
       "skills_group_6              0.037519                0.935709   \n",
       "skills_group_7             -0.587379               -0.054281   \n",
       "skills_group_8              0.080769                1.019064   \n",
       "skills_group_9             -0.908464               -1.000000   \n",
       "\n",
       "                 Developer, mobile  Developer, full-stack  \\\n",
       "skills_group_0           -1.000000              -1.000000   \n",
       "skills_group_1           -0.394241               2.666041   \n",
       "skills_group_10           0.120411              -0.252599   \n",
       "skills_group_11          -1.000000               1.909005   \n",
       "skills_group_12           5.868242              -0.419962   \n",
       "skills_group_13          -0.683204               0.539000   \n",
       "skills_group_14          -0.512044               0.245822   \n",
       "skills_group_15          -0.287712               0.044069   \n",
       "skills_group_16           0.558838               0.699218   \n",
       "skills_group_17           0.648874              -0.172539   \n",
       "skills_group_18           0.191833               0.089409   \n",
       "skills_group_19           0.373093              -0.000459   \n",
       "skills_group_2           -0.694461               0.229385   \n",
       "skills_group_20          -0.092306               0.464943   \n",
       "skills_group_21          -0.499533              -0.254126   \n",
       "skills_group_22          -0.037797               0.240425   \n",
       "skills_group_23           0.376670              -0.674405   \n",
       "skills_group_24          -0.389022              -0.369232   \n",
       "skills_group_25           1.674789               0.701197   \n",
       "skills_group_26          -0.399182              -0.006540   \n",
       "skills_group_27           0.032214              -0.102019   \n",
       "skills_group_28          -0.151209               0.677121   \n",
       "skills_group_29           1.228925              -0.019749   \n",
       "skills_group_3           -0.232994               0.125498   \n",
       "skills_group_30          -0.343307              -0.026302   \n",
       "skills_group_31           5.223343              -0.550264   \n",
       "skills_group_32           0.212998               0.914281   \n",
       "skills_group_33          -0.925937              -0.272044   \n",
       "skills_group_34           0.086064               0.892985   \n",
       "skills_group_35          -0.199570               0.446511   \n",
       "skills_group_36          -0.128836              -0.002469   \n",
       "skills_group_37           0.411631               1.338752   \n",
       "skills_group_4            0.394454               1.551677   \n",
       "skills_group_5           -1.000000               0.638654   \n",
       "skills_group_6            0.098013               0.364529   \n",
       "skills_group_7           10.718289              -0.224657   \n",
       "skills_group_8           -0.565548              -0.132682   \n",
       "skills_group_9           -1.000000              -1.000000   \n",
       "\n",
       "                 Cloud infrastructure engineer  \\\n",
       "skills_group_0                             inf   \n",
       "skills_group_1                             NaN   \n",
       "skills_group_10                            inf   \n",
       "skills_group_11                            inf   \n",
       "skills_group_12                            inf   \n",
       "skills_group_13                            NaN   \n",
       "skills_group_14                            inf   \n",
       "skills_group_15                            inf   \n",
       "skills_group_16                            inf   \n",
       "skills_group_17                            inf   \n",
       "skills_group_18                            NaN   \n",
       "skills_group_19                            inf   \n",
       "skills_group_2                             inf   \n",
       "skills_group_20                            inf   \n",
       "skills_group_21                            NaN   \n",
       "skills_group_22                            inf   \n",
       "skills_group_23                            inf   \n",
       "skills_group_24                            inf   \n",
       "skills_group_25                            inf   \n",
       "skills_group_26                            inf   \n",
       "skills_group_27                            NaN   \n",
       "skills_group_28                            inf   \n",
       "skills_group_29                            inf   \n",
       "skills_group_3                             inf   \n",
       "skills_group_30                            NaN   \n",
       "skills_group_31                            inf   \n",
       "skills_group_32                            NaN   \n",
       "skills_group_33                            inf   \n",
       "skills_group_34                            inf   \n",
       "skills_group_35                            inf   \n",
       "skills_group_36                            inf   \n",
       "skills_group_37                            inf   \n",
       "skills_group_4                             inf   \n",
       "skills_group_5                             inf   \n",
       "skills_group_6                             inf   \n",
       "skills_group_7                             inf   \n",
       "skills_group_8                             inf   \n",
       "skills_group_9                             NaN   \n",
       "\n",
       "                 Developer, embedded applications or devices  \\\n",
       "skills_group_0                                     -0.509357   \n",
       "skills_group_1                                     -0.651572   \n",
       "skills_group_10                                    -0.053671   \n",
       "skills_group_11                                    -0.699259   \n",
       "skills_group_12                                     0.390197   \n",
       "skills_group_13                                     3.441872   \n",
       "skills_group_14                                    -0.691525   \n",
       "skills_group_15                                     0.411474   \n",
       "skills_group_16                                     0.451416   \n",
       "skills_group_17                                     0.566963   \n",
       "skills_group_18                                     0.200676   \n",
       "skills_group_19                                    -0.029553   \n",
       "skills_group_2                                     -0.464502   \n",
       "skills_group_20                                    -0.319241   \n",
       "skills_group_21                                    -0.817656   \n",
       "skills_group_22                                    -0.171774   \n",
       "skills_group_23                                    -0.537161   \n",
       "skills_group_24                                     0.722106   \n",
       "skills_group_25                                     0.599708   \n",
       "skills_group_26                                     0.139870   \n",
       "skills_group_27                                     1.128035   \n",
       "skills_group_28                                    -0.431895   \n",
       "skills_group_29                                     0.752378   \n",
       "skills_group_3                                      0.691294   \n",
       "skills_group_30                                     0.859648   \n",
       "skills_group_31                                     0.312544   \n",
       "skills_group_32                                    -0.129184   \n",
       "skills_group_33                                     1.161988   \n",
       "skills_group_34                                    -0.319241   \n",
       "skills_group_35                                    -0.839442   \n",
       "skills_group_36                                     0.000000   \n",
       "skills_group_37                                     0.383428   \n",
       "skills_group_4                                     -0.657881   \n",
       "skills_group_5                                     -0.601028   \n",
       "skills_group_6                                     -0.028501   \n",
       "skills_group_7                                      0.851573   \n",
       "skills_group_8                                      0.318403   \n",
       "skills_group_9                                     -0.828226   \n",
       "\n",
       "                 Developer, QA or test  System administrator  Scientist  \\\n",
       "skills_group_0               -1.000000             -0.354765   0.122838   \n",
       "skills_group_1                1.004194              1.184363  -0.598746   \n",
       "skills_group_10              -0.208691             -0.015893  -0.416301   \n",
       "skills_group_11               1.112895             -0.164621  -0.722157   \n",
       "skills_group_12               0.243065              0.022089  -0.279943   \n",
       "skills_group_13               2.246270             -0.263951   0.038624   \n",
       "skills_group_14               0.282931              0.076672  -0.493489   \n",
       "skills_group_15               0.175340              0.179318   0.196965   \n",
       "skills_group_16               0.023490              0.546017  -0.606420   \n",
       "skills_group_17              -0.146769              0.046263  -0.182764   \n",
       "skills_group_18               0.515227             -0.106661  -0.207635   \n",
       "skills_group_19               0.186353             -0.095006  -0.048874   \n",
       "skills_group_2                1.116843             -0.247126  -0.727216   \n",
       "skills_group_20              -0.117532              0.287037  -0.105714   \n",
       "skills_group_21              -0.325161              0.607456  -0.180174   \n",
       "skills_group_22               0.191458             -0.016357  -0.203274   \n",
       "skills_group_23               0.104606              0.462889  -0.551150   \n",
       "skills_group_24               0.712324             -0.314348   0.341376   \n",
       "skills_group_25              -0.144024             -0.043421  -0.225141   \n",
       "skills_group_26               0.217030              0.286916  -0.065419   \n",
       "skills_group_27              -0.415957             -0.097161  -0.007781   \n",
       "skills_group_28               0.917781             -0.408169  -0.447792   \n",
       "skills_group_29               1.249939              0.166354  -0.439618   \n",
       "skills_group_3               -0.304506              1.268285  -0.361288   \n",
       "skills_group_30               0.125216             -0.181519   0.179602   \n",
       "skills_group_31              -0.133399              0.122235  -0.255512   \n",
       "skills_group_32               0.403749              0.104650  -0.147950   \n",
       "skills_group_33               0.257020              2.268088  -0.504756   \n",
       "skills_group_34               0.685547             -0.202434  -0.271526   \n",
       "skills_group_35               0.244268             -0.087710  -0.683689   \n",
       "skills_group_36               0.000000              0.095679  -0.009409   \n",
       "skills_group_37               0.388022             -0.334447  -0.294646   \n",
       "skills_group_4                0.119221             -0.051445  -0.538518   \n",
       "skills_group_5                0.030055              0.000772  -0.812488   \n",
       "skills_group_6                0.172970              0.459311  -0.115400   \n",
       "skills_group_7                0.025937             -0.613658  -0.644758   \n",
       "skills_group_8                1.301329              0.530425   0.184806   \n",
       "skills_group_9               -1.000000             -0.905035   1.340360   \n",
       "\n",
       "                 Security professional  Developer, game or graphics  \\\n",
       "skills_group_0               -1.000000                    -1.000000   \n",
       "skills_group_1                0.203870                    -0.150731   \n",
       "skills_group_10              -0.762485                     0.241270   \n",
       "skills_group_11              -0.431295                     0.146700   \n",
       "skills_group_12              -0.239991                     0.808071   \n",
       "skills_group_13               0.696122                     2.818621   \n",
       "skills_group_14               2.488181                    -0.160835   \n",
       "skills_group_15              -0.478516                    -0.013239   \n",
       "skills_group_16              -0.213486                     2.485132   \n",
       "skills_group_17               2.270933                     4.847263   \n",
       "skills_group_18               0.251036                     0.000000   \n",
       "skills_group_19               0.000000                    -0.175322   \n",
       "skills_group_2               -0.977758                    -0.712825   \n",
       "skills_group_20              -0.018329                     0.856361   \n",
       "skills_group_21              -1.000000                     0.163230   \n",
       "skills_group_22               0.827789                     0.000000   \n",
       "skills_group_23               1.080518                    -0.452958   \n",
       "skills_group_24              -0.762485                    -0.378317   \n",
       "skills_group_25              -0.211933                     8.573560   \n",
       "skills_group_26               1.193301                    -0.141164   \n",
       "skills_group_27               0.000000                     0.159187   \n",
       "skills_group_28               0.196613                    -0.494265   \n",
       "skills_group_29              -0.977758                     0.997920   \n",
       "skills_group_3                1.533028                     1.482786   \n",
       "skills_group_30               0.271630                     3.217218   \n",
       "skills_group_31              -0.487564                     1.921109   \n",
       "skills_group_32               0.000000                    -0.699586   \n",
       "skills_group_33               2.017154                    -0.811439   \n",
       "skills_group_34               0.532885                     0.009123   \n",
       "skills_group_35              -0.725405                    -0.163380   \n",
       "skills_group_36               0.000000                     0.000000   \n",
       "skills_group_37               0.621430                     0.890562   \n",
       "skills_group_4                0.814089                     1.998949   \n",
       "skills_group_5               -1.000000                    -0.665492   \n",
       "skills_group_6                0.019765                     0.321269   \n",
       "skills_group_7                0.370625                     0.168554   \n",
       "skills_group_8                0.278909                     0.631432   \n",
       "skills_group_9               -1.000000                    -0.665492   \n",
       "\n",
       "                 Developer, front-end  Blockchain  \\\n",
       "skills_group_0              -0.261794   -0.162034   \n",
       "skills_group_1               6.778736   -1.000000   \n",
       "skills_group_10              1.036621    0.000000   \n",
       "skills_group_11             53.417014    2.744693   \n",
       "skills_group_12              1.065725    2.025803   \n",
       "skills_group_13             -0.824704   -1.000000   \n",
       "skills_group_14             -1.000000    1.007848   \n",
       "skills_group_15              0.000000    6.498812   \n",
       "skills_group_16             10.237299    0.965202   \n",
       "skills_group_17              3.090484    3.548800   \n",
       "skills_group_18              2.098367    1.015848   \n",
       "skills_group_19              1.541766    1.015350   \n",
       "skills_group_2               0.000000    0.000000   \n",
       "skills_group_20              2.644405    2.016303   \n",
       "skills_group_21              0.609554   -1.000000   \n",
       "skills_group_22              2.068463    2.030699   \n",
       "skills_group_23             -1.000000   -1.000000   \n",
       "skills_group_24             -0.076022    1.788803   \n",
       "skills_group_25              1.036621    2.797836   \n",
       "skills_group_26              0.000000    0.917128   \n",
       "skills_group_27              0.000000    1.010563   \n",
       "skills_group_28              4.311697   14.340527   \n",
       "skills_group_29              2.314699    0.015350   \n",
       "skills_group_3               0.632368    0.999407   \n",
       "skills_group_30              0.000000    5.057930   \n",
       "skills_group_31              7.817641    1.027635   \n",
       "skills_group_32              2.094485    1.008418   \n",
       "skills_group_33              0.546381    0.015350   \n",
       "skills_group_34              5.228890    1.015350   \n",
       "skills_group_35              0.390091    3.180525   \n",
       "skills_group_36              0.000000    0.000000   \n",
       "skills_group_37             -0.526119    0.849729   \n",
       "skills_group_4               1.025694   -1.000000   \n",
       "skills_group_5               3.708018    0.376249   \n",
       "skills_group_6               0.603429   -0.005664   \n",
       "skills_group_7               0.800246    0.050040   \n",
       "skills_group_8               0.665235   -0.415453   \n",
       "skills_group_9              -1.000000   -1.000000   \n",
       "\n",
       "                 Developer, desktop or enterprise applications  \\\n",
       "skills_group_0                                       -0.650385   \n",
       "skills_group_1                                        0.671469   \n",
       "skills_group_10                                       0.358787   \n",
       "skills_group_11                                      -0.744021   \n",
       "skills_group_12                                      -0.013828   \n",
       "skills_group_13                                       1.242668   \n",
       "skills_group_14                                      -0.502433   \n",
       "skills_group_15                                       0.237591   \n",
       "skills_group_16                                       0.156124   \n",
       "skills_group_17                                       0.442473   \n",
       "skills_group_18                                       0.154301   \n",
       "skills_group_19                                      -0.063914   \n",
       "skills_group_2                                       -0.434614   \n",
       "skills_group_20                                      -0.044407   \n",
       "skills_group_21                                       4.312719   \n",
       "skills_group_22                                      -0.088808   \n",
       "skills_group_23                                      -0.464840   \n",
       "skills_group_24                                      -0.264230   \n",
       "skills_group_25                                      -0.417659   \n",
       "skills_group_26                                       0.138442   \n",
       "skills_group_27                                       0.081452   \n",
       "skills_group_28                                      -0.440161   \n",
       "skills_group_29                                       1.164573   \n",
       "skills_group_3                                       -0.059597   \n",
       "skills_group_30                                       0.509230   \n",
       "skills_group_31                                       0.788652   \n",
       "skills_group_32                                      -0.225279   \n",
       "skills_group_33                                      -0.035742   \n",
       "skills_group_34                                       0.015972   \n",
       "skills_group_35                                      -0.230537   \n",
       "skills_group_36                                      -0.001800   \n",
       "skills_group_37                                       0.086726   \n",
       "skills_group_4                                        2.940103   \n",
       "skills_group_5                                       -0.234132   \n",
       "skills_group_6                                       -0.163125   \n",
       "skills_group_7                                        0.161697   \n",
       "skills_group_8                                        0.587272   \n",
       "skills_group_9                                       -0.912017   \n",
       "\n",
       "                 DevOps specialist  Academic researcher  \n",
       "skills_group_0                 NaN             1.329480  \n",
       "skills_group_1                 inf            -0.299176  \n",
       "skills_group_10                inf            -0.160908  \n",
       "skills_group_11                inf            -0.469353  \n",
       "skills_group_12                inf            -0.136599  \n",
       "skills_group_13                inf             0.195871  \n",
       "skills_group_14                inf            -0.612703  \n",
       "skills_group_15                inf             0.554479  \n",
       "skills_group_16                NaN            -0.494700  \n",
       "skills_group_17                inf             0.214999  \n",
       "skills_group_18                inf            -0.060068  \n",
       "skills_group_19                NaN             0.056324  \n",
       "skills_group_2                 inf            -0.674749  \n",
       "skills_group_20                inf             0.000000  \n",
       "skills_group_21                NaN            -0.232863  \n",
       "skills_group_22                inf             0.135796  \n",
       "skills_group_23                inf             0.045546  \n",
       "skills_group_24                inf             0.190838  \n",
       "skills_group_25                inf            -0.251227  \n",
       "skills_group_26                inf             0.092851  \n",
       "skills_group_27                NaN             0.137390  \n",
       "skills_group_28                inf             0.094329  \n",
       "skills_group_29                inf            -0.123334  \n",
       "skills_group_3                 inf             0.185568  \n",
       "skills_group_30                inf             0.595838  \n",
       "skills_group_31                NaN            -0.342635  \n",
       "skills_group_32                inf             0.104082  \n",
       "skills_group_33                inf             0.651990  \n",
       "skills_group_34                inf            -0.065173  \n",
       "skills_group_35                inf            -0.387507  \n",
       "skills_group_36                inf            -0.007116  \n",
       "skills_group_37                inf             0.012750  \n",
       "skills_group_4                 inf            -0.359071  \n",
       "skills_group_5                 inf            -0.871520  \n",
       "skills_group_6                 NaN            -0.054475  \n",
       "skills_group_7                 inf            -0.001009  \n",
       "skills_group_8                 inf             0.453217  \n",
       "skills_group_9                 NaN             3.782368  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_results = pd.DataFrame(simulated_results)\n",
    "simulated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5c23e41b-67d7-4edf-b6e1-ecb5e11178f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_job = 'Data scientist or machine learning specialist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "23c1d843-03d2-41d1-8975-37b578a044c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skills_group_0     1.517494\n",
       "skills_group_9     0.769869\n",
       "skills_group_2     0.534456\n",
       "skills_group_24    0.342185\n",
       "skills_group_27   -0.011894\n",
       "skills_group_36   -0.029647\n",
       "skills_group_19   -0.051636\n",
       "skills_group_30   -0.079168\n",
       "skills_group_6    -0.088489\n",
       "skills_group_18   -0.100783\n",
       "skills_group_20   -0.135479\n",
       "skills_group_22   -0.186196\n",
       "skills_group_34   -0.209285\n",
       "skills_group_37   -0.232492\n",
       "skills_group_23   -0.241807\n",
       "skills_group_32   -0.250467\n",
       "skills_group_8    -0.251357\n",
       "skills_group_26   -0.255513\n",
       "skills_group_28   -0.276586\n",
       "skills_group_15   -0.306133\n",
       "skills_group_10   -0.312854\n",
       "skills_group_35   -0.339027\n",
       "skills_group_17   -0.356376\n",
       "skills_group_33   -0.402352\n",
       "skills_group_16   -0.411330\n",
       "skills_group_25   -0.435840\n",
       "skills_group_31   -0.437352\n",
       "skills_group_29   -0.445276\n",
       "skills_group_12   -0.469629\n",
       "skills_group_3    -0.507166\n",
       "skills_group_7    -0.531493\n",
       "skills_group_14   -0.587044\n",
       "skills_group_11   -0.614438\n",
       "skills_group_1    -0.640197\n",
       "skills_group_4    -0.649516\n",
       "skills_group_21   -0.650650\n",
       "skills_group_13   -0.650933\n",
       "skills_group_5    -0.746542\n",
       "Name: Data scientist or machine learning specialist, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_results = simulated_results[target_job].sort_values(ascending=False)\n",
    "target_results.head(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3fd7591f-caf3-4358-882f-bfee7c132e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.0\n",
    "recommendations = target_results[target_results > threshold].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "687b31e7-8340-4ae7-a082-1cd7e515656c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulated_results = []\n",
    "for cluster in recommendations:\n",
    "    for skill in clusters_config[cluster]:\n",
    "        additional_skill_prob = model.predict_job_probabilities(set([skill] + entry_skills))\n",
    "        additional_skill_uplift = (additional_skill_prob - base_predictions) / base_predictions\n",
    "        additional_skill_uplift.name = skill\n",
    "        simulated_results.append(additional_skill_uplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aaec187c-c228-4ae8-b42b-307ff7405d6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data scientist or machine learning specialist</th>\n",
       "      <th>Engineer, data</th>\n",
       "      <th>Data or business analyst</th>\n",
       "      <th>Developer, back-end</th>\n",
       "      <th>Database administrator</th>\n",
       "      <th>Developer, mobile</th>\n",
       "      <th>Developer, full-stack</th>\n",
       "      <th>Cloud infrastructure engineer</th>\n",
       "      <th>Developer, embedded applications or devices</th>\n",
       "      <th>Developer, QA or test</th>\n",
       "      <th>System administrator</th>\n",
       "      <th>Scientist</th>\n",
       "      <th>Security professional</th>\n",
       "      <th>Developer, game or graphics</th>\n",
       "      <th>Developer, front-end</th>\n",
       "      <th>Blockchain</th>\n",
       "      <th>Developer, desktop or enterprise applications</th>\n",
       "      <th>DevOps specialist</th>\n",
       "      <th>Academic researcher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hugging Face Transformers</th>\n",
       "      <td>0.199043</td>\n",
       "      <td>-0.080110</td>\n",
       "      <td>2.535418e-01</td>\n",
       "      <td>-0.183535</td>\n",
       "      <td>-0.292712</td>\n",
       "      <td>-0.213310</td>\n",
       "      <td>-0.243768</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.413626</td>\n",
       "      <td>0.025322</td>\n",
       "      <td>-0.183597</td>\n",
       "      <td>0.071933</td>\n",
       "      <td>0.014202</td>\n",
       "      <td>0.282830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.120549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.114486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keras</th>\n",
       "      <td>0.127764</td>\n",
       "      <td>-0.377128</td>\n",
       "      <td>1.022150e-01</td>\n",
       "      <td>-0.183916</td>\n",
       "      <td>-0.293407</td>\n",
       "      <td>-0.342473</td>\n",
       "      <td>-0.243768</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.967567</td>\n",
       "      <td>-0.385080</td>\n",
       "      <td>-0.421383</td>\n",
       "      <td>0.419960</td>\n",
       "      <td>-0.223313</td>\n",
       "      <td>-0.194101</td>\n",
       "      <td>0.885345</td>\n",
       "      <td>1.918531</td>\n",
       "      <td>-0.004639</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.816835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumPy</th>\n",
       "      <td>-0.027646</td>\n",
       "      <td>-0.085753</td>\n",
       "      <td>9.115677e-02</td>\n",
       "      <td>-0.427534</td>\n",
       "      <td>-0.397502</td>\n",
       "      <td>-0.565548</td>\n",
       "      <td>-0.700572</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.451397</td>\n",
       "      <td>-0.252420</td>\n",
       "      <td>-0.363741</td>\n",
       "      <td>0.475863</td>\n",
       "      <td>-0.701829</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>-0.174406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.318041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pandas</th>\n",
       "      <td>0.107544</td>\n",
       "      <td>0.151311</td>\n",
       "      <td>5.901292e-01</td>\n",
       "      <td>-0.208889</td>\n",
       "      <td>-0.121299</td>\n",
       "      <td>-0.547562</td>\n",
       "      <td>-0.569347</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.472731</td>\n",
       "      <td>0.826792</td>\n",
       "      <td>-0.463757</td>\n",
       "      <td>-0.186075</td>\n",
       "      <td>-0.245225</td>\n",
       "      <td>-0.376637</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.080226</td>\n",
       "      <td>-0.517334</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.433036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scikit-learn</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.210319e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorFlow</th>\n",
       "      <td>0.213591</td>\n",
       "      <td>-0.051954</td>\n",
       "      <td>1.363916e-01</td>\n",
       "      <td>-0.259494</td>\n",
       "      <td>0.057676</td>\n",
       "      <td>-0.713692</td>\n",
       "      <td>-0.140220</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.315776</td>\n",
       "      <td>-0.036505</td>\n",
       "      <td>-0.293726</td>\n",
       "      <td>0.229155</td>\n",
       "      <td>-0.283969</td>\n",
       "      <td>0.603529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>-0.170058</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.481468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Torch/PyTorch</th>\n",
       "      <td>0.241399</td>\n",
       "      <td>-0.245293</td>\n",
       "      <td>1.399135e-01</td>\n",
       "      <td>-0.396871</td>\n",
       "      <td>-0.397904</td>\n",
       "      <td>-0.470516</td>\n",
       "      <td>-0.359903</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.232357</td>\n",
       "      <td>-0.397007</td>\n",
       "      <td>-0.469962</td>\n",
       "      <td>0.431249</td>\n",
       "      <td>0.447929</td>\n",
       "      <td>-0.051679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.385988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPython/Jupyter</th>\n",
       "      <td>0.477927</td>\n",
       "      <td>-0.157996</td>\n",
       "      <td>3.543821e-01</td>\n",
       "      <td>-0.006150</td>\n",
       "      <td>0.434635</td>\n",
       "      <td>-0.465666</td>\n",
       "      <td>-0.382604</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.363004</td>\n",
       "      <td>0.090531</td>\n",
       "      <td>-0.315395</td>\n",
       "      <td>0.623515</td>\n",
       "      <td>-0.223313</td>\n",
       "      <td>-0.004345</td>\n",
       "      <td>1.030280</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.515815</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.217036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spyder</th>\n",
       "      <td>0.124296</td>\n",
       "      <td>-0.304549</td>\n",
       "      <td>3.481499e-01</td>\n",
       "      <td>-0.100891</td>\n",
       "      <td>-0.158026</td>\n",
       "      <td>-0.518926</td>\n",
       "      <td>-0.379413</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.445749</td>\n",
       "      <td>-0.162843</td>\n",
       "      <td>-0.192009</td>\n",
       "      <td>0.187695</td>\n",
       "      <td>0.014202</td>\n",
       "      <td>-0.241435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.208532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.471292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Julia</th>\n",
       "      <td>-0.072374</td>\n",
       "      <td>-0.506413</td>\n",
       "      <td>-2.410750e-02</td>\n",
       "      <td>-0.595690</td>\n",
       "      <td>-0.435897</td>\n",
       "      <td>-0.694384</td>\n",
       "      <td>-0.619285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.123257</td>\n",
       "      <td>-0.620548</td>\n",
       "      <td>-0.468594</td>\n",
       "      <td>0.876859</td>\n",
       "      <td>-0.524970</td>\n",
       "      <td>0.813359</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.367086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.633780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.250101</td>\n",
       "      <td>-0.413155</td>\n",
       "      <td>6.283844e-01</td>\n",
       "      <td>-0.672524</td>\n",
       "      <td>0.319037</td>\n",
       "      <td>-0.990390</td>\n",
       "      <td>-0.671902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.711601</td>\n",
       "      <td>-0.554583</td>\n",
       "      <td>-0.616979</td>\n",
       "      <td>0.413088</td>\n",
       "      <td>-0.534678</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.805454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.348061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tidyverse</th>\n",
       "      <td>0.210361</td>\n",
       "      <td>-0.295639</td>\n",
       "      <td>1.668451e-01</td>\n",
       "      <td>-0.497254</td>\n",
       "      <td>0.094361</td>\n",
       "      <td>-0.249145</td>\n",
       "      <td>-0.539966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.368053</td>\n",
       "      <td>-0.267938</td>\n",
       "      <td>-0.333986</td>\n",
       "      <td>0.763750</td>\n",
       "      <td>-0.512436</td>\n",
       "      <td>-0.141227</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.156629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.581765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RStudio</th>\n",
       "      <td>0.054669</td>\n",
       "      <td>-0.372853</td>\n",
       "      <td>3.062885e-01</td>\n",
       "      <td>-0.660353</td>\n",
       "      <td>-0.338175</td>\n",
       "      <td>-0.370132</td>\n",
       "      <td>-0.715925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.138155</td>\n",
       "      <td>-0.577671</td>\n",
       "      <td>-0.397118</td>\n",
       "      <td>0.474729</td>\n",
       "      <td>-0.740244</td>\n",
       "      <td>0.145947</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.204975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.609563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scala</th>\n",
       "      <td>-0.055937</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>5.668960e-01</td>\n",
       "      <td>1.400155</td>\n",
       "      <td>0.070651</td>\n",
       "      <td>-0.508515</td>\n",
       "      <td>1.632150</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.171774</td>\n",
       "      <td>0.146595</td>\n",
       "      <td>-0.173995</td>\n",
       "      <td>-0.289776</td>\n",
       "      <td>-0.977758</td>\n",
       "      <td>-0.537503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090098</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.341943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cassandra</th>\n",
       "      <td>0.164549</td>\n",
       "      <td>0.225532</td>\n",
       "      <td>2.518329e-01</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>-0.076650</td>\n",
       "      <td>-0.508515</td>\n",
       "      <td>0.770350</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.171774</td>\n",
       "      <td>0.338405</td>\n",
       "      <td>-0.187101</td>\n",
       "      <td>-0.259574</td>\n",
       "      <td>-0.478516</td>\n",
       "      <td>-0.537503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.215572</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.392658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neo4j</th>\n",
       "      <td>0.096752</td>\n",
       "      <td>-0.014710</td>\n",
       "      <td>3.169432e-01</td>\n",
       "      <td>0.951647</td>\n",
       "      <td>-0.077060</td>\n",
       "      <td>-0.508515</td>\n",
       "      <td>0.890769</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.171774</td>\n",
       "      <td>0.211561</td>\n",
       "      <td>-0.053863</td>\n",
       "      <td>-0.217739</td>\n",
       "      <td>-0.241001</td>\n",
       "      <td>-0.537503</td>\n",
       "      <td>0.289822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.195250</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.329871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Play Framework</th>\n",
       "      <td>0.197466</td>\n",
       "      <td>0.105550</td>\n",
       "      <td>2.623199e-01</td>\n",
       "      <td>0.868399</td>\n",
       "      <td>-0.077060</td>\n",
       "      <td>-0.689611</td>\n",
       "      <td>1.136731</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.171774</td>\n",
       "      <td>0.265041</td>\n",
       "      <td>-0.193369</td>\n",
       "      <td>-0.217739</td>\n",
       "      <td>-0.241001</td>\n",
       "      <td>-0.537503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.195537</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.261031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apache Kafka</th>\n",
       "      <td>0.176673</td>\n",
       "      <td>0.193489</td>\n",
       "      <td>3.843892e-01</td>\n",
       "      <td>0.943446</td>\n",
       "      <td>0.350914</td>\n",
       "      <td>-0.513366</td>\n",
       "      <td>0.771194</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.171774</td>\n",
       "      <td>0.647614</td>\n",
       "      <td>-0.328282</td>\n",
       "      <td>-0.446272</td>\n",
       "      <td>-0.740244</td>\n",
       "      <td>-0.537503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.265089</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.291401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apache Spark</th>\n",
       "      <td>0.304274</td>\n",
       "      <td>0.424209</td>\n",
       "      <td>5.267681e-01</td>\n",
       "      <td>0.411030</td>\n",
       "      <td>-0.203099</td>\n",
       "      <td>-0.508515</td>\n",
       "      <td>0.477426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.263176</td>\n",
       "      <td>0.073754</td>\n",
       "      <td>-0.367189</td>\n",
       "      <td>-0.227849</td>\n",
       "      <td>-0.241001</td>\n",
       "      <td>-0.389040</td>\n",
       "      <td>0.460078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.360783</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.365628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hadoop</th>\n",
       "      <td>0.419609</td>\n",
       "      <td>0.103101</td>\n",
       "      <td>3.428245e-01</td>\n",
       "      <td>0.900062</td>\n",
       "      <td>-0.077060</td>\n",
       "      <td>-0.513366</td>\n",
       "      <td>0.764721</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.373100</td>\n",
       "      <td>0.333662</td>\n",
       "      <td>-0.188394</td>\n",
       "      <td>-0.261045</td>\n",
       "      <td>-0.716031</td>\n",
       "      <td>-0.712825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.354433</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Python</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.210319e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Django</th>\n",
       "      <td>0.190382</td>\n",
       "      <td>-0.107453</td>\n",
       "      <td>-1.527638e-01</td>\n",
       "      <td>-0.259243</td>\n",
       "      <td>0.152924</td>\n",
       "      <td>0.223720</td>\n",
       "      <td>-0.163152</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.105272</td>\n",
       "      <td>0.440245</td>\n",
       "      <td>-0.167933</td>\n",
       "      <td>0.298925</td>\n",
       "      <td>-0.502729</td>\n",
       "      <td>-0.188561</td>\n",
       "      <td>0.016096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.256008</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.110815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FastAPI</th>\n",
       "      <td>0.149674</td>\n",
       "      <td>-0.198911</td>\n",
       "      <td>-1.166281e-01</td>\n",
       "      <td>0.125831</td>\n",
       "      <td>0.292317</td>\n",
       "      <td>-0.128836</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.228895</td>\n",
       "      <td>0.245679</td>\n",
       "      <td>-0.278577</td>\n",
       "      <td>0.240620</td>\n",
       "      <td>-0.261728</td>\n",
       "      <td>0.307262</td>\n",
       "      <td>-0.732530</td>\n",
       "      <td>1.798180</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.053258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flask</th>\n",
       "      <td>0.128673</td>\n",
       "      <td>-0.409568</td>\n",
       "      <td>-2.949437e-02</td>\n",
       "      <td>0.061351</td>\n",
       "      <td>0.314663</td>\n",
       "      <td>-0.343734</td>\n",
       "      <td>0.120881</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.339570</td>\n",
       "      <td>0.927781</td>\n",
       "      <td>0.073965</td>\n",
       "      <td>0.103758</td>\n",
       "      <td>-0.499242</td>\n",
       "      <td>0.051695</td>\n",
       "      <td>0.191448</td>\n",
       "      <td>-0.141563</td>\n",
       "      <td>-0.091497</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.003583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PyCharm</th>\n",
       "      <td>0.825358</td>\n",
       "      <td>-0.249079</td>\n",
       "      <td>-1.210407e-02</td>\n",
       "      <td>-0.406017</td>\n",
       "      <td>0.088169</td>\n",
       "      <td>-0.568530</td>\n",
       "      <td>-0.391218</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.269877</td>\n",
       "      <td>0.065141</td>\n",
       "      <td>-0.254921</td>\n",
       "      <td>0.666212</td>\n",
       "      <td>-0.524970</td>\n",
       "      <td>0.131513</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.166187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.229771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Data scientist or machine learning specialist  \\\n",
       "Hugging Face Transformers                                       0.199043   \n",
       "Keras                                                           0.127764   \n",
       "NumPy                                                          -0.027646   \n",
       "Pandas                                                          0.107544   \n",
       "Scikit-learn                                                    0.000000   \n",
       "TensorFlow                                                      0.213591   \n",
       "Torch/PyTorch                                                   0.241399   \n",
       "IPython/Jupyter                                                 0.477927   \n",
       "Spyder                                                          0.124296   \n",
       "Julia                                                          -0.072374   \n",
       "R                                                               0.250101   \n",
       "Tidyverse                                                       0.210361   \n",
       "RStudio                                                         0.054669   \n",
       "Scala                                                          -0.055937   \n",
       "Cassandra                                                       0.164549   \n",
       "Neo4j                                                           0.096752   \n",
       "Play Framework                                                  0.197466   \n",
       "Apache Kafka                                                    0.176673   \n",
       "Apache Spark                                                    0.304274   \n",
       "Hadoop                                                          0.419609   \n",
       "Python                                                          0.000000   \n",
       "Django                                                          0.190382   \n",
       "FastAPI                                                         0.149674   \n",
       "Flask                                                           0.128673   \n",
       "PyCharm                                                         0.825358   \n",
       "\n",
       "                           Engineer, data  Data or business analyst  \\\n",
       "Hugging Face Transformers       -0.080110              2.535418e-01   \n",
       "Keras                           -0.377128              1.022150e-01   \n",
       "NumPy                           -0.085753              9.115677e-02   \n",
       "Pandas                           0.151311              5.901292e-01   \n",
       "Scikit-learn                     0.000000              1.210319e-16   \n",
       "TensorFlow                      -0.051954              1.363916e-01   \n",
       "Torch/PyTorch                   -0.245293              1.399135e-01   \n",
       "IPython/Jupyter                 -0.157996              3.543821e-01   \n",
       "Spyder                          -0.304549              3.481499e-01   \n",
       "Julia                           -0.506413             -2.410750e-02   \n",
       "R                               -0.413155              6.283844e-01   \n",
       "Tidyverse                       -0.295639              1.668451e-01   \n",
       "RStudio                         -0.372853              3.062885e-01   \n",
       "Scala                            0.007371              5.668960e-01   \n",
       "Cassandra                        0.225532              2.518329e-01   \n",
       "Neo4j                           -0.014710              3.169432e-01   \n",
       "Play Framework                   0.105550              2.623199e-01   \n",
       "Apache Kafka                     0.193489              3.843892e-01   \n",
       "Apache Spark                     0.424209              5.267681e-01   \n",
       "Hadoop                           0.103101              3.428245e-01   \n",
       "Python                           0.000000              1.210319e-16   \n",
       "Django                          -0.107453             -1.527638e-01   \n",
       "FastAPI                         -0.198911             -1.166281e-01   \n",
       "Flask                           -0.409568             -2.949437e-02   \n",
       "PyCharm                         -0.249079             -1.210407e-02   \n",
       "\n",
       "                           Developer, back-end  Database administrator  \\\n",
       "Hugging Face Transformers            -0.183535               -0.292712   \n",
       "Keras                                -0.183916               -0.293407   \n",
       "NumPy                                -0.427534               -0.397502   \n",
       "Pandas                               -0.208889               -0.121299   \n",
       "Scikit-learn                          0.000000                0.000000   \n",
       "TensorFlow                           -0.259494                0.057676   \n",
       "Torch/PyTorch                        -0.396871               -0.397904   \n",
       "IPython/Jupyter                      -0.006150                0.434635   \n",
       "Spyder                               -0.100891               -0.158026   \n",
       "Julia                                -0.595690               -0.435897   \n",
       "R                                    -0.672524                0.319037   \n",
       "Tidyverse                            -0.497254                0.094361   \n",
       "RStudio                              -0.660353               -0.338175   \n",
       "Scala                                 1.400155                0.070651   \n",
       "Cassandra                             0.786885               -0.076650   \n",
       "Neo4j                                 0.951647               -0.077060   \n",
       "Play Framework                        0.868399               -0.077060   \n",
       "Apache Kafka                          0.943446                0.350914   \n",
       "Apache Spark                          0.411030               -0.203099   \n",
       "Hadoop                                0.900062               -0.077060   \n",
       "Python                                0.000000                0.000000   \n",
       "Django                               -0.259243                0.152924   \n",
       "FastAPI                               0.125831                0.292317   \n",
       "Flask                                 0.061351                0.314663   \n",
       "PyCharm                              -0.406017                0.088169   \n",
       "\n",
       "                           Developer, mobile  Developer, full-stack  \\\n",
       "Hugging Face Transformers          -0.213310              -0.243768   \n",
       "Keras                              -0.342473              -0.243768   \n",
       "NumPy                              -0.565548              -0.700572   \n",
       "Pandas                             -0.547562              -0.569347   \n",
       "Scikit-learn                        0.000000               0.000000   \n",
       "TensorFlow                         -0.713692              -0.140220   \n",
       "Torch/PyTorch                      -0.470516              -0.359903   \n",
       "IPython/Jupyter                    -0.465666              -0.382604   \n",
       "Spyder                             -0.518926              -0.379413   \n",
       "Julia                              -0.694384              -0.619285   \n",
       "R                                  -0.990390              -0.671902   \n",
       "Tidyverse                          -0.249145              -0.539966   \n",
       "RStudio                            -0.370132              -0.715925   \n",
       "Scala                              -0.508515               1.632150   \n",
       "Cassandra                          -0.508515               0.770350   \n",
       "Neo4j                              -0.508515               0.890769   \n",
       "Play Framework                     -0.689611               1.136731   \n",
       "Apache Kafka                       -0.513366               0.771194   \n",
       "Apache Spark                       -0.508515               0.477426   \n",
       "Hadoop                             -0.513366               0.764721   \n",
       "Python                              0.000000               0.000000   \n",
       "Django                              0.223720              -0.163152   \n",
       "FastAPI                            -0.128836               0.002810   \n",
       "Flask                              -0.343734               0.120881   \n",
       "PyCharm                            -0.568530              -0.391218   \n",
       "\n",
       "                           Cloud infrastructure engineer  \\\n",
       "Hugging Face Transformers                            inf   \n",
       "Keras                                                inf   \n",
       "NumPy                                                inf   \n",
       "Pandas                                               inf   \n",
       "Scikit-learn                                         NaN   \n",
       "TensorFlow                                           inf   \n",
       "Torch/PyTorch                                        inf   \n",
       "IPython/Jupyter                                      inf   \n",
       "Spyder                                               inf   \n",
       "Julia                                                NaN   \n",
       "R                                                    NaN   \n",
       "Tidyverse                                            NaN   \n",
       "RStudio                                              NaN   \n",
       "Scala                                                inf   \n",
       "Cassandra                                            inf   \n",
       "Neo4j                                                inf   \n",
       "Play Framework                                       inf   \n",
       "Apache Kafka                                         inf   \n",
       "Apache Spark                                         NaN   \n",
       "Hadoop                                               inf   \n",
       "Python                                               NaN   \n",
       "Django                                               inf   \n",
       "FastAPI                                              inf   \n",
       "Flask                                                inf   \n",
       "PyCharm                                              inf   \n",
       "\n",
       "                           Developer, embedded applications or devices  \\\n",
       "Hugging Face Transformers                                     0.413626   \n",
       "Keras                                                         0.967567   \n",
       "NumPy                                                         0.451397   \n",
       "Pandas                                                       -0.472731   \n",
       "Scikit-learn                                                  0.000000   \n",
       "TensorFlow                                                    0.315776   \n",
       "Torch/PyTorch                                                 0.232357   \n",
       "IPython/Jupyter                                              -0.363004   \n",
       "Spyder                                                        0.445749   \n",
       "Julia                                                         0.123257   \n",
       "R                                                            -0.711601   \n",
       "Tidyverse                                                    -0.368053   \n",
       "RStudio                                                      -0.138155   \n",
       "Scala                                                        -0.171774   \n",
       "Cassandra                                                    -0.171774   \n",
       "Neo4j                                                        -0.171774   \n",
       "Play Framework                                               -0.171774   \n",
       "Apache Kafka                                                 -0.171774   \n",
       "Apache Spark                                                 -0.263176   \n",
       "Hadoop                                                       -0.373100   \n",
       "Python                                                        0.000000   \n",
       "Django                                                       -0.105272   \n",
       "FastAPI                                                       0.228895   \n",
       "Flask                                                         0.339570   \n",
       "PyCharm                                                       0.269877   \n",
       "\n",
       "                           Developer, QA or test  System administrator  \\\n",
       "Hugging Face Transformers               0.025322             -0.183597   \n",
       "Keras                                  -0.385080             -0.421383   \n",
       "NumPy                                  -0.252420             -0.363741   \n",
       "Pandas                                  0.826792             -0.463757   \n",
       "Scikit-learn                            0.000000              0.000000   \n",
       "TensorFlow                             -0.036505             -0.293726   \n",
       "Torch/PyTorch                          -0.397007             -0.469962   \n",
       "IPython/Jupyter                         0.090531             -0.315395   \n",
       "Spyder                                 -0.162843             -0.192009   \n",
       "Julia                                  -0.620548             -0.468594   \n",
       "R                                      -0.554583             -0.616979   \n",
       "Tidyverse                              -0.267938             -0.333986   \n",
       "RStudio                                -0.577671             -0.397118   \n",
       "Scala                                   0.146595             -0.173995   \n",
       "Cassandra                               0.338405             -0.187101   \n",
       "Neo4j                                   0.211561             -0.053863   \n",
       "Play Framework                          0.265041             -0.193369   \n",
       "Apache Kafka                            0.647614             -0.328282   \n",
       "Apache Spark                            0.073754             -0.367189   \n",
       "Hadoop                                  0.333662             -0.188394   \n",
       "Python                                  0.000000              0.000000   \n",
       "Django                                  0.440245             -0.167933   \n",
       "FastAPI                                 0.245679             -0.278577   \n",
       "Flask                                   0.927781              0.073965   \n",
       "PyCharm                                 0.065141             -0.254921   \n",
       "\n",
       "                           Scientist  Security professional  \\\n",
       "Hugging Face Transformers   0.071933               0.014202   \n",
       "Keras                       0.419960              -0.223313   \n",
       "NumPy                       0.475863              -0.701829   \n",
       "Pandas                     -0.186075              -0.245225   \n",
       "Scikit-learn                0.000000               0.000000   \n",
       "TensorFlow                  0.229155              -0.283969   \n",
       "Torch/PyTorch               0.431249               0.447929   \n",
       "IPython/Jupyter             0.623515              -0.223313   \n",
       "Spyder                      0.187695               0.014202   \n",
       "Julia                       0.876859              -0.524970   \n",
       "R                           0.413088              -0.534678   \n",
       "Tidyverse                   0.763750              -0.512436   \n",
       "RStudio                     0.474729              -0.740244   \n",
       "Scala                      -0.289776              -0.977758   \n",
       "Cassandra                  -0.259574              -0.478516   \n",
       "Neo4j                      -0.217739              -0.241001   \n",
       "Play Framework             -0.217739              -0.241001   \n",
       "Apache Kafka               -0.446272              -0.740244   \n",
       "Apache Spark               -0.227849              -0.241001   \n",
       "Hadoop                     -0.261045              -0.716031   \n",
       "Python                      0.000000               0.000000   \n",
       "Django                      0.298925              -0.502729   \n",
       "FastAPI                     0.240620              -0.261728   \n",
       "Flask                       0.103758              -0.499242   \n",
       "PyCharm                     0.666212              -0.524970   \n",
       "\n",
       "                           Developer, game or graphics  Developer, front-end  \\\n",
       "Hugging Face Transformers                     0.282830              0.000000   \n",
       "Keras                                        -0.194101              0.885345   \n",
       "NumPy                                        -0.017584              0.000000   \n",
       "Pandas                                       -0.376637             -1.000000   \n",
       "Scikit-learn                                  0.000000              0.000000   \n",
       "TensorFlow                                    0.603529              0.000000   \n",
       "Torch/PyTorch                                -0.051679              0.000000   \n",
       "IPython/Jupyter                              -0.004345              1.030280   \n",
       "Spyder                                       -0.241435              0.000000   \n",
       "Julia                                         0.813359             -1.000000   \n",
       "R                                            -1.000000             -1.000000   \n",
       "Tidyverse                                    -0.141227             -1.000000   \n",
       "RStudio                                       0.145947             -1.000000   \n",
       "Scala                                        -0.537503              0.000000   \n",
       "Cassandra                                    -0.537503              0.000000   \n",
       "Neo4j                                        -0.537503              0.289822   \n",
       "Play Framework                               -0.537503              0.000000   \n",
       "Apache Kafka                                 -0.537503              0.000000   \n",
       "Apache Spark                                 -0.389040              0.460078   \n",
       "Hadoop                                       -0.712825              0.000000   \n",
       "Python                                        0.000000              0.000000   \n",
       "Django                                       -0.188561              0.016096   \n",
       "FastAPI                                       0.307262             -0.732530   \n",
       "Flask                                         0.051695              0.191448   \n",
       "PyCharm                                       0.131513             -1.000000   \n",
       "\n",
       "                           Blockchain  \\\n",
       "Hugging Face Transformers   -1.000000   \n",
       "Keras                        1.918531   \n",
       "NumPy                        0.003794   \n",
       "Pandas                      -0.080226   \n",
       "Scikit-learn                 0.000000   \n",
       "TensorFlow                   0.015350   \n",
       "Torch/PyTorch               -1.000000   \n",
       "IPython/Jupyter             -1.000000   \n",
       "Spyder                      -1.000000   \n",
       "Julia                       -1.000000   \n",
       "R                           -1.000000   \n",
       "Tidyverse                    0.000000   \n",
       "RStudio                      0.000000   \n",
       "Scala                        0.000000   \n",
       "Cassandra                    0.000000   \n",
       "Neo4j                        0.000000   \n",
       "Play Framework               0.000000   \n",
       "Apache Kafka                 0.000000   \n",
       "Apache Spark                 0.000000   \n",
       "Hadoop                       0.000000   \n",
       "Python                       0.000000   \n",
       "Django                       0.000000   \n",
       "FastAPI                      1.798180   \n",
       "Flask                       -0.141563   \n",
       "PyCharm                     -1.000000   \n",
       "\n",
       "                           Developer, desktop or enterprise applications  \\\n",
       "Hugging Face Transformers                                      -0.120549   \n",
       "Keras                                                          -0.004639   \n",
       "NumPy                                                          -0.174406   \n",
       "Pandas                                                         -0.517334   \n",
       "Scikit-learn                                                    0.000000   \n",
       "TensorFlow                                                     -0.170058   \n",
       "Torch/PyTorch                                                  -0.385988   \n",
       "IPython/Jupyter                                                -0.515815   \n",
       "Spyder                                                         -0.208532   \n",
       "Julia                                                          -0.367086   \n",
       "R                                                              -0.805454   \n",
       "Tidyverse                                                      -0.156629   \n",
       "RStudio                                                        -0.204975   \n",
       "Scala                                                          -0.090098   \n",
       "Cassandra                                                      -0.215572   \n",
       "Neo4j                                                          -0.195250   \n",
       "Play Framework                                                 -0.195537   \n",
       "Apache Kafka                                                   -0.265089   \n",
       "Apache Spark                                                   -0.360783   \n",
       "Hadoop                                                         -0.354433   \n",
       "Python                                                          0.000000   \n",
       "Django                                                         -0.256008   \n",
       "FastAPI                                                        -0.004895   \n",
       "Flask                                                          -0.091497   \n",
       "PyCharm                                                        -0.166187   \n",
       "\n",
       "                           DevOps specialist  Academic researcher  \n",
       "Hugging Face Transformers                NaN             0.114486  \n",
       "Keras                                    NaN             0.816835  \n",
       "NumPy                                    NaN             1.318041  \n",
       "Pandas                                   inf             0.433036  \n",
       "Scikit-learn                             NaN             0.000000  \n",
       "TensorFlow                               inf             0.481468  \n",
       "Torch/PyTorch                            NaN             0.921359  \n",
       "IPython/Jupyter                          inf             0.217036  \n",
       "Spyder                                   NaN             0.471292  \n",
       "Julia                                    NaN             2.633780  \n",
       "R                                        NaN             1.348061  \n",
       "Tidyverse                                NaN             1.581765  \n",
       "RStudio                                  NaN             1.609563  \n",
       "Scala                                    inf            -0.341943  \n",
       "Cassandra                                inf            -0.392658  \n",
       "Neo4j                                    inf            -0.329871  \n",
       "Play Framework                           inf            -0.261031  \n",
       "Apache Kafka                             inf            -0.291401  \n",
       "Apache Spark                             inf            -0.365628  \n",
       "Hadoop                                   inf            -0.403200  \n",
       "Python                                   NaN             0.000000  \n",
       "Django                                   inf             0.110815  \n",
       "FastAPI                                  inf            -0.053258  \n",
       "Flask                                    inf             0.003583  \n",
       "PyCharm                                  NaN            -0.229771  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_results = pd.DataFrame(simulated_results)\n",
    "simulated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9aa6985a-aee9-481d-8ff0-904e78f71289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_job = 'Data scientist or machine learning specialist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8396fd1b-0189-4082-93e2-131424ac239f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyCharm                      0.825358\n",
       "IPython/Jupyter              0.477927\n",
       "Hadoop                       0.419609\n",
       "Apache Spark                 0.304274\n",
       "R                            0.250101\n",
       "Torch/PyTorch                0.241399\n",
       "TensorFlow                   0.213591\n",
       "Tidyverse                    0.210361\n",
       "Hugging Face Transformers    0.199043\n",
       "Play Framework               0.197466\n",
       "Django                       0.190382\n",
       "Apache Kafka                 0.176673\n",
       "Cassandra                    0.164549\n",
       "FastAPI                      0.149674\n",
       "Flask                        0.128673\n",
       "Keras                        0.127764\n",
       "Spyder                       0.124296\n",
       "Pandas                       0.107544\n",
       "Neo4j                        0.096752\n",
       "RStudio                      0.054669\n",
       "Scikit-learn                 0.000000\n",
       "Python                       0.000000\n",
       "NumPy                       -0.027646\n",
       "Scala                       -0.055937\n",
       "Julia                       -0.072374\n",
       "Name: Data scientist or machine learning specialist, dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_results = simulated_results[target_job].sort_values(ascending=False)\n",
    "target_results.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b7434290-3165-48d7-af56-b5e5a3c506d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.10\n",
    "recommendations = target_results[target_results > threshold].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "57fb09f0-5e8d-403c-ab83-e0ab783ef51c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current skills: ['SQL', 'Python', 'Scikit-learn']\n",
      "Your target job: Data scientist or machine learning specialist\n",
      "You might also consider learning: ['PyCharm', 'IPython/Jupyter', 'Hadoop', 'Apache Spark', 'R', 'Torch/PyTorch', 'TensorFlow', 'Tidyverse', 'Hugging Face Transformers', 'Play Framework', 'Django', 'Apache Kafka', 'Cassandra', 'FastAPI', 'Flask', 'Keras', 'Spyder', 'Pandas']\n"
     ]
    }
   ],
   "source": [
    "print(\"Your current skills: \" + str(entry_skills))\n",
    "print(\"Your target job: \" + str(target_job))\n",
    "print(\"You might also consider learning: \" + str(recommendations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3315561d-4b34-41fc-8a88-da7dc773abf3",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a83aa4f-9ece-40ad-8e13-3d0a1a797227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Snippet for Creating LDA visualization\n",
    "def row_tokenizer(col):\n",
    "    col_tokenized=[]\n",
    "    for row in col:\n",
    "        words=[word for word in word_tokenize(row) if len(word) > 2]\n",
    "        col_tokenized.append(words)\n",
    "    return col_tokenized\n",
    "    \n",
    "def get_lda_objects(col):    \n",
    "    col_tokenized = create_corpus(col)\n",
    "    dictionary=gensim.corpora.Dictionary(col_tokenized)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in col_tokenized]\n",
    "\n",
    "    # Creating the object for LDA model using gensim library\n",
    "    LDA = gensim.models.ldamodel.LdaModel\n",
    "     \n",
    "    # Build LDA model\n",
    "    lda_model = LDA(corpus=bow_corpus, id2word=dictionary, \n",
    "                    num_topics=10, random_state=100,\n",
    "                    chunksize=1000, passes=50,iterations=100)\n",
    "    \n",
    "    return lda_model, bow_corpus, dic\n",
    "    \n",
    "def plot_lda_vis(lda_model, bow_corpus, dic):\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\n",
    "    \n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7a008ef-8b91-4687-9e7f-b1f3c86d5940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model, bow_corpus, dic = get_lda_objects(temp_df['review_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3c838da-faf8-40cb-bc70-a162c7a14789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.054*\"movie\" + 0.021*\"see\" + 0.019*\"like\" + 0.015*\"one\" + 0.015*\"get\" + 0.014*\"watch\" + 0.013*\"good\" + 0.012*\"make\" + 0.012*\"think\" + 0.010*\"time\"'),\n",
       " (1,\n",
       "  '0.026*\"jane\" + 0.020*\"joe\" + 0.014*\"kelly\" + 0.013*\"bond\" + 0.012*\"sean\" + 0.011*\"morgan\" + 0.011*\"patrick\" + 0.010*\"karen\" + 0.009*\"tommy\" + 0.008*\"sinatra\"'),\n",
       " (2,\n",
       "  '0.010*\"one\" + 0.010*\"get\" + 0.010*\"man\" + 0.007*\"kill\" + 0.006*\"take\" + 0.006*\"scene\" + 0.005*\"murder\" + 0.004*\"two\" + 0.004*\"end\" + 0.004*\"woman\"'),\n",
       " (3,\n",
       "  '0.055*\"film\" + 0.014*\"story\" + 0.014*\"well\" + 0.013*\"one\" + 0.013*\"character\" + 0.012*\"good\" + 0.012*\"see\" + 0.010*\"make\" + 0.009*\"great\" + 0.009*\"movie\"'),\n",
       " (4,\n",
       "  '0.008*\"make\" + 0.007*\"film\" + 0.007*\"people\" + 0.006*\"would\" + 0.005*\"seem\" + 0.005*\"even\" + 0.005*\"one\" + 0.005*\"character\" + 0.005*\"point\" + 0.005*\"life\"'),\n",
       " (5,\n",
       "  '0.021*\"lynch\" + 0.020*\"unfunny\" + 0.014*\"dinosaur\" + 0.014*\"blah\" + 0.014*\"kim\" + 0.013*\"bell\" + 0.011*\"hardy\" + 0.010*\"stan\" + 0.010*\"retarded\" + 0.010*\"lemmon\"'),\n",
       " (6,\n",
       "  '0.021*\"play\" + 0.014*\"comedy\" + 0.013*\"role\" + 0.012*\"star\" + 0.010*\"cast\" + 0.010*\"good\" + 0.009*\"song\" + 0.008*\"john\" + 0.008*\"great\" + 0.008*\"performance\"'),\n",
       " (7,\n",
       "  '0.049*\"show\" + 0.038*\"series\" + 0.030*\"episode\" + 0.011*\"season\" + 0.010*\"first\" + 0.009*\"animation\" + 0.009*\"new\" + 0.008*\"cartoon\" + 0.008*\"original\" + 0.008*\"space\"'),\n",
       " (8,\n",
       "  '0.049*\"film\" + 0.032*\"bad\" + 0.015*\"make\" + 0.013*\"movie\" + 0.012*\"horror\" + 0.012*\"even\" + 0.011*\"act\" + 0.011*\"plot\" + 0.009*\"scene\" + 0.009*\"look\"'),\n",
       " (9,\n",
       "  '0.016*\"life\" + 0.014*\"love\" + 0.014*\"war\" + 0.013*\"family\" + 0.012*\"young\" + 0.011*\"man\" + 0.009*\"father\" + 0.009*\"live\" + 0.009*\"woman\" + 0.009*\"year\"')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9194201b-ec14-405f-a3f6-d6a00db46559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el240402351133087936430626804\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el240402351133087936430626804_data = {\"mdsDat\": {\"x\": [0.20369728981442706, 0.19674068973974526, 0.15553075469254732, 0.10604112303798373, 0.16559848177170966, 0.019705000240722848, -0.0363665556099355, -0.061742235962320605, -0.38226472254689997, -0.36693982517797985], \"y\": [-0.06402467962178307, -0.05916897281502635, -0.09271432142377427, 0.07889830789476267, -0.21752672148578062, 0.2146216241690163, 0.19997145288084048, 0.15136430470632237, -0.10232355682574377, -0.10909743747883313], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [28.65656640054897, 19.97589534690409, 13.50875216002245, 13.345973006481534, 9.507712270937038, 6.19813644134107, 4.759828759337519, 2.9893620809723767, 0.6734046101740738, 0.3843689232808784]}, \"tinfo\": {\"Term\": [\"film\", \"movie\", \"show\", \"bad\", \"play\", \"see\", \"series\", \"good\", \"love\", \"life\", \"great\", \"first\", \"episode\", \"role\", \"story\", \"comedy\", \"watch\", \"man\", \"get\", \"act\", \"make\", \"family\", \"horror\", \"think\", \"performance\", \"star\", \"even\", \"like\", \"really\", \"year\", \"that\", \"kinda\", \"brando\", \"clown\", \"moron\", \"lol\", \"nerd\", \"anyways\", \"hop\", \"came\", \"madonna\", \"nerve\", \"apocalypse\", \"priceless\", \"jordan\", \"aussie\", \"basketball\", \"geek\", \"troma\", \"travolta\", \"crocodile\", \"norris\", \"cannibal\", \"tonight\", \"what\", \"karate\", \"showtime\", \"dud\", \"prank\", \"profanity\", \"hey\", \"kid\", \"movie\", \"laugh\", \"buy\", \"joke\", \"think\", \"funny\", \"watch\", \"hate\", \"yeah\", \"teen\", \"see\", \"maybe\", \"dad\", \"get\", \"want\", \"really\", \"like\", \"guy\", \"say\", \"please\", \"guess\", \"thing\", \"know\", \"people\", \"wait\", \"would\", \"something\", \"lot\", \"sure\", \"time\", \"good\", \"ever\", \"happen\", \"one\", \"make\", \"could\", \"well\", \"come\", \"first\", \"end\", \"even\", \"way\", \"love\", \"show\", \"bad\", \"much\", \"look\", \"character\", \"adaptation\", \"stunning\", \"shakespeare\", \"brilliantly\", \"visually\", \"bollywood\", \"che\", \"emma\", \"layer\", \"bettie\", \"hamlet\", \"passionate\", \"superbly\", \"turkish\", \"ensemble\", \"painting\", \"stellar\", \"homer\", \"miyazaki\", \"filler\", \"flawless\", \"scarlett\", \"contribution\", \"masterful\", \"penguin\", \"troubled\", \"velvet\", \"pitiful\", \"gus\", \"akshay\", \"superb\", \"novel\", \"beautifully\", \"excellent\", \"film\", \"cinema\", \"performance\", \"compelling\", \"cinematography\", \"brilliant\", \"book\", \"story\", \"outstanding\", \"unique\", \"version\", \"masterpiece\", \"visual\", \"character\", \"style\", \"great\", \"drama\", \"well\", \"beautiful\", \"quite\", \"role\", \"fine\", \"wonderful\", \"director\", \"work\", \"actor\", \"good\", \"many\", \"much\", \"one\", \"also\", \"see\", \"although\", \"time\", \"scene\", \"make\", \"cast\", \"music\", \"feel\", \"give\", \"act\", \"first\", \"like\", \"play\", \"movie\", \"way\", \"really\", \"watch\", \"would\", \"werewolf\", \"christian\", \"football\", \"freedom\", \"wolf\", \"theory\", \"catholic\", \"maria\", \"button\", \"eastwood\", \"wine\", \"mummy\", \"cancel\", \"scarecrow\", \"christ\", \"altman\", \"observe\", \"philosophy\", \"flynn\", \"incoherent\", \"penn\", \"addict\", \"vague\", \"examine\", \"dilemma\", \"pot\", \"cusack\", \"morality\", \"educate\", \"pseudo\", \"religious\", \"documentary\", \"religion\", \"belief\", \"question\", \"self\", \"interview\", \"subject\", \"sexual\", \"medium\", \"artistic\", \"system\", \"human\", \"reality\", \"social\", \"message\", \"issue\", \"answer\", \"point\", \"attempt\", \"game\", \"gay\", \"world\", \"state\", \"viewer\", \"event\", \"audience\", \"present\", \"people\", \"allow\", \"fact\", \"view\", \"seem\", \"may\", \"problem\", \"become\", \"life\", \"would\", \"without\", \"make\", \"way\", \"need\", \"use\", \"even\", \"character\", \"take\", \"film\", \"real\", \"one\", \"work\", \"know\", \"show\", \"say\", \"man\", \"thing\", \"like\", \"could\", \"murder\", \"police\", \"prison\", \"detective\", \"officer\", \"ford\", \"sheriff\", \"kidnap\", \"murderer\", \"seagal\", \"cowboy\", \"convict\", \"sinister\", \"fbi\", \"fulci\", \"claire\", \"stalk\", \"fighter\", \"walker\", \"powell\", \"thug\", \"argento\", \"wallace\", \"lugosi\", \"frankenstein\", \"ruth\", \"colonel\", \"wax\", \"francis\", \"marine\", \"cop\", \"revenge\", \"guard\", \"agent\", \"escape\", \"chief\", \"kill\", \"criminal\", \"gang\", \"killer\", \"gun\", \"town\", \"serial\", \"rescue\", \"victim\", \"fire\", \"house\", \"crime\", \"doctor\", \"car\", \"hero\", \"man\", \"chase\", \"match\", \"attack\", \"run\", \"train\", \"dead\", \"shoot\", \"death\", \"get\", \"take\", \"head\", \"woman\", \"turn\", \"black\", \"scene\", \"one\", \"fight\", \"back\", \"two\", \"around\", \"end\", \"later\", \"find\", \"also\", \"set\", \"look\", \"come\", \"film\", \"little\", \"play\", \"guy\", \"zombie\", \"gore\", \"vampire\", \"slasher\", \"laughable\", \"dracula\", \"ninja\", \"discussion\", \"atrocious\", \"mstk\", \"useless\", \"caine\", \"ranger\", \"boll\", \"uninspired\", \"horrendous\", \"stinker\", \"robocop\", \"nun\", \"sheen\", \"abysmal\", \"trashy\", \"robinson\", \"bon\", \"uwe\", \"mutant\", \"myers\", \"belushi\", \"dreck\", \"scifi\", \"halloween\", \"poorly\", \"budget\", \"horror\", \"pointless\", \"awful\", \"amateurish\", \"terrible\", \"cheap\", \"bad\", \"waste\", \"low\", \"porn\", \"whatsoever\", \"effect\", \"badly\", \"redeem\", \"script\", \"avoid\", \"poor\", \"dull\", \"horrible\", \"film\", \"decent\", \"ridiculous\", \"plot\", \"flick\", \"act\", \"boring\", \"dialogue\", \"even\", \"special\", \"make\", \"look\", \"scene\", \"minute\", \"nothing\", \"director\", \"actor\", \"movie\", \"like\", \"one\", \"good\", \"ever\", \"well\", \"really\", \"could\", \"americans\", \"african\", \"betty\", \"germany\", \"europe\", \"africa\", \"hitler\", \"tarzan\", \"wwii\", \"india\", \"greek\", \"nation\", \"columbo\", \"jewish\", \"soviet\", \"china\", \"hoffman\", \"cultural\", \"civil\", \"hank\", \"bruno\", \"victoria\", \"rita\", \"russia\", \"poetry\", \"germans\", \"mann\", \"falk\", \"philosophical\", \"poem\", \"war\", \"german\", \"marriage\", \"mother\", \"france\", \"father\", \"country\", \"family\", \"russian\", \"east\", \"son\", \"american\", \"young\", \"child\", \"indian\", \"life\", \"daughter\", \"america\", \"live\", \"soldier\", \"husband\", \"love\", \"woman\", \"sister\", \"man\", \"marry\", \"wife\", \"world\", \"year\", \"old\", \"home\", \"british\", \"boy\", \"history\", \"friend\", \"heart\", \"story\", \"two\", \"show\", \"film\", \"become\", \"girl\", \"day\", \"stewart\", \"singer\", \"williams\", \"arthur\", \"allen\", \"danny\", \"davis\", \"joan\", \"singing\", \"eddie\", \"jimmy\", \"russell\", \"broadway\", \"keaton\", \"concert\", \"nancy\", \"stooge\", \"murphy\", \"punk\", \"roberts\", \"woody\", \"brooks\", \"lloyd\", \"mgm\", \"widow\", \"album\", \"matthau\", \"muppet\", \"hyde\", \"carol\", \"ben\", \"gary\", \"chaplin\", \"dance\", \"musical\", \"charlie\", \"sing\", \"tony\", \"julie\", \"song\", \"band\", \"comedy\", \"mary\", \"howard\", \"john\", \"james\", \"play\", \"role\", \"george\", \"michael\", \"star\", \"rock\", \"cast\", \"career\", \"number\", \"tom\", \"performance\", \"oscar\", \"support\", \"jack\", \"great\", \"good\", \"stage\", \"funny\", \"music\", \"talent\", \"big\", \"actor\", \"also\", \"year\", \"well\", \"fun\", \"series\", \"episode\", \"season\", \"animation\", \"cartoon\", \"disney\", \"planet\", \"batman\", \"animate\", \"trek\", \"von\", \"anime\", \"critter\", \"superman\", \"burt\", \"branagh\", \"carter\", \"kirk\", \"jet\", \"dire\", \"karloff\", \"abc\", \"streisand\", \"enterprise\", \"spock\", \"amazon\", \"nero\", \"ernest\", \"doc\", \"futuristic\", \"robot\", \"space\", \"alien\", \"pilot\", \"show\", \"sci\", \"science\", \"television\", \"earth\", \"lion\", \"ape\", \"adventure\", \"voice\", \"original\", \"air\", \"ship\", \"new\", \"first\", \"program\", \"star\", \"special\", \"effect\", \"animal\", \"fan\", \"year\", \"back\", \"world\", \"use\", \"jane\", \"kelly\", \"sean\", \"morgan\", \"patrick\", \"karen\", \"tommy\", \"sinatra\", \"jon\", \"frankie\", \"hopper\", \"khan\", \"connery\", \"derek\", \"lou\", \"sullivan\", \"niro\", \"doug\", \"glover\", \"mario\", \"antonio\", \"voight\", \"marty\", \"timothy\", \"dee\", \"spade\", \"beatty\", \"sid\", \"ned\", \"meg\", \"joe\", \"bond\", \"dennis\", \"bobby\", \"foster\", \"carpenter\", \"gene\", \"jones\", \"laura\", \"james\", \"lynch\", \"unfunny\", \"dinosaur\", \"blah\", \"bell\", \"hardy\", \"stan\", \"retarded\", \"lemmon\", \"laurel\", \"monk\", \"egg\", \"tripe\", \"tasteless\", \"dreary\", \"novak\", \"shove\", \"pimp\", \"immature\", \"abortion\", \"inability\", \"frontal\", \"brow\", \"arrow\", \"casper\", \"delete\", \"hmm\", \"arquette\", \"upside\", \"lizard\", \"kim\", \"lake\"], \"Freq\": [100343.0, 106187.0, 22726.0, 26620.0, 17583.0, 49474.0, 6583.0, 40882.0, 18185.0, 14943.0, 20883.0, 18598.0, 5143.0, 9160.0, 26727.0, 7670.0, 28347.0, 16594.0, 36459.0, 14602.0, 47075.0, 6584.0, 7829.0, 23633.0, 9322.0, 8823.0, 25527.0, 45923.0, 24178.0, 13863.0, 763.0037962014145, 542.4497917522144, 305.8218032953048, 288.5249605402288, 284.8377501226798, 261.43125414156145, 259.3070783607541, 254.8942892173301, 254.41857675116594, 252.63486538890217, 230.0991023493896, 222.41734986190286, 220.750540900978, 219.2980369797762, 218.75702074519648, 218.29520576730337, 211.25526237050755, 210.43716195801144, 205.54101432546446, 205.35469661506235, 203.3064761377987, 199.6649374831751, 195.62841441925593, 194.64735513693785, 187.26753640198368, 185.6675459876077, 183.72860593282152, 181.5916026260754, 175.58091490474533, 172.10626632986117, 817.6300137754738, 5965.053178667387, 89248.13162391685, 5996.086991299581, 2521.4179308444786, 2982.174391395641, 19101.378806235123, 7675.315209390138, 22520.46637712826, 2301.2547872418777, 1060.7425750684788, 1180.723582632686, 34457.085799554494, 3954.2841721491172, 947.888237960226, 25342.906404509988, 10285.157712666527, 17165.357320162253, 30912.28568534611, 7151.173122333989, 14200.405427172667, 1844.0726260270865, 2667.9128454437405, 11614.86554338581, 13163.034580320531, 12336.994806877863, 2195.5557811536264, 15244.063901659465, 6837.667330450144, 6612.17678428484, 3889.7144219376028, 17309.115631776982, 21160.44913434189, 7562.7942196476115, 4739.82368330185, 25537.034984202397, 20262.654080281427, 8715.915274693409, 15078.383528546105, 8482.063537937567, 9208.031393484116, 9353.634752953463, 11040.569392775393, 8396.366527909584, 8323.485598830028, 8901.086370180885, 9162.830029279376, 8411.20462126916, 8408.248543763975, 8455.50731329117, 1049.6233695634812, 749.4680756606228, 656.9170993598968, 529.4884394815779, 416.6523296293674, 381.0282612633158, 335.74771818194756, 296.65855525095594, 270.3478713141403, 269.1513627971531, 265.2186799175536, 255.05980503979097, 249.2768816686907, 244.21014041406468, 243.9738489214821, 238.90016876219377, 235.14838346235337, 234.8760398346775, 223.45912813312734, 219.01957000039383, 215.10215606317922, 214.87752997553616, 208.96472841064795, 208.01160113665915, 203.5434325926731, 202.57966231274088, 201.85231761050554, 201.39600920608214, 200.0829472035353, 196.05883503665828, 1406.456307173628, 1972.0088153236604, 865.5530007884922, 3495.9328003658347, 62977.049261788175, 2485.1683695921824, 6800.315360900943, 598.2943499427777, 1749.7013274761653, 1949.0106706130498, 4130.407202600892, 16391.39998404186, 660.9631847248712, 1044.626955951278, 3289.9035135582103, 1265.7799709651026, 1280.2037748749306, 14812.51451066965, 2554.3002409661176, 10482.004046970414, 2126.905414238586, 15911.962031495888, 2619.274132947239, 4113.780380263977, 4793.365305655536, 2064.883674217851, 2114.6567390520804, 4985.706111319866, 6310.2246141982405, 6212.597778855119, 13852.174396130127, 6039.676788457478, 7415.834686875292, 15443.022407055038, 7082.3912884242445, 13661.523599245771, 2886.007237187661, 9353.544798144681, 7419.032885020323, 11797.965774197974, 4092.129388696211, 3359.9672764761654, 4493.475366193774, 5362.837658035479, 4708.021739458397, 5225.866495788953, 7693.553920866466, 5011.9830393291695, 9864.753414000901, 4557.276281595458, 4719.264508117818, 4578.74813545059, 4382.4172249564845, 708.2401321063144, 640.6705819089734, 581.0544442827604, 503.5302297938958, 499.07371455617965, 486.35240737632733, 460.48729092599564, 416.8023247906452, 381.27560698620937, 359.44547844366514, 349.4655610493966, 344.32673599164707, 331.70562514404986, 326.30234524193634, 298.93435181433773, 292.1860834992462, 272.6729384679688, 253.27327371706903, 244.98198188524813, 242.11770424510436, 241.16515902307685, 240.89369950299644, 238.1382096905659, 237.36583651577007, 236.75257602334955, 233.61418660727153, 225.5394049947649, 224.56534703081203, 220.14775421447195, 219.64534441542875, 646.1593007197453, 2394.9832276256666, 519.3852792748686, 507.84574476314424, 1976.1041638326326, 1981.257247599203, 895.5923573178811, 1555.6059998506862, 1206.8640059346046, 705.7564871844729, 579.454521943868, 641.3285227115466, 2521.0021042243707, 1556.440395008525, 872.8906793666913, 1328.0853370642697, 1124.6136075273662, 997.0834438067648, 3788.5780148233216, 2171.7077156873856, 1809.152410582991, 1026.7982302879188, 3608.877479743445, 1205.9180800177849, 1804.2176732128644, 1513.4917132332757, 2477.5419224398443, 1464.450187702775, 5394.220158818542, 1248.4212734850073, 2906.383493947696, 2107.7781032887347, 4008.3955291807138, 2506.7100268511645, 1999.8304739598254, 2670.276866903444, 3612.8603066194405, 4609.297144050358, 2240.9938874811864, 6034.912321476001, 3588.2451960861254, 2227.653357900456, 2695.074053667681, 3880.4013793676695, 3806.383764481494, 3107.2306236319973, 5837.147793159434, 2419.490071714898, 3815.8771176052815, 2548.6677170289327, 2725.105932070913, 2546.300363325297, 2418.3214469817717, 2324.186403437865, 2282.692064624643, 2411.1227258797476, 2239.427197681454, 3739.6007041983867, 2219.152605121875, 975.9969052259138, 959.3023472081195, 802.2024524733458, 741.7616648637229, 517.2216108678132, 468.25964764256196, 440.2758224110486, 427.4671863916331, 419.3397883257259, 371.98821001457526, 362.1785592462139, 359.5149719379402, 351.52194804782647, 314.1737880665313, 312.2022501592898, 309.48090435112084, 309.46621277456745, 307.4304216156007, 304.9004470294576, 304.52160143650394, 300.9992063750383, 291.6604923014411, 290.84166086554774, 290.38781208680535, 285.4607365786829, 283.6755756532669, 273.86388687657274, 270.24825853280237, 1915.5610297517335, 956.1699955875903, 521.5267355032757, 903.5824827252559, 1566.8869184028895, 474.69016861654944, 5200.247995073188, 842.2366402502722, 961.2587456248882, 2224.1056372416215, 1423.3991276759923, 1941.7324608915367, 746.2851027434577, 692.9549038115131, 1351.014267494009, 1384.9991986250657, 2945.233609915557, 1346.093933090372, 1197.5741878111455, 2082.5829344615277, 1900.6994250745456, 7495.298359663422, 1215.8727255739946, 1239.1000185221633, 1218.4416014840297, 2981.360554991545, 1234.5295048895086, 2186.021308338242, 2372.387181898558, 2233.66089529951, 7798.797664766185, 4864.08286533174, 2111.737830635081, 3131.7668241161277, 2844.9114627582767, 2099.466960067191, 4849.832752737196, 7880.847611499073, 1925.8936872079787, 2922.339475340572, 3353.1183580860265, 2375.0547780364095, 3294.645166741155, 1871.3503134255789, 2851.9935193192946, 2824.671486903564, 2202.0560891528903, 2724.247215372681, 2320.478796120684, 2729.5741660322687, 2248.087844309398, 2224.050095915627, 2159.746319500294, 2028.790275954892, 2010.3172134210477, 1553.7257268373626, 1125.7686997431074, 900.9473294126586, 386.4268434044995, 373.0723240457552, 358.9861511755783, 342.26674043761676, 339.7773188015049, 334.2234339692688, 316.564766346619, 304.98958595182864, 249.63266978941462, 246.63245642299373, 238.8296041844595, 231.51685034891244, 220.45290841778888, 212.77925784822366, 192.2514611901297, 191.2526770855201, 190.4203011566757, 181.65111680102052, 176.0756770842725, 173.31615726910704, 172.88391083594968, 167.6598907241302, 164.4598120444005, 160.21862127441355, 156.45112741127886, 552.7189961347996, 1313.2848990243315, 3495.129980128614, 6507.530615463475, 863.9215581691866, 2911.3707377347505, 443.7689947445061, 2804.9540016686806, 1781.624068897953, 17457.122464277683, 3365.714644976499, 3182.191476085027, 793.4501368113229, 611.4773241284275, 4018.5360564416505, 1068.3660160331244, 740.8086734092839, 4081.007759743073, 1377.2710284085572, 2608.103372696134, 1227.4305335872932, 1803.6641058363493, 26989.158402137855, 1452.9176642589052, 1278.7386255545239, 5798.477982790488, 1990.2008103211613, 5881.228827772123, 1590.3306752578258, 1895.2078120416013, 6363.886184676226, 2191.767284256314, 8091.342302288677, 4827.36237251273, 5007.78087220588, 2761.4878670635508, 2788.869303303518, 2963.4223216473943, 3356.103751155775, 7074.058969334269, 4070.7987537753606, 4135.121464835132, 3073.090766005541, 2382.1019881106904, 2763.4938274779906, 2292.7047718704193, 2208.768934747802, 697.066148628753, 574.4070548472544, 550.435101578962, 482.6404125881996, 470.9195576618878, 438.07500325152, 433.25313729368673, 415.6126523635758, 415.52316208604714, 410.59870057991736, 400.3214640357387, 399.1335440476973, 398.7322315339633, 394.7337510166556, 385.4936198366032, 385.1108211180808, 381.2622815399705, 377.41045524315973, 332.23201046278626, 313.48897702277844, 278.27172374090867, 266.8906281597564, 253.74874584187296, 249.23514497532233, 234.91428753674694, 216.91927382934136, 214.67036776155237, 209.74345161740578, 194.35197676516933, 193.55066226477805, 4988.895504447254, 1220.1540448541837, 824.8116268174235, 3026.2534825839707, 615.0628082796081, 3192.7514960572366, 1876.5584490696606, 4648.270296128289, 679.4285343770008, 423.9515985591795, 2061.639320451328, 2647.767438245065, 4287.284739144467, 3076.7231700358357, 723.1615718017712, 5724.261786092891, 1600.2277823456002, 1056.8637024164655, 3144.013867756499, 1219.9465239316764, 1323.7973491475675, 5055.615727651194, 3132.8170572447625, 1223.2658601949727, 4020.3862062182156, 848.4730891786045, 1715.644362648367, 2360.9429027708325, 3123.2318206011582, 2489.3662354884545, 1499.072732128663, 981.811439286597, 1546.989928875302, 1147.9142144351197, 1641.2947916558578, 1136.0755368840003, 2025.302024163187, 1453.30446695567, 1544.3474070401028, 1809.8405556916355, 1300.7720956771207, 1298.2389092437559, 1253.598612645546, 756.9049307389572, 728.0498907628383, 696.8896221024628, 655.4156979430727, 648.3727246206763, 629.6688443972567, 589.7343805289497, 562.2475509028503, 549.7210650086754, 541.3620231093595, 467.65301326210914, 441.4533378105669, 427.91388934523167, 392.8525725871216, 379.8201964752601, 368.8192727892647, 363.6301759277287, 357.8364964850835, 357.14590553805215, 351.07704405391814, 343.88895557624, 339.6058539719313, 337.5661904947866, 330.9209259907408, 307.9988977320764, 293.16945157637156, 289.80554042064455, 277.04294686518983, 273.0649640924403, 269.28219347265673, 946.2669173415285, 573.2207103726546, 433.7544708239741, 1748.5489290122578, 1795.1857832977862, 744.5394709087674, 813.4378590440786, 817.5780819662164, 500.69000235869214, 2451.3335108396736, 899.220405081314, 3888.452281173874, 791.8098860769992, 561.3483609138034, 2264.0676107358736, 1263.3908170637897, 5826.60288280696, 3659.0011306629526, 1208.3950486774086, 1579.605366819157, 3378.643762597019, 1131.8763576609292, 2723.0504247897084, 1134.8379592536205, 1227.6770252503811, 919.1512783526074, 2150.703216914941, 958.1648264930758, 1129.5554097656645, 952.2792599551883, 2259.545078710235, 2621.383168734196, 906.0846522525333, 1394.3190649146661, 1292.8509188652556, 939.721143941302, 1106.3204989187846, 1146.895105290664, 1074.3711386456503, 1023.7431383456336, 1025.0870030883602, 945.7027510244949, 6583.044435167782, 5142.581544939973, 1912.8067335244748, 1590.796688716808, 1458.9168502628486, 1149.4236661610987, 907.9810649390835, 852.4934134553324, 838.6434724509552, 589.1285880452931, 362.4557363784622, 326.20677164888195, 316.9677923836878, 307.9345059076369, 300.0317572450583, 279.6875344543415, 267.4112606708542, 254.95114557099478, 243.98038190820134, 237.6722891171266, 236.74453457622096, 236.73755173643735, 230.5015346939337, 221.97121333861858, 213.89618116344772, 212.40129161207062, 208.5096626987575, 206.5766802134849, 203.4256135642345, 202.23548082502273, 630.751169038469, 1324.38473662901, 1011.5598550626875, 569.505042477761, 8432.466406988397, 988.4037951899093, 793.167527703855, 1059.3069885150396, 949.6525659420079, 313.59451893051863, 351.5918021381054, 702.5185489837162, 955.684375052104, 1453.851161181205, 693.2214997807814, 547.7507676259116, 1488.722499910257, 1779.7245624273141, 428.31940620550574, 1106.95403459716, 844.6809802004149, 748.6483925321623, 514.3106301336585, 747.5818640833733, 873.6344826066437, 740.3156173195763, 683.5450496162701, 659.7358935674085, 1001.810888171752, 532.581898049084, 482.87130445818514, 446.05720465928755, 415.0304191511004, 401.7610331678392, 362.4442669933689, 317.66596094159587, 289.94890099930666, 281.7565550223565, 273.03146717111076, 271.5909938934888, 255.4276072354099, 240.0371799483818, 238.62044636466618, 233.60545862757567, 217.92572529165554, 213.73166448687513, 197.45023025719348, 187.99429251479629, 184.17822260171872, 179.43212803125041, 175.27999590138913, 173.23782173000563, 172.84001461246783, 170.22081942712643, 169.5264559809887, 163.021653287167, 160.92024740425484, 158.99772996879832, 763.733182196789, 486.9378977518919, 263.1740881310728, 225.88532101753466, 216.5625642235869, 221.97407536285547, 238.38914255343715, 263.4699574007478, 200.0277498945881, 219.24149737440163, 464.77253111426654, 433.272256456304, 309.74209177310536, 309.21261034985207, 298.35845789459444, 253.74178925351202, 231.19854953874616, 228.45163632410922, 215.49400627143277, 212.32588978995258, 184.68445378355437, 174.00658873468217, 155.95288832443936, 154.52865698117566, 154.3632366066907, 152.15555727983303, 146.45229538485336, 142.35467099339047, 137.2931212642259, 137.19173691342795, 134.75447734757893, 128.9471582041164, 128.8689011263374, 120.02210515252419, 119.17584923336874, 109.69918853212766, 109.38152651035324, 108.36244040768666, 103.87698785020311, 103.01590199292707, 305.03865873337884, 161.5765721729], \"Total\": [100343.0, 106187.0, 22726.0, 26620.0, 17583.0, 49474.0, 6583.0, 40882.0, 18185.0, 14943.0, 20883.0, 18598.0, 5143.0, 9160.0, 26727.0, 7670.0, 28347.0, 16594.0, 36459.0, 14602.0, 47075.0, 6584.0, 7829.0, 23633.0, 9322.0, 8823.0, 25527.0, 45923.0, 24178.0, 13863.0, 763.9320851608945, 543.3780763346276, 306.75053555418816, 289.45334900493077, 285.76610767869516, 262.3595707824517, 260.2353828349996, 255.8225655941692, 255.34757672469493, 253.56317371945974, 231.0275844166787, 223.34608428311415, 221.6792915931803, 220.22662718320552, 219.68587896925447, 219.22369647543894, 212.1837135135199, 211.36560635530978, 206.4693123749264, 206.28313386232134, 204.23490124128483, 200.59337093717005, 196.55678747373227, 195.57577515118388, 188.19581850404805, 186.59593073801523, 184.6571382154148, 182.52036278268338, 176.50933739825481, 173.0348126860335, 831.7093739278716, 6289.152264828549, 106187.66167293524, 6564.738356004888, 2729.6070742059383, 3270.175783652338, 23633.519143917947, 9070.458271556332, 28347.843145332678, 2656.586382318529, 1170.6284970972752, 1323.313522549498, 49474.99616366488, 4852.952316881802, 1051.4085900886712, 36459.654279837254, 13834.394621367095, 24178.044275188484, 45923.31678619801, 9420.646318115147, 20053.211286187463, 2218.506238825208, 3369.177412330971, 17240.81992657395, 19923.552065328884, 19067.302997172526, 2824.1865466520394, 25325.455589786496, 10491.243321696575, 10119.223431271415, 5450.614324804004, 32119.954739761677, 40882.24752310484, 12489.988461034352, 7231.529347053003, 57919.900222645374, 47075.7851151682, 16242.446455733078, 36527.14633745238, 16499.172219139935, 18598.55488138467, 19337.48325525175, 25527.613430544738, 18203.765001461823, 18185.76279330206, 22726.25437245278, 26620.77602737253, 19968.192826745988, 20317.14698237169, 29758.24725132115, 1050.5498561308939, 750.3945777250657, 657.8436100019001, 530.4149770572858, 417.5788387179382, 381.9547347209403, 336.67416253509697, 297.58505677579524, 271.27441213548656, 270.0778187245299, 266.145122922409, 255.98657280788154, 250.2034152511892, 245.13670253758556, 244.90032946159872, 239.82675392606018, 236.07499063579488, 235.80279179370865, 224.38557518709166, 219.94641346526276, 216.0286179225133, 215.80422335402324, 209.89134547145068, 208.93808165904036, 204.4700692038639, 203.5064940362865, 202.77894458551336, 202.32315244246402, 201.00950876603332, 196.98541967756847, 1467.276579768195, 2130.204502215044, 923.3322676713401, 4252.442725974701, 100343.27242184345, 3097.937934157053, 9322.222379062143, 665.521543395359, 2177.859979857488, 2493.3603673221005, 5745.691541102248, 26727.006614854887, 752.6515395577441, 1275.7966742322083, 4767.936208040744, 1616.2142913076366, 1667.2642801582663, 29758.24725132115, 3793.07468388897, 20883.090748011793, 3203.2656020191416, 36527.14633745238, 4253.4701554004005, 7466.313482183817, 9160.579910788889, 3200.8964913944246, 3333.182499269678, 10331.922897727818, 14257.63939363043, 13979.099025143305, 40882.24752310484, 13943.623074965504, 19968.192826745988, 57919.900222645374, 18765.83198184535, 49474.99616366488, 5234.941502614563, 32119.954739761677, 22391.575504446944, 47075.7851151682, 8995.344002010444, 6777.137713876315, 11421.163659805474, 17929.453634878377, 14602.536460316951, 18598.55488138467, 45923.31678619801, 17583.061119336413, 106187.66167293524, 18203.765001461823, 24178.044275188484, 28347.843145332678, 25325.455589786496, 709.1667921044941, 641.5971134150757, 581.9811082189168, 504.4567851675519, 500.0004947206031, 487.2789781719944, 461.4138760266405, 417.7289512060226, 382.2023905600726, 360.3720948644691, 350.392184906736, 345.25337356180563, 332.632525547974, 327.2289978350688, 299.86086342966246, 293.11265097425246, 273.5994919381895, 254.19982304826823, 245.9085384202358, 243.04432405451487, 242.09185605956495, 241.82028078756062, 239.0647614132525, 238.29238511719836, 237.6792226698605, 234.54106995668272, 226.46594696114, 225.49189179177063, 221.0743521347039, 220.57186837049915, 659.1907421285575, 2619.446111160376, 540.3181552630037, 552.5893774509649, 2509.2176407956745, 2552.8926552324133, 1047.8989838886425, 1971.801717440307, 1512.8373606022628, 820.6208241741854, 664.3091014349508, 748.173718302494, 3754.9545812354436, 2152.93764900844, 1118.8206390388298, 1894.263978325316, 1559.0924128662327, 1339.8958792668884, 8311.577791369386, 4028.939962214451, 3181.066204848051, 1483.4243157172791, 8311.438954110637, 1872.9248833364804, 3337.6667046241614, 2611.5281997483735, 5429.757624397299, 2595.563066974661, 19067.302997172526, 2038.7423239901805, 7790.180075497801, 5041.619139236284, 14745.816587104258, 7054.905648194502, 4882.594300299113, 8294.555666478704, 14943.428153094961, 25325.455589786496, 6546.980531104826, 47075.7851151682, 18203.765001461823, 6778.843796958348, 10435.567627340348, 25527.613430544738, 29758.24725132115, 17941.043411902898, 100343.27242184345, 9831.611479928397, 57919.900222645374, 14257.63939363043, 19923.552065328884, 22726.25437245278, 20053.211286187463, 16594.70012955817, 17240.81992657395, 45923.31678619801, 16242.446455733078, 3740.525610324567, 2220.0774813273083, 976.9217934204742, 960.2272515664035, 803.1274095368495, 742.6865702417539, 518.1465460032233, 469.1845731308523, 441.2007344621056, 428.39201753568705, 420.2647868707675, 372.91317346011647, 363.1034798399514, 360.43983988854677, 352.4467815291281, 315.0988906115775, 313.12714216534175, 310.40585948769416, 310.39113643999195, 308.35541491616476, 305.8253262867408, 305.4465270167257, 301.9242633964614, 292.58537539211505, 291.76660655993015, 291.3128273847629, 286.38569808171405, 284.60048938174197, 274.78891005116725, 271.17317028305985, 2078.5301164289335, 1049.5078931373212, 551.8091012569131, 1014.6607330490287, 1883.544139173943, 500.4118899008925, 7306.0078760838405, 950.5745436595366, 1106.8650533775462, 3026.1144373389047, 1804.3000048149402, 2612.237787068599, 860.548849918994, 794.2033924453369, 1745.4162420881783, 1827.3002673735175, 4592.241135069452, 1786.5026710070752, 1578.7196151487728, 3148.850647152396, 2849.475789558997, 16594.70012955817, 1631.7216073677173, 1671.9704326205729, 1666.095693157517, 5603.504948522158, 1712.8773015669412, 3774.2591218448406, 4795.29207126017, 4535.697698509918, 36459.654279837254, 17941.043411902898, 4379.857797094027, 9184.583876616, 7804.218750835423, 4487.952862888453, 22391.575504446944, 57919.900222645374, 4141.979075301517, 10352.803272932628, 13994.097592572152, 7847.155196472062, 19337.48325525175, 4318.093653063021, 16639.445107615495, 18765.83198184535, 7763.400330710934, 20317.14698237169, 16499.172219139935, 100343.27242184345, 12851.788614313191, 17583.061119336413, 9420.646318115147, 2029.7177849697007, 2011.2447264211826, 1554.6532514495796, 1126.6961995328818, 901.8748783623208, 387.35439687238124, 373.99987130592484, 359.91433667336605, 343.19424390468834, 340.7048600546638, 335.1511838034941, 317.49239516584583, 305.9172031213736, 250.56016044525376, 247.5621204083677, 239.75719023284086, 232.4444024163373, 221.38069964487505, 213.70708700439513, 193.17922290784918, 192.18020207250868, 191.3479079633842, 182.57906059047116, 177.00358248734423, 174.24362403858083, 173.81148033234953, 168.58745453438027, 165.38760157530874, 161.14616904327985, 157.37871161413926, 565.2390734778776, 1395.8769316717467, 3918.0758075384942, 7829.962761334434, 945.5440614450905, 3492.641657397673, 472.56961859681513, 3387.471001367661, 2091.522470337201, 26620.77602737253, 4444.33724129006, 4260.679548995811, 920.5160353128043, 705.354405260749, 6246.299446683683, 1344.1622521365005, 888.936246633141, 6709.383660456937, 1855.9862328848424, 4123.564139704645, 1667.7777092697909, 2798.5199506590766, 100343.27242184345, 2220.6459196700034, 1887.8904159534973, 13958.595937299335, 3392.5169740371693, 14602.536460316951, 2640.312299598883, 3480.204951615898, 25527.613430544738, 4528.2046299893045, 47075.7851151682, 20317.14698237169, 22391.575504446944, 7462.625533021272, 8808.972738637978, 10331.922897727818, 13979.099025143305, 106187.66167293524, 45923.31678619801, 57919.900222645374, 40882.24752310484, 12489.988461034352, 36527.14633745238, 24178.044275188484, 16242.446455733078, 697.9922851370334, 575.3332227588219, 551.3613949298361, 483.56651746140676, 471.8457088483901, 439.0011665606513, 434.17925503953586, 416.53880413064843, 416.44929693187123, 411.52483201363134, 401.24762665975413, 400.0597166808614, 399.6583939960166, 395.6598804445375, 386.4197117929341, 386.0370098585824, 382.1884588753101, 378.33660645727986, 333.15808492889335, 314.41516843664516, 279.19788130315334, 267.81678350053267, 254.67508370315633, 250.161252281325, 235.8404652323495, 217.845344690099, 215.59664040848216, 210.66954902857938, 195.27827454469676, 194.476820736784, 5455.303904572567, 1302.8369243413508, 888.9592685838277, 3554.2924101283925, 664.8882296567674, 4223.372957902187, 2364.469563029221, 6584.379530511107, 770.7588057044577, 456.5551884639693, 3047.7467663788843, 4595.477696915785, 8572.847767836125, 5646.528242671749, 898.9793683490785, 14943.428153094961, 2751.9874476265463, 1559.383009791179, 7465.837866296552, 1949.031702603432, 2234.8924806760433, 18185.76279330206, 9184.583876616, 2358.519808393379, 16594.70012955817, 1353.2065976466233, 4656.444796906239, 8311.438954110637, 13863.380790893245, 10687.831704921044, 4103.098578654906, 1764.3543915569608, 4868.912985603607, 2687.2886054711225, 6797.468342679297, 2972.7313741493076, 26727.006614854887, 13994.097592572152, 22726.25437245278, 100343.27242184345, 8294.555666478704, 8111.160065824622, 8094.223139847268, 757.8329467492607, 728.9779194283042, 697.817670095591, 656.3437049130664, 649.3007597404687, 630.5969891163372, 590.662414875641, 563.1756042023072, 550.6491613760782, 542.2900021015512, 468.5810221741011, 442.3813490630023, 428.8418397638245, 393.7805515375934, 380.7482080071313, 369.74734073665235, 364.5581783105704, 358.76454509790875, 358.07402781469693, 352.00504635987653, 344.816942973833, 340.53385547635713, 338.49419836761217, 331.8489184716014, 308.9271388380354, 294.0974698198879, 290.7334622144066, 277.9709283472939, 273.99304421684144, 270.2102486925293, 1032.3246476917775, 605.0480805054119, 449.27027439386256, 2113.72067415193, 2212.4964445209193, 860.4756750524331, 965.4994655070071, 993.59753467639, 548.3217676423541, 3931.967591075917, 1140.0696640567091, 7670.326806868044, 1010.4784653572401, 648.6166593986887, 4278.32683183581, 2039.1394555335405, 17583.061119336413, 9160.579910788889, 1924.903246248485, 2825.367110903112, 8823.730394469898, 1843.8209054321508, 8995.344002010444, 2310.8412312580535, 2814.056976524085, 1568.0500944776963, 9322.222379062143, 1842.5228248812311, 2768.316344582827, 1875.0096737284307, 20883.090748011793, 40882.24752310484, 1701.232205433116, 9070.458271556332, 6777.137713876315, 2507.7362647122523, 8797.269962230395, 13979.099025143305, 18765.83198184535, 13863.380790893245, 36527.14633745238, 5615.504656035602, 6583.973835810253, 5143.510883806642, 1913.736086551658, 1591.726026516108, 1459.8461920165555, 1150.3530085202435, 908.9104364120794, 853.4227394419465, 839.5728408007844, 590.057963310944, 363.38542768643873, 327.1360704508288, 317.8972133265624, 308.8637995828125, 300.9612155031903, 280.6169471850152, 268.3407422675647, 255.8805521293333, 244.90987511573763, 238.60200766254903, 237.6738832832097, 237.66687528869812, 231.43098665289972, 222.90059237400965, 214.82547344066154, 213.33087810582265, 209.43919409532745, 207.50612864313516, 204.3550528965007, 203.164884748841, 741.3963584816827, 1910.6628101559345, 1416.4554653891598, 730.2604829797918, 22726.25437245278, 1510.6290319511986, 1176.447342451796, 1774.8923063354707, 1872.6523818188075, 366.1091938511796, 443.237782135169, 1480.692558277366, 3018.030341561288, 6716.90759080876, 1729.514944323272, 1105.5467856239263, 8543.124278033582, 18598.55488138467, 723.2129581674341, 8823.730394469898, 4528.2046299893045, 6246.299446683683, 1471.1947672062354, 6835.67237384826, 13863.380790893245, 10352.803272932628, 8311.438954110637, 10435.567627340348, 1002.74452473809, 533.5155125648583, 483.80492361845114, 446.99088469115003, 415.9641059550983, 402.6946930601603, 363.3779375485424, 318.5995302239181, 290.8825678879205, 282.69018203366704, 273.9651253536847, 272.5245977052456, 256.36117498623184, 240.97083909424873, 239.55414872343133, 234.5391204219354, 218.8593923839888, 214.66536826144113, 198.38393226147306, 188.9280300220176, 185.11191153022557, 180.36571157424646, 176.21362421359774, 174.1714614986332, 173.77368607187242, 171.15454208716858, 170.46011059567533, 163.95540280761372, 161.85390833845483, 159.9313907350041, 1099.654649609258, 844.2471974081649, 415.52528396439374, 309.1589891557597, 324.18330814139233, 356.8551689035274, 451.9084475237687, 867.0275368595943, 334.0403460670117, 2039.1394555335405, 465.7125508614578, 434.2122906537641, 310.6820820623584, 310.1525794844579, 299.29852095049336, 254.6817611568295, 232.13855769629268, 229.39169853376336, 216.43405270419083, 213.26586244984185, 185.62451024002527, 174.94667074936493, 156.89295775800903, 155.4687531368475, 155.30333612429544, 153.09560150968514, 147.3924050716148, 143.29482491040096, 138.23326567983025, 138.13184356311362, 135.69472107548032, 129.8873002808259, 129.80904077771183, 120.96219686016143, 120.11585335632918, 110.63927536899219, 110.32164895311688, 109.3024196949754, 104.81714362663239, 103.9559950840896, 339.35200872493425, 471.76509444661207], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.6804, -8.0216, -8.5947, -8.6529, -8.6658, -8.7515, -8.7597, -8.7768, -8.7787, -8.7857, -8.8792, -8.9131, -8.9206, -8.9272, -8.9297, -8.9318, -8.9646, -8.9685, -8.992, -8.9929, -9.003, -9.021, -9.0415, -9.0465, -9.0851, -9.0937, -9.1042, -9.1159, -9.1496, -9.1696, -7.6113, -5.624, -2.9185, -5.6188, -6.4851, -6.3173, -4.4602, -5.3719, -4.2955, -6.5765, -7.351, -7.2438, -3.8702, -6.0351, -7.4634, -4.1774, -5.0792, -4.567, -3.9788, -5.4426, -4.7567, -6.7979, -6.4286, -4.9576, -4.8325, -4.8973, -6.6235, -4.6857, -5.4875, -5.521, -6.0516, -4.5587, -4.3578, -5.3867, -5.8539, -4.1698, -4.4011, -5.2448, -4.6967, -5.272, -5.1898, -5.1742, -5.0083, -5.2821, -5.2908, -5.2237, -5.1948, -5.2804, -5.2807, -5.2751, -7.0006, -7.3375, -7.4693, -7.6849, -7.9246, -8.0139, -8.1405, -8.2642, -8.3571, -8.3615, -8.3763, -8.4153, -8.4383, -8.4588, -8.4598, -8.4808, -8.4966, -8.4978, -8.5476, -8.5677, -8.5857, -8.5868, -8.6147, -8.6192, -8.6409, -8.6457, -8.6493, -8.6515, -8.6581, -8.6784, -6.708, -6.37, -7.1935, -5.7975, -2.9063, -6.1387, -5.1321, -7.5627, -6.4896, -6.3817, -5.6307, -4.2523, -7.4631, -7.0054, -5.8582, -6.8134, -6.802, -4.3536, -6.1113, -4.6994, -6.2944, -4.282, -6.0862, -5.6347, -5.4818, -6.324, -6.3002, -5.4425, -5.2069, -5.2225, -4.4206, -5.2507, -5.0454, -4.3119, -5.0915, -4.4345, -5.9892, -4.8133, -5.045, -4.5811, -5.64, -5.8371, -5.5464, -5.3696, -5.4998, -5.3954, -5.0087, -5.4372, -4.7601, -5.5323, -5.4974, -5.5276, -5.5715, -7.0028, -7.1031, -7.2008, -7.344, -7.3529, -7.3787, -7.4333, -7.533, -7.6221, -7.6811, -7.7092, -7.724, -7.7614, -7.7778, -7.8654, -7.8882, -7.9574, -8.0312, -8.0644, -8.0762, -8.0802, -8.0813, -8.0928, -8.096, -8.0986, -8.112, -8.1471, -8.1515, -8.1713, -8.1736, -7.0946, -5.7845, -7.313, -7.3355, -5.9768, -5.9741, -6.7681, -6.216, -6.4699, -7.0064, -7.2035, -7.1021, -5.7332, -6.2155, -6.7938, -6.3741, -6.5404, -6.6608, -5.3259, -5.8824, -6.065, -6.6314, -5.3745, -6.4706, -6.0678, -6.2435, -5.7506, -6.2764, -4.9725, -6.436, -5.591, -5.9122, -5.2695, -5.7389, -5.9648, -5.6757, -5.3734, -5.1298, -5.851, -4.8603, -5.3802, -5.8569, -5.6665, -5.3019, -5.3212, -5.5241, -4.8936, -5.7743, -5.3187, -5.7223, -5.6554, -5.7232, -5.7748, -5.8145, -5.8325, -5.7778, -5.8517, -5.3268, -5.8486, -6.6701, -6.6873, -6.8661, -6.9445, -7.305, -7.4045, -7.4661, -7.4956, -7.5148, -7.6346, -7.6614, -7.6688, -7.6912, -7.8036, -7.8099, -7.8186, -7.8187, -7.8253, -7.8335, -7.8348, -7.8464, -7.8779, -7.8807, -7.8823, -7.8994, -7.9057, -7.9409, -7.9542, -5.9957, -6.6906, -7.2967, -6.7471, -6.1967, -7.3908, -4.997, -6.8174, -6.6853, -5.8464, -6.2927, -5.9822, -6.9384, -7.0125, -6.3449, -6.3201, -5.5656, -6.3485, -6.4655, -5.9121, -6.0035, -4.6315, -6.4503, -6.4314, -6.4482, -5.5534, -6.4351, -5.8637, -5.7819, -5.8421, -4.5918, -5.0639, -5.8982, -5.5042, -5.6002, -5.9041, -5.0668, -4.5813, -5.9904, -5.5734, -5.4359, -5.7807, -5.4535, -6.0191, -5.5977, -5.6074, -5.8564, -5.6436, -5.804, -5.6416, -5.8357, -5.8464, -5.8758, -5.5992, -5.6084, -5.866, -6.1882, -6.411, -7.2575, -7.2926, -7.3311, -7.3788, -7.3861, -7.4026, -7.4569, -7.4941, -7.6944, -7.7065, -7.7386, -7.7697, -7.8187, -7.8541, -7.9556, -7.9608, -7.9652, -8.0123, -8.0435, -8.0593, -8.0618, -8.0925, -8.1117, -8.1379, -8.1617, -6.8995, -6.0341, -5.0553, -4.4337, -6.4529, -5.238, -7.1191, -5.2753, -5.7291, -3.4469, -5.093, -5.1491, -6.538, -6.7985, -4.9157, -6.2405, -6.6067, -4.9003, -5.9865, -5.348, -6.1017, -5.7168, -3.0112, -5.9331, -6.0608, -4.549, -5.6184, -4.5349, -5.8427, -5.6673, -4.456, -5.5219, -4.2158, -4.7323, -4.6956, -5.2909, -5.281, -5.2203, -5.0959, -4.3502, -4.9028, -4.8871, -5.184, -5.4387, -5.2901, -5.4769, -5.5142, -6.2397, -6.4332, -6.4758, -6.6073, -6.6319, -6.7042, -6.7152, -6.7568, -6.757, -6.7689, -6.7943, -6.7972, -6.7983, -6.8083, -6.832, -6.833, -6.8431, -6.8532, -6.9807, -7.0388, -7.1579, -7.1997, -7.2502, -7.2681, -7.3273, -7.407, -7.4174, -7.4407, -7.5169, -7.521, -4.2716, -5.6798, -6.0714, -4.7715, -6.3648, -4.7179, -5.2493, -4.3423, -6.2653, -6.7369, -5.1553, -4.9051, -4.4231, -4.7549, -6.2029, -4.1341, -5.4086, -5.8235, -4.7333, -5.68, -5.5983, -4.2583, -4.7369, -5.6773, -4.4874, -6.0431, -5.339, -5.0197, -4.7399, -4.9668, -5.4739, -5.8971, -5.4425, -5.7408, -5.3833, -5.7512, -5.1731, -5.5049, -5.4442, -5.2855, -5.6158, -5.6178, -5.6528, -5.8933, -5.9321, -5.9759, -6.0372, -6.048, -6.0773, -6.1428, -6.1906, -6.2131, -6.2284, -6.3748, -6.4324, -6.4636, -6.5491, -6.5828, -6.6122, -6.6264, -6.6424, -6.6444, -6.6615, -6.6822, -6.6947, -6.7007, -6.7206, -6.7924, -6.8418, -6.8533, -6.8983, -6.9128, -6.9267, -5.67, -6.1712, -6.45, -5.056, -5.0296, -5.9097, -5.8212, -5.8162, -6.3065, -4.7181, -5.721, -4.2567, -5.8482, -6.1922, -4.7976, -5.381, -3.8523, -4.3176, -5.4255, -5.1576, -4.3973, -5.4909, -4.613, -5.4883, -5.4096, -5.6991, -4.849, -5.6575, -5.4929, -5.6636, -4.7996, -4.651, -5.7134, -5.2823, -5.3579, -5.6769, -5.5137, -5.4777, -5.543, -5.5913, -5.59, -5.6706, -3.2651, -3.512, -4.501, -4.6854, -4.7719, -5.0103, -5.2461, -5.3092, -5.3256, -5.6787, -6.1645, -6.2698, -6.2986, -6.3275, -6.3535, -6.4237, -6.4686, -6.5163, -6.5603, -6.5865, -6.5904, -6.5904, -6.6171, -6.6548, -6.6919, -6.6989, -6.7174, -6.7267, -6.7421, -6.7479, -5.6104, -4.8687, -5.1381, -5.7126, -3.0175, -5.1613, -5.3813, -5.092, -5.2013, -6.3093, -6.1949, -5.5027, -5.1949, -4.7754, -5.516, -5.7515, -4.7517, -4.5731, -5.9975, -5.048, -5.3184, -5.4391, -5.8145, -5.4405, -5.2847, -5.4503, -5.5301, -5.5655, -3.6573, -4.2891, -4.3871, -4.4664, -4.5385, -4.571, -4.674, -4.8059, -4.8972, -4.9258, -4.9573, -4.9626, -5.0239, -5.0861, -5.092, -5.1133, -5.1827, -5.2022, -5.2814, -5.3305, -5.351, -5.3771, -5.4005, -5.4122, -5.4145, -5.4298, -5.4339, -5.473, -5.486, -5.498, -3.9287, -4.3787, -4.9941, -5.1469, -5.189, -5.1643, -5.093, -4.9929, -5.2684, -5.1767, -3.8646, -3.9348, -4.2704, -4.2721, -4.3078, -4.4698, -4.5629, -4.5748, -4.6332, -4.648, -4.7875, -4.847, -4.9566, -4.9658, -4.9668, -4.9812, -5.0194, -5.0478, -5.084, -5.0848, -5.1027, -5.1467, -5.1473, -5.2185, -5.2255, -5.3084, -5.3113, -5.3207, -5.3629, -5.3713, -4.2857, -4.9212], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2486, 1.2481, 1.2468, 1.2466, 1.2465, 1.2462, 1.2462, 1.2462, 1.2461, 1.2461, 1.2458, 1.2456, 1.2456, 1.2456, 1.2456, 1.2455, 1.2454, 1.2454, 1.2453, 1.2453, 1.2452, 1.2451, 1.2451, 1.245, 1.2448, 1.2448, 1.2447, 1.2447, 1.2445, 1.2444, 1.2327, 1.1969, 1.076, 1.1592, 1.1705, 1.1576, 1.0369, 1.0828, 1.0197, 1.1062, 1.1512, 1.1358, 0.888, 1.045, 1.1461, 0.8861, 0.9533, 0.9072, 0.854, 0.9742, 0.9047, 1.0649, 1.0164, 0.8548, 0.8353, 0.8144, 0.998, 0.7422, 0.8217, 0.8243, 0.9124, 0.6315, 0.5912, 0.7481, 0.8273, 0.4309, 0.4068, 0.6273, 0.365, 0.5844, 0.5468, 0.5235, 0.4116, 0.476, 0.4682, 0.3124, 0.1833, 0.3852, 0.3675, -0.0085, 1.6098, 1.6094, 1.6092, 1.6089, 1.6084, 1.6082, 1.6079, 1.6075, 1.6072, 1.6072, 1.6072, 1.607, 1.6069, 1.6069, 1.6069, 1.6068, 1.6067, 1.6067, 1.6065, 1.6064, 1.6063, 1.6063, 1.6062, 1.6062, 1.6061, 1.6061, 1.6061, 1.6061, 1.606, 1.6059, 1.5683, 1.5335, 1.546, 1.4148, 1.1448, 1.3902, 1.2952, 1.5042, 1.3917, 1.3643, 1.2806, 1.1217, 1.4807, 1.4107, 1.2396, 1.3662, 1.3465, 0.913, 1.2152, 0.9214, 1.2011, 0.7797, 1.1258, 1.0146, 0.963, 1.1723, 1.1556, 0.882, 0.7955, 0.7997, 0.5284, 0.774, 0.6201, 0.2887, 0.6362, 0.3238, 1.0152, 0.3769, 0.506, 0.2268, 0.823, 0.909, 0.6778, 0.4037, 0.4787, 0.3412, -0.1759, 0.3555, -0.7656, 0.2257, -0.0231, -0.2125, -0.1436, 2.0005, 2.0004, 2.0002, 2.0, 2.0, 1.9999, 1.9998, 1.9996, 1.9994, 1.9993, 1.9992, 1.9991, 1.999, 1.999, 1.9987, 1.9987, 1.9984, 1.9982, 1.9981, 1.998, 1.998, 1.998, 1.9979, 1.9979, 1.9979, 1.9979, 1.9977, 1.9977, 1.9976, 1.9976, 1.9819, 1.9122, 1.9623, 1.9174, 1.763, 1.7483, 1.8448, 1.7647, 1.7759, 1.851, 1.8652, 1.8477, 1.6034, 1.6774, 1.7536, 1.6467, 1.6752, 1.7063, 1.2162, 1.3838, 1.4375, 1.6339, 1.1676, 1.5616, 1.3867, 1.4563, 1.2172, 1.4295, 0.7392, 1.5114, 1.0159, 1.1297, 0.6993, 0.9671, 1.1092, 0.8684, 0.5821, 0.2981, 0.9297, -0.0524, 0.3779, 0.889, 0.648, 0.118, -0.0546, 0.2485, -0.8425, 0.5998, -0.7181, 0.2801, 0.0124, -0.187, -0.1135, 0.0361, -0.0201, -0.945, 0.0204, 2.0137, 2.0135, 2.013, 2.013, 2.0128, 2.0127, 2.0122, 2.012, 2.0119, 2.0118, 2.0118, 2.0115, 2.0114, 2.0114, 2.0113, 2.011, 2.011, 2.011, 2.011, 2.011, 2.0109, 2.0109, 2.0109, 2.0108, 2.0108, 2.0108, 2.0107, 2.0107, 2.0106, 2.0105, 1.9323, 1.9208, 1.9575, 1.898, 1.8299, 1.9612, 1.674, 1.8929, 1.8729, 1.706, 1.7768, 1.7173, 1.8715, 1.8776, 1.7578, 1.7368, 1.5698, 1.7309, 1.7376, 1.6005, 1.609, 1.2191, 1.7198, 1.7143, 1.701, 1.3829, 1.6865, 1.4678, 1.3102, 1.3056, 0.4717, 0.7087, 1.2845, 0.938, 1.0048, 1.2542, 0.4842, 0.0193, 1.2482, 0.7491, 0.5852, 0.8188, 0.2442, 1.1778, 0.2502, 0.1203, 0.7539, 0.0047, 0.0524, -1.5905, 0.2706, -0.0537, 0.541, 2.3526, 2.3526, 2.3525, 2.3522, 2.352, 2.3507, 2.3506, 2.3505, 2.3504, 2.3503, 2.3503, 2.3501, 2.35, 2.3494, 2.3493, 2.3492, 2.3491, 2.3489, 2.3487, 2.3483, 2.3482, 2.3482, 2.348, 2.3478, 2.3477, 2.3477, 2.3475, 2.3474, 2.3473, 2.3472, 2.3307, 2.2921, 2.2388, 2.1681, 2.2628, 2.171, 2.2902, 2.1644, 2.1927, 1.9311, 2.0751, 2.0612, 2.2045, 2.2102, 1.912, 2.1234, 2.1708, 1.8559, 2.0548, 1.895, 2.0465, 1.9138, 1.0399, 1.9288, 1.9635, 1.4746, 1.8197, 1.4436, 1.8461, 1.7453, 0.9639, 1.6274, 0.5921, 0.9159, 0.8554, 1.3589, 1.2029, 1.1042, 0.9263, -0.3557, -0.0701, -0.2865, -0.2349, 0.6961, -0.2285, -0.0026, 0.3579, 2.7796, 2.7793, 2.7792, 2.779, 2.779, 2.7788, 2.7788, 2.7787, 2.7787, 2.7787, 2.7786, 2.7786, 2.7786, 2.7786, 2.7785, 2.7785, 2.7785, 2.7785, 2.7781, 2.778, 2.7776, 2.7775, 2.7773, 2.7772, 2.777, 2.7767, 2.7766, 2.7765, 2.7762, 2.7761, 2.6915, 2.7154, 2.706, 2.6201, 2.703, 2.5012, 2.5498, 2.4327, 2.6548, 2.7068, 2.39, 2.2296, 2.088, 2.1737, 2.5633, 1.8214, 2.2387, 2.3919, 1.9161, 2.3124, 2.2572, 1.5008, 1.7053, 2.1244, 1.3632, 2.3141, 1.7825, 1.5223, 1.2905, 1.3238, 1.774, 2.1948, 1.6344, 1.9303, 1.3599, 1.819, 0.201, 0.5161, 0.092, -1.2344, 0.9283, 0.9487, 0.9158, 3.0437, 3.0437, 3.0436, 3.0435, 3.0435, 3.0435, 3.0434, 3.0433, 3.0433, 3.0432, 3.043, 3.0429, 3.0428, 3.0426, 3.0425, 3.0424, 3.0424, 3.0424, 3.0424, 3.0423, 3.0423, 3.0422, 3.0422, 3.0422, 3.0419, 3.0418, 3.0418, 3.0416, 3.0416, 3.0415, 2.9579, 2.9909, 3.0098, 2.8553, 2.8359, 2.9002, 2.8736, 2.85, 2.9541, 2.5725, 2.8076, 2.3656, 2.8011, 2.9005, 2.4086, 2.5662, 1.9405, 2.1272, 2.5794, 2.4635, 2.085, 2.557, 1.85, 2.3338, 2.2155, 2.5108, 1.5784, 2.3911, 2.1485, 2.3674, 0.8212, 0.298, 2.415, 1.1723, 1.3883, 2.0634, 0.9716, 0.5445, 0.1847, 0.4392, -0.5283, 1.2636, 3.51, 3.5099, 3.5096, 3.5095, 3.5095, 3.5093, 3.5091, 3.509, 3.509, 3.5085, 3.5075, 3.5073, 3.5072, 3.5071, 3.507, 3.5068, 3.5066, 3.5065, 3.5063, 3.5062, 3.5062, 3.5062, 3.5061, 3.5059, 3.5058, 3.5057, 3.5057, 3.5056, 3.5056, 3.5055, 3.3485, 3.1436, 3.1734, 3.2615, 2.5187, 3.0859, 3.1159, 2.994, 2.8311, 3.3553, 3.2785, 2.7645, 2.3602, 1.9797, 2.5959, 2.8078, 1.7629, 1.1635, 2.9863, 1.4343, 1.831, 1.3886, 2.4591, 1.297, 0.7458, 0.8722, 1.012, 0.749, 4.9996, 4.9988, 4.9986, 4.9985, 4.9983, 4.9983, 4.998, 4.9976, 4.9974, 4.9973, 4.9972, 4.9971, 4.9969, 4.9967, 4.9967, 4.9966, 4.9963, 4.9962, 4.9959, 4.9956, 4.9955, 4.9954, 4.9953, 4.9952, 4.9952, 4.9951, 4.9951, 4.9949, 4.9948, 4.9947, 4.636, 4.4503, 4.5439, 4.6868, 4.5971, 4.5258, 4.361, 3.8094, 4.4878, 2.7705, 5.5593, 5.5592, 5.5583, 5.5583, 5.5582, 5.5576, 5.5573, 5.5572, 5.557, 5.5569, 5.5562, 5.5559, 5.5553, 5.5553, 5.5553, 5.5552, 5.5549, 5.5547, 5.5545, 5.5545, 5.5544, 5.5541, 5.5541, 5.5535, 5.5535, 5.5528, 5.5528, 5.5527, 5.5523, 5.5522, 5.4547, 4.4898]}, \"token.table\": {\"Topic\": [8, 10, 5, 1, 2, 5, 1, 2, 5, 7, 2, 3, 2, 4, 6, 8, 6, 6, 4, 7, 1, 2, 3, 4, 6, 8, 2, 7, 5, 8, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 3, 3, 5, 8, 1, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 6, 1, 3, 4, 6, 8, 8, 8, 8, 1, 3, 9, 1, 5, 8, 1, 4, 1, 2, 3, 4, 5, 6, 10, 10, 7, 3, 6, 5, 3, 4, 5, 8, 2, 3, 4, 5, 1, 2, 3, 5, 1, 3, 4, 5, 6, 1, 5, 1, 2, 3, 4, 6, 8, 1, 5, 1, 3, 4, 5, 6, 1, 3, 7, 1, 8, 9, 1, 2, 4, 6, 7, 2, 6, 1, 2, 3, 4, 6, 7, 8, 3, 5, 10, 5, 1, 7, 2, 6, 1, 2, 3, 4, 5, 7, 1, 2, 4, 6, 10, 7, 9, 5, 2, 5, 4, 6, 7, 9, 1, 2, 1, 2, 5, 1, 4, 6, 7, 8, 1, 1, 2, 7, 8, 2, 2, 4, 6, 7, 8, 7, 7, 10, 6, 2, 4, 5, 8, 3, 1, 2, 8, 5, 1, 3, 1, 1, 4, 2, 3, 4, 5, 6, 7, 7, 4, 9, 8, 8, 10, 1, 2, 5, 7, 8, 3, 6, 7, 1, 2, 3, 5, 8, 4, 7, 1, 4, 2, 1, 4, 5, 4, 8, 1, 6, 8, 6, 3, 3, 2, 5, 6, 2, 5, 6, 4, 1, 4, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 7, 2, 4, 7, 9, 2, 4, 1, 4, 1, 2, 3, 5, 1, 4, 6, 4, 2, 3, 4, 3, 4, 8, 1, 6, 3, 1, 3, 8, 1, 6, 7, 7, 1, 4, 6, 7, 1, 2, 3, 4, 6, 7, 8, 1, 4, 5, 1, 3, 4, 5, 6, 1, 2, 4, 5, 7, 9, 10, 7, 9, 9, 4, 2, 5, 3, 10, 8, 2, 3, 4, 5, 7, 5, 8, 8, 1, 4, 8, 3, 6, 9, 5, 1, 2, 3, 6, 10, 5, 1, 1, 2, 5, 1, 3, 5, 6, 8, 4, 6, 3, 7, 3, 2, 3, 5, 8, 10, 2, 1, 2, 3, 4, 5, 2, 8, 8, 8, 1, 3, 4, 6, 8, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 5, 6, 8, 3, 1, 2, 6, 7, 8, 1, 2, 3, 5, 6, 6, 1, 6, 1, 2, 5, 7, 8, 1, 4, 6, 7, 4, 1, 2, 3, 1, 4, 5, 6, 8, 4, 2, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 4, 6, 7, 1, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 2, 1, 4, 5, 3, 3, 4, 4, 9, 4, 6, 4, 4, 9, 3, 1, 4, 6, 7, 10, 4, 1, 2, 4, 5, 7, 8, 1, 7, 8, 1, 3, 1, 4, 7, 8, 4, 7, 1, 3, 6, 1, 7, 8, 9, 4, 7, 8, 4, 6, 6, 6, 1, 2, 3, 4, 5, 7, 1, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 4, 5, 7, 8, 5, 1, 2, 7, 8, 6, 1, 4, 1, 2, 4, 5, 1, 4, 5, 2, 1, 4, 5, 4, 5, 2, 6, 1, 2, 3, 4, 5, 10, 1, 2, 3, 6, 1, 3, 4, 5, 1, 2, 4, 6, 2, 3, 4, 5, 6, 8, 1, 3, 1, 2, 3, 5, 6, 8, 6, 10, 6, 1, 4, 6, 7, 2, 1, 9, 5, 1, 5, 4, 5, 1, 4, 3, 7, 1, 2, 3, 5, 6, 8, 1, 4, 6, 7, 7, 10, 10, 3, 6, 1, 4, 6, 9, 1, 2, 3, 2, 3, 6, 1, 4, 7, 8, 2, 4, 7, 8, 9, 9, 8, 6, 7, 7, 7, 9, 2, 4, 6, 7, 1, 5, 7, 8, 9, 2, 4, 5, 8, 9, 1, 6, 7, 1, 9, 8, 7, 9, 9, 1, 8, 4, 1, 4, 5, 4, 5, 9, 10, 1, 8, 1, 2, 3, 4, 6, 4, 10, 1, 2, 4, 6, 7, 8, 1, 5, 7, 5, 4, 7, 9, 10, 2, 10, 1, 2, 3, 6, 1, 2, 3, 4, 5, 8, 6, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 6, 8, 10, 7, 1, 1, 2, 4, 5, 7, 8, 1, 2, 4, 5, 9, 1, 2, 6, 7, 1, 2, 3, 4, 5, 7, 4, 10, 1, 1, 2, 3, 4, 5, 6, 7, 1, 3, 4, 6, 7, 6, 1, 2, 3, 5, 6, 8, 3, 4, 9, 6, 7, 1, 4, 6, 7, 9, 6, 7, 2, 1, 2, 5, 6, 2, 4, 7, 7, 1, 2, 3, 5, 6, 1, 2, 3, 5, 2, 3, 9, 1, 2, 3, 6, 7, 2, 4, 5, 7, 1, 2, 3, 4, 5, 2, 10, 3, 9, 1, 1, 6, 1, 2, 5, 5, 1, 2, 3, 4, 5, 3, 7, 4, 4, 7, 1, 2, 5, 7, 2, 7, 5, 5, 7, 6, 9, 1, 2, 3, 5, 6, 1, 8, 1, 1, 2, 3, 4, 6, 7, 8, 5, 9, 1, 1, 2, 3, 5, 10, 2, 6, 9, 1, 2, 3, 4, 5, 7, 5, 3, 4, 1, 2, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 5, 7, 8, 1, 2, 5, 6, 7, 2, 6, 2, 2, 9, 2, 3, 1, 2, 3, 5, 6, 2, 5, 7, 6, 3, 4, 8, 10, 2, 8, 1, 2, 4, 6, 7, 1, 2, 5, 1, 2, 4, 5, 6, 6, 1, 2, 3, 4, 5, 3, 5, 4, 1, 2, 3, 4, 5, 6, 7, 2, 3, 5, 3, 5, 3, 4, 1, 2, 3, 6, 8, 1, 4, 1, 2, 3, 5, 6, 1, 1, 3, 8, 3, 7, 1, 3, 1, 2, 3, 4, 5, 7, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 6, 8, 1, 2, 5, 2, 3, 5, 3, 6, 3, 6, 4, 7, 8, 10, 1, 4, 1, 3, 5, 6, 7, 5, 5, 5, 8, 1, 4, 7, 1, 2, 4, 7, 1, 3, 4, 5, 8, 7, 6, 4, 6, 4, 1, 2, 3, 5, 3, 2, 1, 2, 4, 5, 5, 8, 3, 5, 8, 5, 2, 5, 7, 4, 9, 8, 1, 2, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 7, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 6, 2, 5, 4, 4, 8, 1, 2, 3, 4, 5, 10, 1, 2, 3, 6, 7, 8, 1, 9, 9, 1, 7, 7, 7, 4, 1, 4, 6, 7, 5, 3, 6, 4, 6, 1, 2, 3, 5, 1, 4, 6, 1, 2, 7, 6, 3, 5, 8, 9, 1, 2, 5, 6, 8, 8, 2, 3, 5, 7, 4, 10, 1, 2, 4, 5, 7, 8, 1, 3, 4, 6, 2, 7, 5, 7, 1, 2, 4, 5, 6, 8, 2, 1, 2, 3, 4, 5, 8, 2, 3, 6, 9, 2, 7, 2, 8, 2, 3, 6, 7, 1, 2, 3, 5, 1, 3, 4, 1, 2, 3, 4, 5, 6, 2, 5, 7, 6, 10, 1, 3, 1, 2, 7, 8, 1, 5, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 5, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 7, 9, 1, 2, 4, 7, 1, 4, 6, 7, 1, 4, 5, 6, 7, 5, 1, 8, 10, 1, 2, 2, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 5, 1, 2, 8, 10, 1, 2, 3, 4, 5, 8, 5, 5, 3, 5, 2, 1, 2, 5, 7, 8, 3, 4, 6, 1, 2, 3, 6, 1, 2, 3, 5, 8, 2, 3, 5, 8, 2, 1, 2, 3, 5, 7, 8, 9, 8, 1, 2, 4, 4, 4, 1, 2, 3, 4, 5, 6, 6, 8, 1, 5, 1, 2, 5, 4, 1, 2, 3, 4, 5, 1, 2, 4, 5, 7, 8, 3, 1, 3, 5, 7, 1, 4, 6, 7, 7, 3, 1, 2, 3, 4, 5, 6, 3, 1, 3, 4, 6, 1, 2, 6, 7, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 6, 8, 1, 2, 3, 5, 6, 1, 5, 1, 2, 4, 6, 7, 8, 1, 2, 4, 6, 7, 5], \"Freq\": [0.9971940755821018, 0.9918060634396987, 0.9938588779708776, 0.27481526999816147, 0.32240974112916626, 0.4027382513977542, 0.23341990740111734, 0.44444924446311435, 0.24007269667120743, 0.08205106766444425, 0.9994766015837468, 0.9966078908481575, 0.33767982232699184, 0.11751257816979316, 0.07023740304401431, 0.47477783019175057, 0.9977194444185765, 0.9976826946435862, 0.8909381929893984, 0.108410620828356, 0.2503592128077419, 0.021393281463940997, 0.04509935011317291, 0.17519362928578708, 0.10638821052338225, 0.4006903798516517, 0.994997499412995, 0.9962683466112101, 0.2852189919638626, 0.7144594551173983, 0.9979966760843024, 0.07602726356150663, 0.21925281814189332, 0.612142096288776, 0.07259377423937408, 0.008338474068036212, 0.011281464915578404, 0.31770507195033104, 0.37738801065955124, 0.026004709009017368, 0.15053955522638127, 0.039220216866058984, 0.057231675155091505, 0.03181313786553969, 0.21241880151757522, 0.551295558614858, 0.062082833177348876, 0.07583656852740771, 0.05635211011482436, 0.003056385633346406, 0.03896891682516668, 0.9962040158602701, 0.05925052923025277, 0.9395441063654368, 0.9937614370800908, 0.09619189067609946, 0.18212331301341497, 0.0032063963558699817, 0.6778321896309142, 0.04104187335513577, 0.0905237773820978, 0.07833788427296925, 0.12316456249583499, 0.03111754847509612, 0.08073154184797664, 0.5762186598745072, 0.020019681536425474, 0.9985783723428425, 0.3718066514325226, 0.0679719655269694, 0.1699299138174235, 0.040103459660911944, 0.3493759028086227, 0.9993176997004357, 0.9995438746970186, 0.9965272235211996, 0.2552437135541622, 0.7440876678757302, 0.9939933010197238, 0.9967846245609385, 0.2053074075987701, 0.7941561260963415, 0.9969357011730852, 0.998538117224357, 0.4735474075586839, 0.0688147470618109, 0.13240467073559542, 0.3026574523551869, 0.004460215087339596, 0.018095729782920646, 0.9880842556037643, 0.9920454746595436, 0.9979527419810564, 0.8715822179002553, 0.126447161146151, 0.9965202099805031, 0.08522919816861621, 0.731050446263201, 0.13984790967103927, 0.042614599084308105, 0.01464405043344738, 0.5390996193465714, 0.10101912756632345, 0.34500390004223486, 0.1322342634179178, 0.36834056662372644, 0.4563739620467971, 0.043095846294975995, 0.9944180465200028, 0.16002273871307743, 0.033944217302774, 0.7419236067606317, 0.06357805780519575, 0.16606341471405095, 0.8334665521251764, 0.5226603710462695, 0.04308011929156093, 0.017483187425498942, 0.2822423958967288, 0.06297811161008458, 0.07147822483353158, 0.34420484175886695, 0.6557660070484056, 0.06993203376347623, 0.012647282701905276, 0.05207704641960996, 0.7945469368020491, 0.06993203376347623, 0.1491136948553557, 0.06139975670514646, 0.788548303970381, 0.9944212800600057, 0.9983329018829791, 0.9973007726319814, 0.08698779737063185, 0.6157325440910401, 0.027036747831412602, 0.24826787573888442, 0.021864500420185843, 0.9379072196665096, 0.061732923234400744, 0.12646849839580776, 0.17396953592483372, 0.32189789391497303, 0.15431809146485598, 0.15684987265295128, 0.04400476826927534, 0.022424347665986884, 0.9193082978600658, 0.07962512816110807, 0.9956614521636472, 0.9916100024300981, 0.08233843896884128, 0.9163783913473394, 0.9960092290080689, 0.9975308482923267, 0.5060660885836079, 0.16868869619453597, 0.009775760022055319, 0.13549658077081325, 0.05410769500579456, 0.12572082074875793, 0.2653771187858401, 0.09514360178132134, 0.4676965342833571, 0.1713476107021923, 0.9962838307313977, 0.26523569708881073, 0.7310154578301369, 0.9977643674706372, 0.9975003982562546, 0.9943301572022364, 0.03553461603674832, 0.26532513307438743, 0.12200218172616924, 0.5768452669965477, 0.2809061343537373, 0.7187994639906661, 0.3598059214981214, 0.03749556445085686, 0.6022014896652769, 0.49620931964560144, 0.14787694956325872, 0.31773005690883505, 0.03799616065167064, 0.9978014614184779, 0.9975532705987548, 0.12272594207015801, 0.7816760166494705, 0.06737894858753773, 0.028074561911474054, 0.9973323206951357, 0.3083280788730575, 0.03173965517810886, 0.556577524730409, 0.05837829434545023, 0.044775584983403574, 0.9980369458253231, 0.9984322983815799, 0.9937674543093092, 0.9957095616286118, 0.10617456640313178, 0.0017865912615911597, 0.8920194941801576, 0.9968061814822776, 0.9968540475157399, 0.9235761527081243, 0.06191367306928719, 0.014287770708297044, 0.998449111936717, 0.9977789609145576, 0.9980984254413124, 0.9971672945977168, 0.33821864525810796, 0.6615112094578769, 0.20685107809841624, 0.0770282257353935, 0.07313354016450281, 0.125495423950922, 0.026397313313814626, 0.4911631247734361, 0.9955210851609613, 0.3755024774104525, 0.6221011192919437, 0.9950035829213448, 0.9994203553626518, 0.9907101908270264, 0.05458379355923045, 0.45490200253436053, 0.17853680744628128, 0.30271215857797257, 0.009226995652578672, 0.996935774799805, 0.03338747487854032, 0.9660109398190998, 0.2841565206641862, 0.4977779731076856, 0.1278973176026363, 0.07224887883491016, 0.017944605254810245, 0.1336470086652856, 0.8658001865707632, 0.25433260068761077, 0.74522516249671, 0.9979975816082213, 0.10088344877594455, 0.04685581981062827, 0.8520109275769344, 0.9492180533401687, 0.04995884491264046, 0.36181524508470725, 0.5449366172910642, 0.0931545858612609, 0.9973137035255706, 0.9971291237548764, 0.9990693327594674, 0.8021464770488267, 0.16430283976573554, 0.03357071775174164, 0.8035410982273132, 0.19606402796746442, 0.996523917679679, 0.9965125532195793, 0.9984337752301389, 0.995161427085934, 0.9983526081125593, 0.5140863970230227, 0.1722510658263889, 0.09382288877524632, 0.14061311496031748, 0.0015152275318999727, 0.07036716658143473, 0.007333701254395868, 0.45408756206858186, 0.03898138991056732, 0.5068884413788821, 0.8985434144612697, 0.09917034340207995, 0.998034900778529, 0.994690401203283, 0.995753300501988, 0.997551243760998, 0.07793969340137723, 0.9218052626977702, 0.5366186690997847, 0.18950347217635813, 0.13784869207370562, 0.1360016796743259, 0.06555381486976006, 0.14041204217264736, 0.7938355516809008, 0.9969905000126588, 0.13825895925515905, 0.1080323041953267, 0.753427365009895, 0.11361549782747171, 0.8857800849141776, 0.9971776621846612, 0.9939535249177323, 0.9964671500603768, 0.9979425296942328, 0.901647569685587, 0.063724037098032, 0.03423978112730078, 0.14997251239319367, 0.02223567218447982, 0.8274508649075576, 0.9990532953270618, 0.12572731765125164, 0.29287924285233763, 0.5813980006994295, 0.998878522047521, 0.44649127378371156, 0.11464966853106927, 0.1150203032353723, 0.10538380092349363, 0.15492530639866472, 0.04447616451636308, 0.019149459722322994, 0.21964575648817367, 0.5791865183150152, 0.20083411751270885, 0.0813546282242807, 0.17659907102343858, 0.49253723429008966, 0.08598456641590643, 0.1635911494374425, 0.15671116089129336, 0.1031231489773166, 0.06259440047094764, 0.6543141286639346, 0.02296629082027575, 0.9955477374661177, 0.9942219852140196, 0.36580204831296104, 0.632933807278347, 0.9959711345244184, 0.9987219155003135, 0.45514560838278534, 0.5445081615437994, 0.9971422715783451, 0.9978045658190822, 0.9974769379836885, 0.4825819984677309, 0.1828314069605984, 0.029907307967615093, 0.2867810793140567, 0.017808882414372743, 0.9974595714029701, 0.9988238318931474, 0.9933691245834426, 0.0494071266686894, 0.7588427916549987, 0.19129425966595126, 0.9143154309592001, 0.0855142616011945, 0.9969004396618332, 0.9965034684430665, 0.11020004078884077, 0.6640098775010321, 0.06992863778102078, 0.1554663464953051, 0.9916078034328102, 0.9928873950272313, 0.99714901518521, 0.1211192588060446, 0.14270486928632978, 0.7357095572030531, 0.15432655991354344, 0.24403888539961713, 0.05927421505329869, 0.03471012593211184, 0.50730184054625, 0.07009010259561511, 0.9286938593919003, 0.9961925607336908, 0.997621195123362, 0.99514031309227, 0.20844341695646124, 0.02817668309088877, 0.6434209621720566, 0.11991099792656641, 0.9945888038605709, 0.9980339846962274, 0.4837237543546213, 0.22133180122276877, 0.08636077290701491, 0.1703944591189306, 0.03816422179962695, 0.9963236902801313, 0.9959596680994973, 0.9999006741079813, 0.9975608978566335, 0.02176747502077715, 0.07645161958516852, 0.8319422770136047, 0.048313176265627325, 0.021236560995880144, 0.9982076580701472, 0.4325120336861978, 0.14725230818099969, 0.15199227340842744, 0.018920687643368673, 0.2492986670028949, 0.04863052982243758, 0.18380042767535462, 0.579354264735024, 0.11487526729709664, 0.07313725351248486, 0.6055249789537175, 0.18959184849430158, 0.19071274624722398, 0.0037630138848108847, 0.01040833627713649, 0.9945764732827583, 0.039977022844213464, 0.8221157168433546, 0.015990809137685386, 0.08324627109912687, 0.03856606909677064, 0.28535925722589256, 0.18831400375635823, 0.3730337388656967, 0.13093407214143518, 0.022207445569086555, 0.99682180442467, 0.2938773488122118, 0.7059131355447857, 0.3660210529650437, 0.2582042999533582, 0.19237317531935752, 0.07402344236623186, 0.10942595828051667, 0.035279858417716095, 0.17947740054113287, 0.7560307914615267, 0.02912364151261127, 0.9987797134504255, 0.48059901455728626, 0.3933924890519015, 0.1259066101172122, 0.16900133662530473, 0.4649951062004813, 0.21994316809378942, 0.11781807467592671, 0.028247366264515218, 0.9954708990029555, 0.9956970725262031, 0.6276155688369868, 0.05817031734286313, 0.02720660722049278, 0.26896671145563356, 0.01803808024508862, 0.46149375476983284, 0.2207405340890215, 0.09561616927188489, 0.17139994642577983, 0.050662747137774335, 0.07685346922693949, 0.6451317640391465, 0.07341693198508448, 0.039988796996131115, 0.1643289626559763, 0.09960048890136647, 0.03119355971086752, 0.7579487754307282, 0.03994964664725138, 0.03721336947963143, 0.033929836878487475, 0.49509222940844216, 0.280989573293714, 0.016452891203191062, 0.090598436854173, 0.010807291280527462, 0.010377150334038807, 0.09570636059372578, 0.9952385108398821, 0.3015460225634797, 0.11171646388226665, 0.5865851269807669, 0.996305380748174, 0.9983141923250406, 0.9990755585609548, 0.3300601767976654, 0.6693743772438635, 0.0736965971337695, 0.9249674946381273, 0.997129032423396, 0.9973725349553576, 0.9975585213865515, 0.9990945008948582, 0.638031658458673, 0.09150465565167043, 0.24141340823857102, 0.028981378076172147, 0.9931686910197725, 0.9987323432854467, 0.641438342701494, 0.03650606892110113, 0.009616232788972981, 0.07639562715684091, 0.16846215219200816, 0.06749170790779185, 0.8461534985578082, 0.15368572990092308, 0.9942663086178448, 0.4309875720004036, 0.5686772558342306, 0.05059334001838675, 0.8682178528155298, 0.062338222522655105, 0.0180690500065667, 0.051235597630695594, 0.9470321755609218, 0.23863704824659734, 0.6923170862973319, 0.06875982746088398, 0.9935391269239227, 0.37618239033041895, 0.0951520163776942, 0.5266553464625865, 0.33664029673330914, 0.6275640099596257, 0.03480694426100573, 0.06293957322514104, 0.9364180406667327, 0.9961195191418868, 0.9988284601167574, 0.6950971011816496, 0.04725771634518337, 0.014810891947997111, 0.21390767833783236, 0.016703394808018964, 0.012232699645938355, 0.6232154166576767, 0.18234136522981284, 0.16002643141872688, 0.034397052670126965, 0.4201466566357586, 0.2991167555472685, 0.10524581721381608, 0.052595021532924516, 0.07947816085304076, 0.01751308246165249, 0.025879204656709412, 0.9930239700075658, 0.5175840684404962, 0.3388267729696481, 0.0003424469261893642, 0.07516710029856544, 0.06411095668159454, 0.003938139651177688, 0.9993811163779172, 0.364026574980227, 0.5019371953357984, 0.10822152847346922, 0.02581035568460173, 0.9968906316776496, 0.05255440683008612, 0.9459793229415503, 0.7918846868185964, 0.042146786180000256, 0.05965847902943699, 0.10625739051014149, 0.13412403666474576, 0.7886715048509637, 0.07648395479229304, 0.994977805914603, 0.7590774304146416, 0.22928363161734386, 0.011570331410319667, 0.021229954833385484, 0.9783470852385144, 0.995697374012212, 0.9954990452792665, 0.6554630110063299, 0.029869200501554272, 0.2009256867072146, 0.11104151853124111, 0.002765666713106877, 0.997323086059509, 0.8661491360923894, 0.022208952207497162, 0.06210978159723783, 0.049311402359019124, 0.3598290339575987, 0.13744738479852436, 0.48220743636957386, 0.020548612345294343, 0.2475837562721649, 0.3363909731958762, 0.03363909731958762, 0.3821401455505154, 0.11651265864988537, 0.07439964949932439, 0.6671402532934702, 0.05544879538157195, 0.024916863747415245, 0.06141480501123476, 0.9835166292966893, 0.01563045987879824, 0.04725953131399334, 0.206527873065089, 0.21955215334847297, 0.0413055746130178, 0.42719639329499487, 0.05767895554070053, 0.9972839443021558, 0.9880200398955373, 0.9968903852334854, 0.36119044463363464, 0.2634594268886363, 0.36533365486222563, 0.009992448198366411, 0.9965954949574517, 0.9947225787611533, 0.9964771963130755, 0.9968418455684039, 0.35518774835459155, 0.644626456772317, 0.16883860629941183, 0.8311661496191923, 0.35843065544403435, 0.6412990767209484, 0.1325898722362879, 0.8649176549367152, 0.022104128879420853, 0.033821980333571665, 0.6713796253616864, 0.019440980821659303, 0.11691219973573197, 0.13661949536316745, 0.06174793695590032, 0.28860448794605587, 0.5924222357218263, 0.05682599995216914, 0.9963756590256521, 0.9910783726784934, 0.9948802645381181, 0.9957031539058668, 0.9987246650194515, 0.06896710000570899, 0.07786608065160693, 0.8042453758730258, 0.047832020971701396, 0.09733762659210601, 0.04771452283926765, 0.8550442492796764, 0.07055386780939829, 0.7215736480506643, 0.20717181184032407, 0.06506633096852492, 0.3919979775562772, 0.507730713787178, 0.035199818392808564, 0.13584161654481988, 0.07503165101573084, 0.6193789230906408, 0.06179077142471951, 0.1073982455715363, 0.999257513035751, 0.996284857377402, 0.9983322027904469, 0.9987600390399822, 0.997912544162895, 0.3046410981111534, 0.6947635789758841, 0.17857448250912875, 0.2613171092214738, 0.030853182841891355, 0.5291788329851669, 0.9118775861857539, 0.028744632160114306, 0.029662014037564764, 0.02935622007841461, 0.9969658962572808, 0.2964150376709643, 0.16608469036816675, 0.09803610195343176, 0.1349438109241355, 0.3033352331029712, 0.9968779105308336, 0.08571609367632475, 0.9136970836561427, 0.9968063036763009, 0.9982748889614581, 0.9971646725593039, 0.9980178006898879, 0.9990337440004696, 0.9980750445660213, 0.9484585121843307, 0.05135827316605848, 0.9974752513217822, 0.19039125382733676, 0.7117430049620066, 0.09772778952747552, 0.734935854559332, 0.26469587207824863, 0.0972441569566442, 0.8987717536901965, 0.9974638720356119, 0.9965587375749907, 0.6606753633508129, 0.13079996937569083, 0.13677279990359076, 0.05420720142968001, 0.017516956758294743, 0.6549869916986162, 0.34339123836626484, 0.16025590355344257, 0.1076864091796977, 0.43329305715099864, 0.14705563404109254, 0.09332822128907134, 0.058127502589471226, 0.9133646574833172, 0.02543894226145997, 0.06108392722661945, 0.9990299337709578, 0.17063807013469343, 0.23051107719949812, 0.5987300706480471, 0.9940643925131732, 0.9953021292150103, 0.9933741817136751, 0.2611850480367614, 0.11396314035526638, 0.2417785238423825, 0.3830446361676716, 0.6731221123229153, 0.16754016343855171, 0.05250056330261869, 0.012934605807447324, 0.08864777818538394, 0.005247878787196642, 0.14203412772293714, 0.8576676174038897, 0.4648380221058635, 0.31077386345678254, 0.04225092835679342, 0.17491728719350202, 0.00599138394746426, 0.0012449628981743917, 0.28195093942539506, 0.03750416296394804, 0.1821630772534619, 0.04433527836095286, 0.42111817270947366, 0.032816142593454535, 0.9908038484618776, 0.9985400093413848, 0.9948179104791299, 0.4138376321879867, 0.2038672065321885, 0.1340739426831679, 0.23758257023922597, 0.006349316668916542, 0.0041836582702163266, 0.6534098238770922, 0.22234907799810305, 0.054351996843980746, 0.06976819958518256, 0.9976867496289071, 0.45766570776263604, 0.23485404756148245, 0.2780196826201956, 0.02941861752409111, 0.07698302494429685, 0.06078842518467343, 0.053981999198744746, 0.035909764684382374, 0.7468292236974164, 0.025113364844633425, 0.9979993005756677, 0.9984699771132649, 0.995552113747485, 0.43043360722349583, 0.25061716912711857, 0.12819754328548574, 0.006393945406616604, 0.17187180161107954, 0.007286123835446828, 0.005183131824632729, 0.1305235998896998, 0.14004471197768345, 0.451650222148338, 0.24224601641578636, 0.03549325961912889, 0.9972326080436517, 0.3739343764506413, 0.433172925539293, 0.13827109278803923, 0.006095976601132435, 0.039803141336805896, 0.008677801985141466, 0.9982549660397776, 0.9956737228766576, 0.995087917754134, 0.9280515195193149, 0.07086938876329313, 0.08054941513702572, 0.02882043293893581, 0.6266596700568606, 0.26307882375028585, 0.9931127674207152, 0.21573938235579246, 0.7837871138797597, 0.9955102408733167, 0.08538471707755277, 0.7833119697114623, 0.07424758006743719, 0.05692314471836851, 0.1561032389735027, 0.741041812598352, 0.10227453587919143, 0.9974772005643242, 0.28221497200455664, 0.32076970449337605, 0.35535556746128755, 0.022820999745220304, 0.01899387556434485, 0.8147617659966188, 0.06779378376653707, 0.06408470137201529, 0.05316351432147892, 0.1389192141385414, 0.8603242559807915, 0.9941763106621929, 0.18899161051275506, 0.044872309758615024, 0.7010638512875383, 0.0649328717683488, 0.9974418525288217, 0.09662461169965879, 0.19997401322456856, 0.1440520767830078, 0.5592193644156077, 0.42250545549261886, 0.11349892825951417, 0.010184083291290528, 0.08375068496127079, 0.3699770258849098, 0.9938250255795794, 0.9966356261938807, 0.9978185832410114, 0.9977832105193047, 0.9973191093761317, 0.14827142485470493, 0.8513649556173379, 0.8404742942253454, 0.09290156544161249, 0.06661790916715325, 0.9979311711181615, 0.4212198907020799, 0.3713906443284537, 0.0981060237647574, 0.017477795964216607, 0.09174590890098802, 0.9963696993055413, 0.996507086719224, 0.9998594822280815, 0.9972784848974255, 0.9978689502394945, 0.25054825085276244, 0.49578452465564304, 0.06285839509026903, 0.1907885090415912, 0.18802290102214295, 0.8113007387854485, 0.9953312616013748, 0.9965154314951682, 0.9979787799550812, 0.9973511037560755, 0.9947242031581394, 0.5081987582522205, 0.10532769619509011, 0.3286696178188526, 0.03673192766467428, 0.02094752501358935, 0.9952528252632622, 0.9979029994971832, 0.9939731010398738, 0.21057869948417582, 0.15193504832154542, 0.07245592828277089, 0.19465946483723423, 0.13098252624946788, 0.06508157693896706, 0.17429220874482368, 0.9973265463904212, 0.9960733127574393, 0.9970419215031991, 0.427405114274668, 0.07151798724914762, 0.18447099885692841, 0.31660899434582973, 0.9928436774219419, 0.9257327162483513, 0.06759914358000131, 0.0065721389591667945, 0.12828453830593942, 0.14178817391709092, 0.14747391522704947, 0.06360923090516109, 0.08244324899439874, 0.43638064553931744, 0.9966913263649483, 0.9978088704261011, 0.9985962257999641, 0.47998510283783496, 0.01964851298166576, 0.1628955290527623, 0.23288166100650512, 0.05875841024993379, 0.045752965943021695, 0.4409020026249218, 0.26662684052694785, 0.06588409139745083, 0.136067223349924, 0.07139169757035092, 0.0069060892450158095, 0.012223777963677983, 0.13086379232056142, 0.3409306989921338, 0.30341343429954964, 0.008039413862696606, 0.21646866215483082, 0.0575298164932273, 0.2990464989412098, 0.010311948239352062, 0.11288869651501206, 0.5199392849104882, 0.8782284566751857, 0.12090588435316474, 0.9965527035139913, 0.9961459978269174, 0.9976822376226799, 0.997701036607978, 0.9954899099980616, 0.6470238607856309, 0.016835102481331566, 0.28289265664891733, 0.0005244580212252824, 0.052655585331018356, 0.7294397970244634, 0.039690106602801686, 0.23073897108817953, 0.9934540872625123, 0.995280000458378, 0.21909990165033702, 0.7805433996293256, 0.990963910167652, 0.9934602025201229, 0.9989983210934694, 0.221349398354755, 0.2850470669460514, 0.12648537048843142, 0.035716192745834055, 0.3313984954298966, 0.8311899095566552, 0.06085175585149048, 0.10773014554449056, 0.23125535078885184, 0.26936806659420165, 0.08389095903771547, 0.4153712899237184, 0.9975481873110763, 0.9964362975983724, 0.27178955147910794, 0.15171607986505317, 0.4558701241940416, 0.028634755755656348, 0.09204028635746683, 0.08566496613199216, 0.9137596387412498, 0.9995146649896813, 0.12295188890558023, 0.010912889547832564, 0.08657559041280502, 0.0327386686434977, 0.6324625764610518, 0.11034143876141815, 0.0041226471625145245, 0.041550940977681576, 0.017193492818350996, 0.9406273362706191, 0.1368797448022548, 0.8614733145094289, 0.9976930694620663, 0.9956043745282266, 0.9971143883617578, 0.30783301325492324, 0.564039463585992, 0.10903221871231948, 0.018493097166753836, 0.994430159518426, 0.9990564306921164, 0.3457178491968074, 0.16077518460870488, 0.40961830473555383, 0.06451488299584972, 0.01925206032257103, 0.9940196272069763, 0.1451854516905531, 0.2613338130429956, 0.591803555462445, 0.9974073376866964, 0.9970005425379449, 0.21201827667340267, 0.7874964562154956, 0.2000987506839883, 0.5510082063680909, 0.022501064333942458, 0.11665730377895167, 0.10259413857023764, 0.007232484964481504, 0.9970017929295409, 0.40817316756186817, 0.22173374166082047, 0.24604308306308473, 0.027665861344836316, 0.011391825259638484, 0.08452327491749624, 0.00040685090212994583, 0.13609311915509698, 0.03390715937993884, 0.7227334245915731, 0.09986355159845, 0.006502742894782791, 0.7099416232608494, 0.19517707661916392, 0.09483810906712076, 0.032623261915393734, 0.13386786785971913, 0.8335805889416124, 0.9605451805471408, 0.03701522853746207, 0.9799894912268275, 0.018204139155916302, 0.8725724500700839, 0.016368602959467664, 0.1095437274979759, 0.9939330911159432, 0.08861295909075317, 0.9109031063522585, 0.24153944325719393, 0.08051314775239797, 0.6774757629955067, 0.9973492353734015, 0.9971447955923649, 0.9968284392054684, 0.9937632338903533, 0.1483686812614925, 0.8510967079636523, 0.2695489559402269, 0.11606333314126469, 0.6139424911958486, 0.04180958014993364, 0.5232201505447309, 0.035368939865740204, 0.3994288610146402, 0.35531332947695476, 0.04336571525007534, 0.5319884656809655, 0.02694741976445011, 0.04229495684883892, 0.9968774699341911, 0.995357985016724, 0.1180654691539046, 0.880950039071442, 0.9954934103089497, 0.7081160118120772, 0.12671287225454142, 0.12057919130715511, 0.044481653699744564, 0.9962442269994414, 0.9962733660095988, 0.22838946723442327, 0.3313299682073106, 0.21659931874989302, 0.22365554397927095, 0.3448894394191883, 0.6540321807027986, 0.18275361101324386, 0.14280282162895333, 0.6740633187604762, 0.9912395291586859, 0.3095664378594423, 0.6082525916727897, 0.08212378779997724, 0.9967505988003824, 0.9983362641032444, 0.999615366739003, 0.696452807919684, 0.27613948578804665, 0.017342093310360417, 0.010065690522796605, 0.2945242112802426, 0.28462309802974395, 0.271805903479441, 0.0321447101420297, 0.11677888368053826, 0.1100712164391487, 0.08696017811206766, 0.7759824902702974, 0.02663645095324595, 0.8668886142491773, 0.13131154612621587, 0.9998520899635177, 0.004379524248608013, 0.35963622653275207, 0.07831619832804916, 0.2836385998657307, 0.17350644596691156, 0.031944765107493736, 0.010691191548072502, 0.05796429152569428, 0.10774456279630049, 0.028423412271416693, 0.7978385723627895, 0.06543994918302913, 0.998717613139242, 0.9938957052932567, 0.9977872167399989, 0.5038230921051898, 0.4956823240101329, 0.06860895960265066, 0.13075324520018833, 0.014389113108154697, 0.4946518303267093, 0.29132740597234946, 0.9905530744888906, 0.3916615494187721, 0.019448871457487617, 0.11202901975285853, 0.06793904418633684, 0.03779769362439336, 0.37102462472745606, 0.9964413061863431, 0.9941727885067941, 0.9981182325551555, 0.15639573650173513, 0.842051217059011, 0.9986585061052725, 0.9988210980391654, 0.9969609769632675, 0.23998102453316197, 0.19291760806111077, 0.5185455706785461, 0.04833540070102556, 0.9993820876176112, 0.7802859274655387, 0.21898058674576976, 0.3735188088667666, 0.625951850023977, 0.6517816611743786, 0.10256172381158693, 0.17824388803686578, 0.06738953414014123, 0.13091638859289592, 0.19260130351887195, 0.6765653966880987, 0.25788615407242865, 0.11851572761119501, 0.6233520351395686, 0.9963259850633737, 0.13136802509884787, 0.17533182632714756, 0.6929532479317713, 0.9932543882675309, 0.13758212159274072, 0.18727068878113023, 0.4840770634531102, 0.004416761527856845, 0.1866081745519517, 0.9961574694683982, 0.3015461371831928, 0.12402774843207345, 0.0417344556335413, 0.5325551662533581, 0.9964003690081054, 0.9950953529323541, 0.14336340112939241, 0.1412101168436545, 0.08431807939942132, 0.12273720428706086, 0.3829446106057051, 0.12545714233220348, 0.13828637886352932, 0.6439126367158933, 0.10785269702869855, 0.10945446975684754, 0.9954464018704408, 0.9989008834297405, 0.998088134574472, 0.9984688909924965, 0.2538256564889481, 0.6132748136070678, 0.0037789491900624642, 0.05331685738454467, 0.07576606049382664, 0.9981377314285658, 0.9981415407753964, 0.01370919487055429, 0.6733323788345318, 0.05747316311116991, 0.0972825559083564, 0.10624626024679575, 0.05167311912747386, 0.1308447993112221, 0.7891259989467503, 0.07962261043357313, 0.997701362480743, 0.958237880565179, 0.04089208594161504, 0.9951902524992273, 0.9972032993702103, 0.3568956278907089, 0.13799000997393807, 0.09680974521731779, 0.40819034364018325, 0.7136810216598619, 0.1860340760830591, 0.034308059395988216, 0.0658641354179667, 0.11360997843235342, 0.8567528961781005, 0.02806834761269908, 0.3532124556253929, 0.14982406197270887, 0.17317833353765122, 0.2711102073791875, 0.006409883603743125, 0.04620690006524392, 0.31462638679451926, 0.31023996061614195, 0.3748400552431535, 0.998706473141745, 0.9969849045072426, 0.8924566853399061, 0.10730639231013266, 0.17184141196133634, 0.1408536163617511, 0.0901463144715207, 0.5966559189083777, 0.1718095888540515, 0.828051368961537, 0.9987798847842629, 0.9973752650344317, 0.673691857432914, 0.08044861009551887, 0.1324182962134604, 0.016356530675512848, 0.09703714829834396, 0.8082164947032704, 0.1533415306426183, 0.038081505954292626, 0.0002961894907556093, 0.9973013147839594, 0.5388861889824826, 0.29122083377099445, 0.061768455655511326, 0.04729147386125086, 0.04255921314570765, 0.006849324719865167, 6.226658836241061e-05, 0.011394785670321142, 0.9932740904362085, 0.1670864986537754, 0.15560727355542442, 0.023596184924388128, 0.0669621464070474, 0.5860782147435862, 0.9962079768578181, 0.997055999646486, 0.12681190884902668, 0.04931574233017705, 0.8232709637976495, 0.08268772508738222, 0.7434238987022975, 0.13207067201456882, 0.04134386254369111, 0.07939856513691151, 0.7210090290006302, 0.05079172916846545, 0.1208492866422109, 0.02802302298949818, 0.9929557214514092, 0.9937797441880162, 0.9982070179936093, 0.9943084905099034, 0.9977269630555354, 0.9975111652397873, 0.9953629851188388, 0.2962500249948153, 0.12531683583258188, 0.10558391894278883, 0.3645464191653328, 0.06970588823407418, 0.038440747187908554, 0.27240055850570527, 0.24824751842833687, 0.07360245940736528, 0.23960101591543279, 0.019222389884059474, 0.10382948885330266, 0.034085799162440034, 0.009075254703626592, 0.9972080692328196, 0.9977293763381876, 0.09013975527817437, 0.8190960370929757, 0.09013975527817437, 0.9922041032758618, 0.1518844069245862, 0.20535538425197997, 0.2582514048339179, 0.17804493884282724, 0.1432600557427485, 0.06324524200014316, 0.9965651805539524, 0.9928627285764817, 0.9955461381804743, 0.9995798089066031, 0.9961586515448853, 0.09270258256698198, 0.6900260105098884, 0.07026100714918318, 0.04467341648589856, 0.10235036265313847, 0.22573412031999135, 0.7740274024170262, 0.9969502154052603, 0.13666244533187233, 0.4214915766766745, 0.4181196440632611, 0.023603528293893773, 0.04673923845777357, 0.37151702363871303, 0.5404973472937405, 0.039848196890281316, 0.001198442011737784, 0.7677247184102661, 0.0941662349925092, 0.11455892282528189, 0.022791827577804773, 0.9986138217163605, 0.2041066292532138, 0.25513328656651724, 0.040423715533915716, 0.01921783197514026, 0.16401425564990393, 0.3167628856592084, 0.9924280975451131, 0.9961874429163016, 0.777569032259314, 0.11968047946432063, 0.10268443504335202, 0.99551811802377, 0.9969387574683001, 0.743436939706412, 0.047273481630334804, 0.10950967074917008, 0.029491713310667585, 0.013517035267389309, 0.056814918289668434, 0.9145228363571611, 0.08542145555069895, 0.24255585061926768, 0.7573682682601623, 0.7944166998718489, 0.16152904390378314, 0.04402451338543816, 0.9978900620197582, 0.4612232688856274, 0.25033282947972896, 0.197102082987331, 0.06696416922005477, 0.024280691382497298, 0.4127888847571997, 0.4356212186136465, 0.0395869961108178, 0.07564237223664563, 0.02806132158616061, 0.008267823530751712, 0.9983547000261651, 0.9936458816484154, 0.13184861298997716, 0.8662312100739359, 0.9969988430232365, 0.1022239113231314, 0.40975466975742586, 0.3685214954422132, 0.11961915673736175, 0.9988282467890517, 0.996026780942313, 0.27447767584803706, 0.27631058186371676, 0.34229519842818645, 0.016954380645037348, 0.07942592734612092, 0.010539209590158352, 0.9979990125386532, 0.17627363653588954, 0.14165040218233002, 0.34100619495392587, 0.3411150730493773, 0.07110321743616746, 0.6345287125632666, 0.07980361113088838, 0.2145097066112225, 0.9976307922493964, 0.1854444432913068, 0.4425697568714621, 0.17878134869498524, 0.0745565216409452, 0.0663503946117913, 0.05225268920273206, 0.12440685730942035, 0.07495693627056951, 0.43422083948713547, 0.28406633472682924, 0.08229621895516781, 0.601924018541556, 0.17302748945480834, 0.18199080303450746, 0.04300021360481202, 0.9989211245278085, 0.9063507360626252, 0.09311237533536866, 0.39961392416193214, 0.17766229155430305, 0.0605191484425742, 0.22526972656276428, 0.07386365674040045, 0.06304378514756835, 0.12457937303015637, 0.10498261772204188, 0.20728234632452047, 0.5000672024159929, 0.06298957063322513, 0.9996463621814737], \"Term\": [\"abc\", \"abortion\", \"abysmal\", \"act\", \"act\", \"act\", \"actor\", \"actor\", \"actor\", \"actor\", \"adaptation\", \"addict\", \"adventure\", \"adventure\", \"adventure\", \"adventure\", \"africa\", \"african\", \"agent\", \"agent\", \"air\", \"air\", \"air\", \"air\", \"air\", \"air\", \"akshay\", \"album\", \"alien\", \"alien\", \"allen\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"although\", \"altman\", \"amateurish\", \"amateurish\", \"amazon\", \"america\", \"america\", \"america\", \"america\", \"america\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"americans\", \"animal\", \"animal\", \"animal\", \"animal\", \"animal\", \"animate\", \"animation\", \"anime\", \"answer\", \"answer\", \"antonio\", \"anyways\", \"ape\", \"ape\", \"apocalypse\", \"argento\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"arquette\", \"arrow\", \"arthur\", \"artistic\", \"artistic\", \"atrocious\", \"attack\", \"attack\", \"attack\", \"attack\", \"attempt\", \"attempt\", \"attempt\", \"attempt\", \"audience\", \"audience\", \"audience\", \"audience\", \"aussie\", \"avoid\", \"avoid\", \"avoid\", \"avoid\", \"awful\", \"awful\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"badly\", \"badly\", \"badly\", \"badly\", \"badly\", \"band\", \"band\", \"band\", \"basketball\", \"batman\", \"beatty\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautifully\", \"beautifully\", \"become\", \"become\", \"become\", \"become\", \"become\", \"become\", \"become\", \"belief\", \"belief\", \"bell\", \"belushi\", \"ben\", \"ben\", \"bettie\", \"betty\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"black\", \"black\", \"black\", \"black\", \"blah\", \"bobby\", \"bobby\", \"boll\", \"bollywood\", \"bon\", \"bond\", \"bond\", \"bond\", \"bond\", \"book\", \"book\", \"boring\", \"boring\", \"boring\", \"boy\", \"boy\", \"boy\", \"boy\", \"branagh\", \"brando\", \"brilliant\", \"brilliant\", \"brilliant\", \"brilliant\", \"brilliantly\", \"british\", \"british\", \"british\", \"british\", \"british\", \"broadway\", \"brooks\", \"brow\", \"bruno\", \"budget\", \"budget\", \"budget\", \"burt\", \"button\", \"buy\", \"buy\", \"buy\", \"caine\", \"came\", \"cancel\", \"cannibal\", \"car\", \"car\", \"career\", \"career\", \"career\", \"career\", \"career\", \"career\", \"carol\", \"carpenter\", \"carpenter\", \"carter\", \"cartoon\", \"casper\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"catholic\", \"chaplin\", \"chaplin\", \"character\", \"character\", \"character\", \"character\", \"character\", \"charlie\", \"charlie\", \"chase\", \"chase\", \"che\", \"cheap\", \"cheap\", \"cheap\", \"chief\", \"chief\", \"child\", \"child\", \"child\", \"china\", \"christ\", \"christian\", \"cinema\", \"cinema\", \"cinema\", \"cinematography\", \"cinematography\", \"civil\", \"claire\", \"clown\", \"colonel\", \"columbo\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"comedy\", \"comedy\", \"comedy\", \"compelling\", \"compelling\", \"concert\", \"connery\", \"contribution\", \"convict\", \"cop\", \"cop\", \"could\", \"could\", \"could\", \"could\", \"country\", \"country\", \"country\", \"cowboy\", \"crime\", \"crime\", \"crime\", \"criminal\", \"criminal\", \"critter\", \"crocodile\", \"cultural\", \"cusack\", \"dad\", \"dad\", \"dad\", \"dance\", \"dance\", \"dance\", \"danny\", \"daughter\", \"daughter\", \"daughter\", \"davis\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dead\", \"dead\", \"dead\", \"death\", \"death\", \"death\", \"death\", \"death\", \"decent\", \"decent\", \"decent\", \"decent\", \"decent\", \"dee\", \"delete\", \"dennis\", \"dennis\", \"derek\", \"detective\", \"dialogue\", \"dialogue\", \"dilemma\", \"dinosaur\", \"dire\", \"director\", \"director\", \"director\", \"director\", \"director\", \"discussion\", \"disney\", \"doc\", \"doctor\", \"doctor\", \"doctor\", \"documentary\", \"documentary\", \"doug\", \"dracula\", \"drama\", \"drama\", \"drama\", \"drama\", \"dreary\", \"dreck\", \"dud\", \"dull\", \"dull\", \"dull\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"east\", \"east\", \"eastwood\", \"eddie\", \"educate\", \"effect\", \"effect\", \"effect\", \"effect\", \"egg\", \"emma\", \"end\", \"end\", \"end\", \"end\", \"end\", \"ensemble\", \"enterprise\", \"episode\", \"ernest\", \"escape\", \"escape\", \"escape\", \"escape\", \"escape\", \"europe\", \"even\", \"even\", \"even\", \"even\", \"even\", \"event\", \"event\", \"event\", \"event\", \"event\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"examine\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"falk\", \"family\", \"family\", \"fan\", \"fan\", \"fan\", \"fan\", \"fan\", \"father\", \"father\", \"father\", \"father\", \"fbi\", \"feel\", \"feel\", \"feel\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"fighter\", \"filler\", \"film\", \"film\", \"film\", \"film\", \"film\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"flawless\", \"flick\", \"flick\", \"flick\", \"flynn\", \"football\", \"ford\", \"foster\", \"foster\", \"france\", \"france\", \"francis\", \"frankenstein\", \"frankie\", \"freedom\", \"friend\", \"friend\", \"friend\", \"friend\", \"frontal\", \"fulci\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"funny\", \"funny\", \"futuristic\", \"game\", \"game\", \"gang\", \"gang\", \"gang\", \"gang\", \"gary\", \"gary\", \"gay\", \"gay\", \"gay\", \"geek\", \"gene\", \"gene\", \"gene\", \"george\", \"george\", \"george\", \"german\", \"german\", \"germans\", \"germany\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"girl\", \"girl\", \"girl\", \"girl\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"glover\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gore\", \"great\", \"great\", \"great\", \"great\", \"greek\", \"guard\", \"guard\", \"guess\", \"guess\", \"guess\", \"guess\", \"gun\", \"gun\", \"gun\", \"gus\", \"guy\", \"guy\", \"guy\", \"halloween\", \"halloween\", \"hamlet\", \"hank\", \"happen\", \"happen\", \"happen\", \"happen\", \"happen\", \"hardy\", \"hate\", \"hate\", \"hate\", \"hate\", \"head\", \"head\", \"head\", \"head\", \"heart\", \"heart\", \"heart\", \"heart\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"hey\", \"hey\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"hitler\", \"hmm\", \"hoffman\", \"home\", \"home\", \"home\", \"home\", \"homer\", \"hop\", \"hopper\", \"horrendous\", \"horrible\", \"horrible\", \"horror\", \"horror\", \"house\", \"house\", \"howard\", \"howard\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"husband\", \"husband\", \"husband\", \"husband\", \"hyde\", \"immature\", \"inability\", \"incoherent\", \"india\", \"indian\", \"indian\", \"indian\", \"indian\", \"interview\", \"interview\", \"interview\", \"issue\", \"issue\", \"issue\", \"jack\", \"jack\", \"jack\", \"jack\", \"james\", \"james\", \"james\", \"james\", \"james\", \"jane\", \"jet\", \"jewish\", \"jimmy\", \"joan\", \"joe\", \"joe\", \"john\", \"john\", \"john\", \"john\", \"joke\", \"joke\", \"joke\", \"joke\", \"jon\", \"jones\", \"jones\", \"jones\", \"jones\", \"jones\", \"jordan\", \"julie\", \"julie\", \"karate\", \"karen\", \"karloff\", \"keaton\", \"kelly\", \"khan\", \"kid\", \"kid\", \"kidnap\", \"kill\", \"kill\", \"kill\", \"killer\", \"killer\", \"kim\", \"kim\", \"kinda\", \"kirk\", \"know\", \"know\", \"know\", \"know\", \"know\", \"lake\", \"lake\", \"later\", \"later\", \"later\", \"later\", \"later\", \"later\", \"laugh\", \"laugh\", \"laugh\", \"laughable\", \"laura\", \"laura\", \"laura\", \"laurel\", \"layer\", \"lemmon\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"lion\", \"lion\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"live\", \"live\", \"live\", \"live\", \"live\", \"live\", \"lizard\", \"lloyd\", \"lol\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lot\", \"lot\", \"lot\", \"lot\", \"lou\", \"love\", \"love\", \"love\", \"love\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"lugosi\", \"lynch\", \"madonna\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"man\", \"man\", \"man\", \"man\", \"man\", \"mann\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"maria\", \"marine\", \"mario\", \"marriage\", \"marriage\", \"marry\", \"marry\", \"marry\", \"marry\", \"marty\", \"mary\", \"mary\", \"masterful\", \"masterpiece\", \"masterpiece\", \"masterpiece\", \"masterpiece\", \"match\", \"match\", \"match\", \"matthau\", \"may\", \"may\", \"may\", \"may\", \"may\", \"maybe\", \"maybe\", \"maybe\", \"maybe\", \"medium\", \"medium\", \"meg\", \"message\", \"message\", \"message\", \"message\", \"mgm\", \"michael\", \"michael\", \"michael\", \"michael\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"miyazaki\", \"monk\", \"morality\", \"morgan\", \"moron\", \"mother\", \"mother\", \"movie\", \"movie\", \"movie\", \"mstk\", \"much\", \"much\", \"much\", \"much\", \"much\", \"mummy\", \"muppet\", \"murder\", \"murderer\", \"murphy\", \"music\", \"music\", \"music\", \"music\", \"musical\", \"musical\", \"mutant\", \"myers\", \"nancy\", \"nation\", \"ned\", \"need\", \"need\", \"need\", \"need\", \"need\", \"nerd\", \"nero\", \"nerve\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"ninja\", \"niro\", \"norris\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"novak\", \"novel\", \"novel\", \"novel\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"nun\", \"observe\", \"officer\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"original\", \"original\", \"original\", \"original\", \"original\", \"oscar\", \"oscar\", \"oscar\", \"oscar\", \"oscar\", \"outstanding\", \"outstanding\", \"painting\", \"passionate\", \"patrick\", \"penguin\", \"penn\", \"people\", \"people\", \"people\", \"people\", \"people\", \"performance\", \"performance\", \"performance\", \"philosophical\", \"philosophy\", \"pilot\", \"pilot\", \"pimp\", \"pitiful\", \"planet\", \"play\", \"play\", \"play\", \"play\", \"play\", \"please\", \"please\", \"please\", \"plot\", \"plot\", \"plot\", \"plot\", \"poem\", \"poetry\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pointless\", \"pointless\", \"police\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poorly\", \"poorly\", \"poorly\", \"porn\", \"porn\", \"pot\", \"powell\", \"prank\", \"present\", \"present\", \"present\", \"present\", \"priceless\", \"prison\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"profanity\", \"program\", \"program\", \"program\", \"pseudo\", \"punk\", \"question\", \"question\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"ranger\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"really\", \"really\", \"really\", \"redeem\", \"redeem\", \"redeem\", \"religion\", \"religion\", \"religious\", \"religious\", \"rescue\", \"rescue\", \"rescue\", \"retarded\", \"revenge\", \"revenge\", \"ridiculous\", \"ridiculous\", \"ridiculous\", \"rita\", \"roberts\", \"robinson\", \"robocop\", \"robot\", \"robot\", \"rock\", \"rock\", \"rock\", \"role\", \"role\", \"role\", \"role\", \"run\", \"run\", \"run\", \"run\", \"run\", \"russell\", \"russia\", \"russian\", \"russian\", \"ruth\", \"say\", \"say\", \"say\", \"say\", \"scarecrow\", \"scarlett\", \"scene\", \"scene\", \"scene\", \"scene\", \"sci\", \"sci\", \"science\", \"science\", \"science\", \"scifi\", \"script\", \"script\", \"script\", \"seagal\", \"sean\", \"season\", \"see\", \"see\", \"see\", \"see\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"self\", \"self\", \"self\", \"self\", \"serial\", \"serial\", \"series\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sexual\", \"sexual\", \"sexual\", \"sexual\", \"shakespeare\", \"sheen\", \"sheriff\", \"ship\", \"ship\", \"shoot\", \"shoot\", \"shoot\", \"shoot\", \"shoot\", \"shove\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"showtime\", \"sid\", \"sinatra\", \"sing\", \"sing\", \"singer\", \"singing\", \"sinister\", \"sister\", \"sister\", \"sister\", \"sister\", \"slasher\", \"social\", \"social\", \"soldier\", \"soldier\", \"something\", \"something\", \"something\", \"something\", \"son\", \"son\", \"son\", \"song\", \"song\", \"song\", \"soviet\", \"space\", \"space\", \"space\", \"spade\", \"special\", \"special\", \"special\", \"special\", \"special\", \"spock\", \"stage\", \"stage\", \"stage\", \"stage\", \"stalk\", \"stan\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"state\", \"state\", \"state\", \"state\", \"stellar\", \"stewart\", \"stinker\", \"stooge\", \"story\", \"story\", \"story\", \"story\", \"story\", \"streisand\", \"stunning\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"subject\", \"subject\", \"subject\", \"sullivan\", \"superb\", \"superb\", \"superbly\", \"superman\", \"support\", \"support\", \"support\", \"support\", \"sure\", \"sure\", \"sure\", \"sure\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"talent\", \"talent\", \"talent\", \"tarzan\", \"tasteless\", \"teen\", \"teen\", \"television\", \"television\", \"television\", \"television\", \"terrible\", \"terrible\", \"that\", \"theory\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"thug\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timothy\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tommy\", \"tonight\", \"tony\", \"tony\", \"tony\", \"town\", \"town\", \"town\", \"town\", \"train\", \"train\", \"train\", \"train\", \"train\", \"trashy\", \"travolta\", \"trek\", \"tripe\", \"troma\", \"troubled\", \"turkish\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"unfunny\", \"uninspired\", \"unique\", \"unique\", \"unique\", \"upside\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"useless\", \"uwe\", \"vague\", \"vampire\", \"velvet\", \"version\", \"version\", \"version\", \"version\", \"version\", \"victim\", \"victim\", \"victoria\", \"view\", \"view\", \"view\", \"view\", \"viewer\", \"viewer\", \"viewer\", \"viewer\", \"viewer\", \"visual\", \"visual\", \"visual\", \"visual\", \"visually\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voight\", \"von\", \"wait\", \"wait\", \"wait\", \"walker\", \"wallace\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"war\", \"war\", \"waste\", \"waste\", \"watch\", \"watch\", \"watch\", \"wax\", \"way\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"werewolf\", \"what\", \"whatsoever\", \"whatsoever\", \"widow\", \"wife\", \"wife\", \"wife\", \"wife\", \"williams\", \"wine\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"wolf\", \"woman\", \"woman\", \"woman\", \"woman\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"woody\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"would\", \"would\", \"would\", \"would\", \"wwii\", \"yeah\", \"yeah\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"young\", \"young\", \"young\", \"young\", \"young\", \"zombie\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 5, 3, 9, 10, 7, 8, 2, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el240402351133087936430626804\", ldavis_el240402351133087936430626804_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el240402351133087936430626804\", ldavis_el240402351133087936430626804_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el240402351133087936430626804\", ldavis_el240402351133087936430626804_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.203697 -0.064025       1        1  28.656566\n",
       "3      0.196741 -0.059169       2        1  19.975895\n",
       "4      0.155531 -0.092714       3        1  13.508752\n",
       "2      0.106041  0.078898       4        1  13.345973\n",
       "8      0.165598 -0.217527       5        1   9.507712\n",
       "9      0.019705  0.214622       6        1   6.198136\n",
       "6     -0.036367  0.199971       7        1   4.759829\n",
       "7     -0.061742  0.151364       8        1   2.989362\n",
       "1     -0.382265 -0.102324       9        1   0.673405\n",
       "5     -0.366940 -0.109097      10        1   0.384369, topic_info=           Term           Freq          Total Category  logprob  loglift\n",
       "265        film  100343.000000  100343.000000  Default  30.0000  30.0000\n",
       "272       movie  106187.000000  106187.000000  Default  29.0000  29.0000\n",
       "104        show   22726.000000   22726.000000  Default  28.0000  28.0000\n",
       "446         bad   26620.000000   26620.000000  Default  27.0000  27.0000\n",
       "165        play   17583.000000   17583.000000  Default  26.0000  26.0000\n",
       "...         ...            ...            ...      ...      ...      ...\n",
       "728    arquette     108.362440     109.302420  Topic10  -5.3207   5.5527\n",
       "3810     upside     103.876988     104.817144  Topic10  -5.3629   5.5523\n",
       "11594    lizard     103.015902     103.955995  Topic10  -5.3713   5.5522\n",
       "3959        kim     305.038659     339.352009  Topic10  -4.2857   5.4547\n",
       "3430       lake     161.576572     471.765094  Topic10  -4.9212   4.4898\n",
       "\n",
       "[715 rows x 6 columns], token_table=       Topic      Freq      Term\n",
       "term                            \n",
       "14868      8  0.997194       abc\n",
       "20379     10  0.991806  abortion\n",
       "4259       5  0.993859   abysmal\n",
       "289        1  0.274815       act\n",
       "289        2  0.322410       act\n",
       "...      ...       ...       ...\n",
       "251        2  0.104983     young\n",
       "251        4  0.207282     young\n",
       "251        6  0.500067     young\n",
       "251        7  0.062990     young\n",
       "288        5  0.999646    zombie\n",
       "\n",
       "[1442 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 4, 5, 3, 9, 10, 7, 8, 2, 6])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_lda_vis(lda_model, bow_corpus, dic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
